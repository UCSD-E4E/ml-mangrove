{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../ml-mangrove/Segmentation/unet.pyp\" \n",
    "import segmentation_models as sm\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import rasterio\n",
    "import subprocess\n",
    "import tensorflow_datasets as tfds\n",
    "from PIL import Image\n",
    "from segmentation_models.utils import set_trainable\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing notebooks\n",
    "#from ipynb.fs.full.<notebook_name> import *\n",
    "from ipynb.fs.full.create_seg_dataset import create_seg_dataset\n",
    "from ipynb.fs.full.gen_seg_labels import gen_seg_labels, tif_to_jpg, tile_raster\n",
    "from ipynb.fs.full.raster_mask import raster_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources: https://yann-leguilly.gitlab.io/post/2019-12-14-tensorflow-tfdata-segmentation/\n",
    "\n",
    "'''\n",
    "Documentation/Usage: This script is meant to be called with command line arguments.\n",
    "--width (required): tile width\n",
    "--input_rasters (required): space separated list of rasters (orthomosaics)\n",
    "--input_vectors (required for training): space separated list of shapefiles (ordering should correspond with rasters)\n",
    "--train: Flag. Add if training.\n",
    "--test: Flag. Add if testing.\n",
    "--weights (required): path to weights file, either to write to for training, or to use for testing (.h5)\n",
    "--backbone (required): name of backbone to use, ex: resnet34, vgg16\n",
    "\n",
    "For training it should be sufficient to just call the script using the list of rasters and vectors (and other required arguments), \n",
    "and currently you have to manually set the hyperparams in the code, but this should eventually be offloaded to a settings file or \n",
    "command line arguments. This will result in the training weights being saved in the specified .h5 file.\n",
    "\n",
    "For testing you just need to call the script on the list of rasters and it will produce a mask of the entire\n",
    "orthomosaic.\n",
    "'''\n",
    "#keras.backend.set_image_data_format('channels_first')\n",
    "sm.set_framework('tf.keras')    # need this otherwise currently a bug in model.fit when used with tf.Datasets\n",
    "\n",
    "# Globals\n",
    "N_CHANNELS = 3\n",
    "WIDTH = 256\n",
    "HEIGHT = 256\n",
    "LOSS_FUNC = sm.losses.DiceLoss()\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_image(img_path: str) -> dict:\n",
    "    \"\"\"Load an image and its annotation (mask) and returning\n",
    "    a dictionary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_path : str\n",
    "        Image (not the mask) location.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary mapping an image and its annotation.\n",
    "    \"\"\"\n",
    "    image = tf.io.read_file(img_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.uint8)\n",
    "\n",
    "    # Creating mask path from image path\n",
    "    mask_path = tf.strings.regex_replace(img_path, \"images\", \"annotations\")\n",
    "    mask_path = tf.strings.regex_replace(mask_path, \"image\", \"annotation\")\n",
    "    mask = tf.io.read_file(mask_path)\n",
    "    # The masks contain a class index for each pixels\n",
    "    mask = tf.image.decode_jpeg(mask, channels=1)\n",
    "    mask = tf.image.convert_image_dtype(mask, tf.uint8)\n",
    "    \n",
    "    #mask = tf.where(mask == 255, np.dtype('uint8').type(0), mask)\n",
    "    # Note that we have to convert the new value (0)\n",
    "    # With the same dtype than the tensor itself\n",
    "    \n",
    "\n",
    "    return {'image': image, 'segmentation_mask': mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def normalize(input_image: tf.Tensor, input_mask: tf.Tensor) -> tuple:\n",
    "    \"\"\"Rescale the pixel values of the images/masks between 0.0 and 1.0\n",
    "    compared to [0,255] originally.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_image : tf.Tensor\n",
    "        Tensorflow tensor containing an image of size [SIZE,SIZE,3].\n",
    "    input_mask : tf.Tensor\n",
    "        Tensorflow tensor containing an annotation of size [SIZE,SIZE,1].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Normalized image and its annotation.\n",
    "    \"\"\"\n",
    "    input_mask = tf.cast(input_mask, tf.float32) / 255.0 # attempting to fix metrics    \n",
    "    input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "    return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def load_image_train(datapoint: dict) -> tuple:\n",
    "    \"\"\"Apply some transformations to an input dictionary\n",
    "    containing a train image and its annotation.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    An annotation is a regular  channel image.\n",
    "    If a transformation such as rotation is applied to the image,\n",
    "    the same transformation has to be applied on the annotation also.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datapoint : dict\n",
    "        A dict containing an image and its annotation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A modified image and its annotation.\n",
    "    \"\"\"\n",
    "   \n",
    "    input_image = tf.image.resize(datapoint['image'], (HEIGHT, WIDTH))\n",
    "    input_mask = tf.image.resize(datapoint['segmentation_mask'], (HEIGHT, WIDTH))\n",
    "    #input_mask = tf.image.rgb_to_grayscale(datapoint['segmentation_mask'])\n",
    "    \n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        input_mask = tf.image.flip_left_right(input_mask)\n",
    "    \n",
    "    #input_mask = tf.reshape(input_mask, (HEIGHT, WIDTH))  # removing single channel\n",
    "\n",
    "    input_image, input_mask = normalize(input_image, input_mask)\n",
    "    \n",
    "    return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def load_image_val(datapoint: dict) -> tuple:\n",
    "    \"\"\"Normalize and resize a test image and its annotation.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Since this is for the val set, we don't need to apply\n",
    "    any data augmentation technique.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datapoint : dict\n",
    "        A dict containing an image and its annotation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A modified image and its annotation.\n",
    "    \"\"\"\n",
    "    input_image = tf.image.resize(datapoint['image'], (HEIGHT, WIDTH))\n",
    "    input_mask = tf.image.resize(datapoint['segmentation_mask'], (HEIGHT, WIDTH))\n",
    "    \n",
    "    input_image, input_mask = normalize(input_image, input_mask)\n",
    "    #input_mask = tf.reshape(input_mask, (HEIGHT, WIDTH)) # removing single channel\n",
    "\n",
    "    \n",
    "    return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def load_image(datapoint: dict) -> tuple:\n",
    "    \"\"\"Loads and image and resizes it\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datapoint : dict\n",
    "        A dict containing an image and its annotation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A image and its annotation.\n",
    "    \"\"\"\n",
    "    input_image = tf.image.resize(datapoint['image'], (HEIGHT, WIDTH))\n",
    "    input_mask = tf.image.resize(datapoint['segmentation_mask'], (HEIGHT, WIDTH))\n",
    "    #input_mask = tf.image.resize(datapoint['label'], (HEIGHT, WIDTH))\n",
    "\n",
    "    return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\n",
    "    pred_mask = tf.argmin(pred_mask, axis=-1)\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\n",
    "    #return pred_mask[0]\n",
    "    return pred_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(model=None, dataset=None, num=1):\n",
    "    if dataset:\n",
    "        for image, mask in dataset.take(num):\n",
    "            pred_mask = model.predict(image)\n",
    "            display([image[0], mask[0], create_mask(pred_mask[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(backbone, weight_file):\n",
    "    # For tensorboard\n",
    "    logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, update_freq='epoch')\n",
    "\n",
    "\n",
    "    # For more information about autotune:\n",
    "    # https://www.tensorflow.org/guide/data_performance#prefetching\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    print(f\"Tensorflow ver. {tf.__version__}\")\n",
    "\n",
    "    # For reproducibility\n",
    "    SEED = 42\n",
    "\n",
    "    # Data\n",
    "    training_data = \"../dataset/training/\"\n",
    "    #val_data = \"../dataset/validation/\"\n",
    "\n",
    "    # Listing GPU info\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "\n",
    "    # Hyperparams\n",
    "    BATCH_SIZE = 16\n",
    "    BUFFER_SIZE = 1000 # See https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle\n",
    "\n",
    "    # Creating and splitting dataset\n",
    "    DATASET_SIZE = len(glob(training_data + \"images/*.jpg\"))\n",
    "    print(f\"The Training Dataset contains {DATASET_SIZE} images.\")\n",
    "\n",
    "    TRAIN_SIZE = int(0.8 * DATASET_SIZE)\n",
    "    VAL_SIZE = int(0.2 * DATASET_SIZE)\n",
    "\n",
    "    full_dataset = tf.data.Dataset.list_files(training_data + \"images/*.jpg\", seed=SEED)\n",
    "    full_dataset = full_dataset.shuffle(buffer_size=BUFFER_SIZE, seed=SEED)\n",
    "    train_dataset = full_dataset.take(TRAIN_SIZE)\n",
    "    val_dataset = full_dataset.skip(TRAIN_SIZE)\n",
    "    \n",
    "    # Creating dict pairs linking images and annotations\n",
    "    train_dataset = train_dataset.map(parse_image)\n",
    "    val_dataset = val_dataset.map(parse_image)\n",
    "\n",
    "    # -- Train Dataset --# - https://stackoverflow.com/questions/49915925/output-differences-when-changing-order-of-batch-shuffle-and-repeat\n",
    "    train_dataset = train_dataset.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=BUFFER_SIZE, seed=SEED)\n",
    "    train_dataset = train_dataset.repeat()\n",
    "    train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "    train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    #-- Validation Dataset --#\n",
    "    #for image, label in tfds.as_numpy(val_dataset):\n",
    "     # print(type(image), type(label), label)\n",
    "    \n",
    "    val_dataset = val_dataset.map(load_image_val, num_parallel_calls=AUTOTUNE)\n",
    "    val_dataset = val_dataset.repeat()\n",
    "    val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "    val_dataset = val_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    \n",
    "    # define model\n",
    "    model = sm.Unet(\n",
    "        #'resnet34',\n",
    "        #'vgg16', \n",
    "        backbone,\n",
    "        input_shape=(HEIGHT, WIDTH, N_CHANNELS), \n",
    "        encoder_weights='imagenet', \n",
    "        encoder_freeze=True,    # only training decoder network\n",
    "        classes=NUM_CLASSES,\n",
    "        activation='sigmoid'\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        'Adam', \n",
    "        loss=LOSS_FUNC, \n",
    "        metrics=[sm.metrics.iou_score] #was giving score over 100 in later epochs before normalizing masks\n",
    "        #[tf.keras.metrics.MeanIoU(num_classes=2)]]\n",
    "    )\n",
    "    # TODO research step sizes\n",
    "    history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=50,\n",
    "    steps_per_epoch=TRAIN_SIZE / BATCH_SIZE,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps= 0.2 * (VAL_SIZE / BATCH_SIZE),\n",
    "    callbacks=[tensorboard_callback]\n",
    "    )\n",
    "\n",
    "    # Saving model\n",
    "    #model.save_weights(\"unet_500_weights_vgg16.h5\")\n",
    "    model.save_weights(weight_file)\n",
    "\n",
    "    # For reinstantiation\n",
    "    #model = keras.models.load_model(your_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to display slice of batch in datasets, as images\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def showDataImgs(train_dataset, val_dataset, n):\n",
    "    train_dataset_np = tfds.as_numpy(train_dataset)\n",
    "    val_dataset_np = tfds.as_numpy(val_dataset)\n",
    "    \n",
    "\n",
    "    for i in range(n):\n",
    "        example = next(val_dataset_np)\n",
    "    image = example['image']\n",
    "    label = example['segmentation_mask']\n",
    "    \n",
    "    #print(\"val_label: \", label.shape)#label[11])\n",
    "    #print(\"val_image: \", image.shape)\n",
    "    \n",
    "    #label_slice = label[n].reshape(HEIGHT,WIDTH)\n",
    "    #label_slice = np.around(label_slice)\n",
    "    #plt.imshow(label_slice, cmap='gray')\n",
    "    label = label.reshape(HEIGHT,WIDTH)\n",
    "    plt.imshow(label)\n",
    "    plt.show()\n",
    "    plt.imshow(image)\n",
    "    #plt.imshow(image[n])\n",
    "    plt.show()\n",
    "    unique = np.unique(label)\n",
    "    print(unique)\n",
    "    if len(unique) > 2:\n",
    "        with np.printoptions(threshold=np.inf):\n",
    "            print(label)\n",
    "    #print(label_slice)\n",
    "\n",
    "    \n",
    "    for i in range(n):\n",
    "        example = next(train_dataset_np)\n",
    "    image = example['image']\n",
    "    label = example['segmentation_mask']\n",
    "    \n",
    "    #print(\"train_label: \",label.shape)#label[15])\n",
    "    #print(\"train_image: \",image.shape)\n",
    "    \n",
    "    #label_slice = label[n].reshape(HEIGHT,WIDTH)\n",
    "    #label_slice = np.around(label_slice)\n",
    "    #plt.imshow(label_slice, cmap='gray')\n",
    "    label = label.reshape(HEIGHT,WIDTH)\n",
    "    plt.imshow(label)\n",
    "    plt.show()\n",
    "    plt.imshow(image)\n",
    "    #plt.imshow(image[n])\n",
    "    plt.show()\n",
    "    unique = np.unique(label)\n",
    "    print(unique)\n",
    "    if len(unique) > 2:\n",
    "        with np.printoptions(threshold=np.inf):\n",
    "            print(label)\n",
    "    #print(label_slice)\n",
    "\n",
    "    #print(image[0])\n",
    "    # label_grey = np.mean(label[n], -1)\n",
    "    # label_grey.reshape((256,256, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Training Dataset contains 0 images.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: ../dataset/training/images/*.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-2acbe95a5385>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mVAL_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mDATASET_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mfull_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"images/*.jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mfull_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBUFFER_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mlist_files\u001b[0;34m(file_pattern, shuffle, seed)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m       assert_not_empty = control_flow_ops.Assert(\n\u001b[0;32m-> 1073\u001b[0;31m           condition, [message], summarize=1, name=\"assert_not_empty\")\n\u001b[0m\u001b[1;32m   1074\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massert_not_empty\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m         \u001b[0mmatching_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatching_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m       return _add_should_use_warning(fn(*args, **kwargs),\n\u001b[0m\u001b[1;32m    236\u001b[0m                                      \u001b[0mwarn_in_eager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_in_eager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                                      error_in_function=error_in_function)\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mAssert\u001b[0;34m(condition, data, summarize, name)\u001b[0m\n\u001b[1;32m    154\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m           \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Expected '%s' to be true. Summarized data: %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m           (condition, \"\\n\".join(data_str)))\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: ../dataset/training/images/*.jpg'"
     ]
    }
   ],
   "source": [
    "# use to view images/masks in dataset\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "SEED = 42\n",
    "training_data = \"../dataset/training/\"\n",
    "BATCH_SIZE = 16\n",
    "BUFFER_SIZE = 1000 # See https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle\n",
    "\n",
    "# Creating and splitting dataset\n",
    "DATASET_SIZE = len(glob(training_data + \"images/*.jpg\"))\n",
    "print(f\"The Training Dataset contains {DATASET_SIZE} images.\")\n",
    "\n",
    "TRAIN_SIZE = int(0.8 * DATASET_SIZE)\n",
    "VAL_SIZE = int(0.2 * DATASET_SIZE)\n",
    "\n",
    "full_dataset = tf.data.Dataset.list_files(training_data + \"images/*.jpg\", seed=SEED)\n",
    "full_dataset = full_dataset.shuffle(buffer_size=BUFFER_SIZE, seed=SEED)\n",
    "train_dataset = full_dataset.take(TRAIN_SIZE)\n",
    "val_dataset = full_dataset.skip(TRAIN_SIZE)\n",
    "    \n",
    "# Creating d1ict pairs linking images and annotations\n",
    "train_dataset = train_dataset.map(parse_image)\n",
    "val_dataset = val_dataset.map(parse_image)\n",
    "\n",
    "showDataImgs(train_dataset, val_dataset, 3)\n",
    "\n",
    "# -- Train Dataset --# - https://stackoverflow.com/questions/49915925/output-differences-when-changing-order-of-batch-shuffle-and-repeat\n",
    "train_dataset = train_dataset.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=BUFFER_SIZE, seed=SEED)\n",
    "train_dataset = train_dataset.repeat()\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "#-- Validation Dataset --#\n",
    "val_dataset = val_dataset.map(load_image_val, num_parallel_calls=AUTOTUNE)\n",
    "val_dataset = val_dataset.repeat()\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"../dataset/training/vectors/masks/mask_binary.png\")\n",
    "data = np.asarray(image)\n",
    "print(type(data))\n",
    "print(data.shape)\n",
    "print(np.unique(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for example in train_dataset.take(1):  # example is `{'image': tf.Tensor, 'label': tf.Tensor}`\n",
    "#for example in next(val_dataset_np):\n",
    "  #print(example)\n",
    "  #image = example[0]\n",
    "  #label = example[1]\n",
    "  #label = tf.cast(label, tf.float32) / 255.0 #normalizing label  \n",
    "  #print((image==label).all())\n",
    "  #print(image.size)\n",
    "#  image_img = Image.fromarray(image, 'RGB')\n",
    " #\n",
    "  #plt.imshow(image)\n",
    "  #plt.show()\n",
    "#   image_img.save('image_test.png')\n",
    "#   image_img.show()\n",
    "\n",
    "#  label_img = Image.fromarray(label, 'RGB')\n",
    "  #label= np.reshape(label, (256,256))\n",
    "  #plt.imshow(label)\n",
    "  #plt.show()\n",
    "#   label_img.save('label_test.png')\n",
    "#   label_img.show()\n",
    "\n",
    "# for example in val_dataset.take(2):  # example is `{'image': tf.Tensor, 'label': tf.Tensor}`\n",
    "#   #print(example)\n",
    "#   #image = example[0]\n",
    "#   #label = example[1]\n",
    "#   print(image.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(backbone, weight_file, vector_files, raster_files):\n",
    "\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    print(f\"Tensorflow ver. {tf.__version__}\")\n",
    "\n",
    "    # For reproducibility\n",
    "    SEED = 42\n",
    "\n",
    "    # Relevant directories/files\n",
    "    image_dir = \"../dataset/testing/images\"\n",
    "    annotation_dir = \"../dataset/testing/annotations\"\n",
    "    out_dir = \"../dataset/testing/output\"\n",
    "    testing_data = \"../dataset/testing/\"\n",
    "    model_weights = weight_file\n",
    "\n",
    "    #Listing GPU info\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "       try:\n",
    "           for gpu in gpus:\n",
    "               tf.config.experimental.set_memory_growth(gpu, True)\n",
    "           logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "           print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "       except RuntimeError as e:\n",
    "           print(e)\n",
    "\n",
    "    # Hyperparams\n",
    "    BATCH_SIZE = 16\n",
    "    BUFFER_SIZE = 1000 # See https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle\n",
    "\n",
    "    model = sm.Unet(\n",
    "        #'vgg16', \n",
    "        backbone,\n",
    "        input_shape=(HEIGHT, WIDTH, N_CHANNELS), \n",
    "        encoder_weights='imagenet', \n",
    "        weights=model_weights,\n",
    "        encoder_freeze=True,    # only training decoder network\n",
    "        classes= NUM_CLASSES, \n",
    "        activation='sigmoid'\n",
    "    )\n",
    "\n",
    "    # Might be unnecessary\n",
    "    model.compile(\n",
    "        'Adam', \n",
    "        #loss=sm.losses.bce_jaccard_loss, \n",
    "        loss=LOSS_FUNC,\n",
    "        metrics=[sm.metrics.iou_score]\n",
    "    )\n",
    "    \n",
    "    test_dataset = glob(os.path.join(image_dir, \"*.jpg\"))\n",
    "\n",
    "    #Loop for inference\n",
    "    print(\"\\nStarting inference... \\n\")\n",
    "    for img_file in tqdm(test_dataset):\n",
    "        tif_file = img_file.replace(\"jpg\", \"tif\")\n",
    "\n",
    "        img = np.asarray(Image.open(img_file)) / 255.0 # normalization (not) needed as we dont normalize the img for training\n",
    "        img = img[np.newaxis, ...] # needs (batch_size, height, width, channels)\n",
    "        pred_mask = model.predict(img)[0]\n",
    "        pred_mask = create_mask(pred_mask)\n",
    "        pred_mask = np.array(pred_mask).astype('uint8') * 255\n",
    "        #print(pred_mask)\n",
    "        # Reading metadata from .tif\n",
    "        with rasterio.open(tif_file) as src:\n",
    "            tif_meta = src.meta\n",
    "            tif_meta['count'] = 1\n",
    "\n",
    "        # Writing prediction mask as a .tif using extracted metadata\n",
    "        mask_file = tif_file.replace(\"images\", \"output\")\n",
    "        \n",
    "        with rasterio.open(mask_file, \"w\", **tif_meta) as dest:\n",
    "            # Rasterio needs [bands, width, height]\n",
    "            pred_mask = np.rollaxis(pred_mask, axis=2)\n",
    "            dest.write(pred_mask)\n",
    "\t#printing out metrics\n",
    "\t#results = model.evaluate(img, pred_mask, batch_size=128)\n",
    "\t#print(\"IOU: \", results) \n",
    "    print(\"Merging tiles (to create mask ortho)...\")\n",
    "    call = \"gdal_merge.py -o \" + testing_data + \"ortho_mask.tif \" + \" \" + out_dir + \"/*\"\n",
    "    print(call)\n",
    "    subprocess.call(call, shell=True)\n",
    "    \n",
    "#     print(\"Creating raster_masks...\")\n",
    "#     vector_file = vector_files[0]\n",
    "#     raster_file = raster_files[0]\n",
    "#     raster_mask(raster_files[0], vector_files[0])\n",
    "#     temp_dir = os.path.dirname(vector_file)\n",
    "#     mask_file = os.path.join(temp_dir, \"masks\", \"mask_binary.tif\")\n",
    "    \n",
    "#     #out_dir = os.path.dirname(raster_file)\n",
    "#     gen_seg_labels(out_width, raster_file, vector_file, mask_file, image_dir, True, True)\n",
    "    \n",
    "    \n",
    "#     test_dataset = tf.data.Dataset.list_files(testing_data + \"images/*.jpg\", seed=SEED)\n",
    "#     test_dataset = test_dataset.map(parse_image)\n",
    "#     test_dataset = test_dataset.map(load_image_val, num_parallel_calls=AUTOTUNE)\n",
    "#     test_dataset = test_dataset.batch(16)\n",
    "    \n",
    "#     map_file = os.path.join(image_dir, \"map.txt\")\n",
    "#     create_seg_dataset(map_file, \"testing\", 0)\n",
    "#     logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "#     tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, update_freq='epoch')\n",
    "#     print(\"Evaluating...\")\n",
    "#     model.evaluate(test_dataset, \n",
    "#                   callbacks=tensorboard_callback\n",
    "#                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_optimized(backbone, weight_file):\n",
    "    '''\n",
    "    Note: This version of test does not work yet. It is optimized to be very efficient and works well for inference on .jpg files.\n",
    "    It lacks the capabilities to link the output predictions to the input .jpgs since the filenames are lost when in the tf.dataset\n",
    "    we map the parse image function. As a result, we need to somehow modify this dataset to retain filename information so we can use it\n",
    "    to link the output prediction to the input image and its corresponding .tif file, which will be used to write the geospatial info to\n",
    "    the prediction.\n",
    "\n",
    "    Initial ideas would be to modify the parse image function and related functions to save filename info, and use this to link the images\n",
    "    in the prediction stage by replacing .jpg with .tif in the filename.\n",
    "    '''\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    print(f\"Tensorflow ver. {tf.__version__}\")\n",
    "\n",
    "    # For reproducibility\n",
    "    SEED = 42\n",
    "\n",
    "    # Relevant directories/files\n",
    "    images = \"../dataset/testing/images\"\n",
    "    annotations = \"../dataset/testing/annotations\"\n",
    "    testing_data = \"../dataset/testing/\"\n",
    "    #model_weights = \"unet_500_weights_vgg16.h5\"\n",
    "    model_weights = weight_file\n",
    "\n",
    "    # Listing GPU info\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "\n",
    "    # Hyperparams\n",
    "    BATCH_SIZE = 16\n",
    "    BUFFER_SIZE = 1000 # See https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle\n",
    "\n",
    "    model = sm.Unet(\n",
    "        'resnet34', \n",
    "        input_shape=(HEIGHT, WIDTH, N_CHANNELS), \n",
    "        encoder_weights='imagenet', \n",
    "        weights=model_weights,\n",
    "        encoder_freeze=True,    # only training decoder network\n",
    "        classes=2, \n",
    "        activation='softmax'\n",
    "    )\n",
    "\n",
    "    # Might be unnecessary\n",
    "    model.compile(\n",
    "        'Adam', \n",
    "        loss=LOSS_FUNC, \n",
    "        metrics=[sm.metrics.iou_score]\n",
    "    )\n",
    "\n",
    "    test_dataset = tf.data.Dataset.list_files(testing_data + \"images/*.jpg\", seed=SEED)\n",
    "    test_dataset = test_dataset.map(parse_image)\n",
    "    test_dataset = test_dataset.map(load_image_val, num_parallel_calls=AUTOTUNE)\n",
    "    test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "    test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    image_data = []\n",
    "    annotation_data = []\n",
    "    \n",
    "    '''\n",
    "    for img_file in tqdm(os.listdir(images)): \n",
    "        annotation_file = \"annotation_\" + img_file.split('_')[1]\n",
    "        img_file = os.path.join(images, img_file)\n",
    "        ann_file = os.path.join(annotations, annotation_file)\n",
    "        image = np.array(Image.open(img_file))\n",
    "        annotation = np.array(Image.open(ann_file))\n",
    "        image_data.append(image)\n",
    "        annotation_data.append(annotation)\n",
    "    '''\n",
    "\n",
    "\n",
    "    #prediction = model.predict(test_dataset, steps=1)\n",
    "    #print(type(prediction))\n",
    "\n",
    "\n",
    "    #display([first_image[0], first_mask[0], create_mask(first_pred_mask)])\n",
    "\n",
    "    #pred_mask = model.predict(test_dataset)\n",
    "    #display([image[0], mask[0], create_mask(pred_mask)])\n",
    "\n",
    "    show_predictions(model=model, dataset=test_dataset, num=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-8b43b0fc933f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#resampled_1, transform = raster.downsample_raster(img_1, ds_factor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#resampled_10, transform = raster.downsample_raster(img_10, ds_factor)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#for f in os.listdir(\"../dataset/training/annotations\"):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/rasterio/plot.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(source, with_bounds, contour, contour_label_kws, ax, title, transform, adjust, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \"\"\"\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/ipykernel/pylab/backend_inline.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(close, block)\u001b[0m\n\u001b[1;32m     41\u001b[0m             display(\n\u001b[1;32m     42\u001b[0m                 \u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fetch_figure_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigure_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             )\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(include, exclude, metadata, transient, display_id, *objs, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-9>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mFigureCanvasBase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2089\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2090\u001b[0m                     \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2091\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2092\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2093\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m             \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m             \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mRendererAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m             \u001b[0;31m# A GUI class may be need to update a window using this draw, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;31m# don't forget to call the superclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1707\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[0;32m-> 1709\u001b[0;31m                 renderer, self, artists, self.suppressComposite)\n\u001b[0m\u001b[1;32m   1710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2647\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2649\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m             im, l, b, trans = self.make_image(\n\u001b[0;32m--> 619\u001b[0;31m                 renderer, renderer.get_image_magnification())\n\u001b[0m\u001b[1;32m    620\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mmake_image\u001b[0;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[1;32m    879\u001b[0m         return self._make_image(\n\u001b[1;32m    880\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_bbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagnification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m             unsampled=unsampled)\n\u001b[0m\u001b[1;32m    882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_unsampled_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_make_image\u001b[0;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[1;32m    484\u001b[0m                 \u001b[0;31m# Always convert to RGBA, even if only RGB input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m                     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_rgb_to_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m                     raise ValueError(\"Invalid shape {} for image data\"\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_rgb_to_rgba\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \"\"\"\n\u001b[1;32m    168\u001b[0m     \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0mrgba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrgba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mrgba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# #view downsmapled testing output ortho_mask.tif\n",
    "# # !sudo apt-get update\n",
    "# # !sudo apt-get install libgdal-dev -y\n",
    "# # !sudo apt-get install python-gdal -y\n",
    "# # !sudo apt-get install python-numpy python-scipy -y\n",
    "# # !pip install rasterio\n",
    "# # !pip install fiona\n",
    "# # !pip install geopandas\n",
    "# # !pip install -i https://test.pypi.org/simple/ gis-utils-pkg-dillhicks==0.0.1\n",
    "# from gis_utils import raster\n",
    "# from rasterio.plot import show\n",
    "\n",
    "# #img_1, meta2 = raster.load_image(\"../\")\n",
    "# img_10, meta1 = raster.load_image(\"../dataset/training/vectors/masks/mask_binary.png\")\n",
    "# #img_1, meta10 = raster.load_image(\"../dataset/training/vectors/masks/mask.tif\")\n",
    "\n",
    "# #downsampling images \n",
    "# ds_factor = 1\n",
    "# #resampled_1, transform = raster.downsample_raster(img_1, ds_factor)\n",
    "# #resampled_10, transform = raster.downsample_raster(img_10, ds_factor)\n",
    "# show(img_10)\n",
    "\n",
    "# #for f in os.listdir(\"../dataset/training/annotations\"):\n",
    "#     #image = image.imRead(\"../dataset/training/annotations/\" + f)\n",
    "#     #show(image)\n",
    "#     #pyplot.imshow(image)\n",
    "#     #pyplot.show()\n",
    "#     #print(np.asarray(image))\n",
    "#     #img, meta = raster.load_image(\"../dataset/training/annotations/\" + f)\n",
    "#     #show(img)\n",
    "#     #print(np.asarray(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_setup(raster_files, vector_files, out_width):\n",
    "    # Uses raster and vector file to create dataset for training\n",
    "    data_files = zip(raster_files, vector_files)\n",
    "    map_files = [] \n",
    "    folderpath = \"../dataset/training/\"\n",
    "    for raster_file, vector_file in data_files:\n",
    "        # Generates raster masks\n",
    "        print(\"Creating raster_masks...\")\n",
    "        raster_mask(raster_file, vector_file)\n",
    "        temp_dir = os.path.dirname(vector_file)\n",
    "        mask_file = os.path.join(temp_dir, \"masks\", \"mask_binary.tif\")\n",
    "\n",
    "        # Generates segmentation labels\n",
    "        out_dir = os.path.dirname(raster_file)\n",
    "        gen_seg_labels(out_width, raster_file, vector_file, mask_file, out_dir, True, True)\n",
    "        map_file = os.path.join(out_dir, \"map.txt\")\n",
    "        map_files.append(map_file)\n",
    "        \n",
    "        #show downsampled raster_mask\n",
    "        img_1, meta1 = raster.load_image(\"../dataset/training/vectors/masks/mask.tif\")\n",
    "        resampled_1, transform = raster.downsample_raster(img_1, 1/5)\n",
    "        show(resampled_1)\n",
    "        \n",
    "        shutil.rmtree(folderpath + \"vectors/masks\")\n",
    "        shutil.rmtree(folderpath + \"vectors/nm\")  #removng directories\n",
    "        shutil.rmtree(folderpath + \"vectors/m\")\n",
    "\n",
    "    # Creating dataset to train UNet\n",
    "    create_seg_dataset(map_files, \"training\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_setup(raster_files, out_width):\n",
    "    out_dir = \"../dataset/testing/output\"\n",
    "    test_dir = \"../dataset/testing\"\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    print(\"\\nTiling rasters...\")\n",
    "    for raster_file in raster_files:\n",
    "        tile_raster(out_width, raster_file, test_dir, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initalize arguments\n",
    "img_p = \"../dataset/training/images/\"\n",
    "vec_p = \"../dataset/training/vectors/\"\n",
    "\n",
    "weight_file = \"../dataset/training/weights/07_23_vgg16_50_01_03_05_06_rounded_weight.h5\"\n",
    "backbone = \"vgg16\"\n",
    "out_width = \"256\"\n",
    "\n",
    "#raster_files = [\"../dataset/training/images/lap_2018-07_site04_120m_RGB_cc.tif\", \"../dataset/training/images/lap_2019-07_site06_120m_RGB_quick.tif\"]\n",
    "#vector_files = [\"../dataset/training/vectors/lap_2018-07_site04_labels_m-nm.shp\"]#, \"../dataset/training/vectors/lap_2019-07_site06_labels_m-nm.shp\"]\n",
    "raster_files = [img_p + \"lap_2018-07_site1_120m_RGB_cc.tif\", \n",
    "                img_p + \"lap_2019-07_site03_120m_RGB_quick.tif\",\n",
    "                img_p + \"lap_2018-07_site05_120m_RGB_cc.tif\",\n",
    "                img_p + \"lap_2019-07_site06_120m_RGB_quick.tif\",\n",
    "                img_p + \"lap_2018-07_site04_120m_RGB_cc.tif\",\n",
    "                img_p + \"lap_2018-07_site06_120m_RGB_cc.tif\",\n",
    "                img_p + \"psc_2018-05_site01_120m_RGB_cc.tif\",\n",
    "                img_p + \"psc_2018-05_site11_120m_RGB.tif\",\n",
    "                img_p + \"psc_2018-05_site12_120m_RGB.tif\",\n",
    "                img_p + \"psc_2018-05_site8.tif\",\n",
    "                img_p + \"psc_2018-07_site08_120m_RGB.tif\",\n",
    "                img_p + \"psc_2018-07_site11_120m_RGB.tif\",\n",
    "                img_p + \"psc_2018-07_site10_120m_RGB.tif\",\n",
    "                img_p + \"psc_2018-07_site09_120m_RGB.tif\",\n",
    "                img_p + \"psc_2018-05_site13-14_120m_RGB.tif\",\n",
    "               ]\n",
    "\n",
    "vector_files = [vec_p + \"lap_2018-07_site01_labels_m-nm.shp\", \n",
    "                vec_p + \"lap_2019-07_site03_labels_m-nm.shp\", \n",
    "                vec_p + \"lap_2018-07_site05_120m_m-nm_dissolve.shp\",\n",
    "                vec_p + \"lap_2019-07_site06_120m_labels_m-nm.shp\",\n",
    "                vec_p + \"lap_2018-07_site04_labels_m-nm.shp\",\n",
    "                vec_p + \"lap_2018-07_site06_120m_RGB_m-nm.shp\",\n",
    "                vec_p + \"psc_2018-05_site01_120m_RGB_cc labels_m-nm.shp\",\n",
    "                vec_p + \"psc_2018-05_site11_120m_RGB_dissolved.shp\",\n",
    "                vec_p + \"psc_2018-05_site12_labels_m-nm.shp\",\n",
    "                vec_p + \"psc_2018-05_site8_labels_m-nm.shp\",\n",
    "                vec_p + \"psc_2018-07_site08_120m_RGB_labels_m-nm.shp\",\n",
    "                vec_p + \"psc_2018-07_site11_120m_RGB_m-nm.shp\",\n",
    "                vec_p + \"psc_2018-07_site_10_labels_m-nm.shp\",\n",
    "                vec_p + \"psc_201807_site9_mnm.shp\",\n",
    "                vec_p + \"psc_2018_05_site1314_120m_mnm.shp\",\n",
    "               ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_setup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7234e0392685>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Run Testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraster_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraster_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_setup' is not defined"
     ]
    }
   ],
   "source": [
    "#Run Testing\n",
    "test_setup(raster_files, out_width)\n",
    "test(backbone, weight_file, vector_files, raster_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raster_masks...\n",
      "Splitting vectors...\n",
      "Creating mangrove files in ../dataset/training/vectors/m\n",
      "Joined 1 mangrove polygons.\n",
      "Creating nonmangrove files in ../dataset/training/vectors/nm\n",
      "Joined 1 nonmangrove polygons.\n",
      "Reading shapefile...\n",
      "Creating masks...\n",
      "Saving masks...\n",
      "Done.\n",
      "\n",
      "Executing GDAL calls...\n",
      "gdal_retile.py -ps 256 256 -targetDir ../dataset/training/images/images ../dataset/training/images/lap_2018-07_site1_120m_RGB_cc.tif\n",
      "gdal_retile.py -ps 256 256 -targetDir ../dataset/training/images/labels ../dataset/training/vectors/masks/mask_binary.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/8148 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing undersized tiles...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 8148/8148 [00:11<00:00, 735.16it/s]\n",
      "  3%|         | 204/5934 [00:00<00:03, 1727.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Images: 5934\n",
      "Number of Labels: 5934\n",
      "Converting images from .tif to .jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5934/5934 [00:18<00:00, 319.95it/s] \n",
      "  0%|          | 0/5934 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting labels from .tif to .jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5934/5934 [00:25<00:00, 236.71it/s]\n",
      "100%|| 5934/5934 [00:00<00:00, 61326.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Map...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'raster' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-c424a217fea2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Run Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraster_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-f45c54cf4d60>\u001b[0m in \u001b[0;36mtrain_setup\u001b[0;34m(raster_files, vector_files, out_width)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m#show downsampled raster_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mimg_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../dataset/training/vectors/masks/mask.tif\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mresampled_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample_raster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresampled_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'raster' is not defined"
     ]
    }
   ],
   "source": [
    "#Run Training\n",
    "train_setup(raster_files, vector_files, out_width)\n",
    "train(backbone, weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     TRAIN = False\n",
    "#     TEST = False\n",
    "\n",
    "#     parser = argparse.ArgumentParser(description=\"UNet Training and Inference Script (Note: order of rasters and vectors must correspond to one another)\")\n",
    "#     parser.add_argument(\"--width\",help = \"Width of output tiles\")\n",
    "#     parser.add_argument(\"--input_rasters\", nargs='*', help = \"space separated input orthomosaic (.tif)\")\n",
    "#     parser.add_argument(\"--input_vectors\", nargs='*', help = \"space separated input labels (.shp)\")\n",
    "#     parser.add_argument(\"--train\", action='store_true', help = \"training UNet\")\n",
    "#     parser.add_argument(\"--test\", action='store_true', help = \"testing UNet\")\n",
    "#     parser.add_argument(\"--weights\", help = \"path to weight file, either to save or use (.h5)\")\n",
    "#     parser.add_argument(\"--backbone\", help = \"segmentation model backbone, ex: resnet34, vgg16, etc.\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     # Parsing arguments\n",
    "#     if args.width:\n",
    "#         out_width = args.width\n",
    "#     else:\n",
    "#         print(\"Need to specify width, exiting.\")\n",
    "#         exit()\n",
    "#     if args.input_rasters:\n",
    "#         raster_files = args.input_rasters\n",
    "#     else:\n",
    "#         # Always needs a raster\n",
    "#         print(\"Need to specify raster file, exiting.\")\n",
    "#         exit()\n",
    "#     if args.input_vectors:\n",
    "#         vector_files = args.input_vectors\n",
    "#     else:\n",
    "#         # Requires vector labes for training, not inference\n",
    "#         if args.train:\n",
    "#             print(\"Need to specify input vector, exiting.\")\n",
    "#             exit()\n",
    "#     if args.train and args.test:\n",
    "#         print(\"Can't train and test at the same time... exiting.\")\n",
    "#         exit()\n",
    "#     elif args.train:\n",
    "#         TRAIN = True\n",
    "#     elif args.test:\n",
    "#         TEST = True\n",
    "#     if args.weights:\n",
    "#         weight_file = args.weights\n",
    "#     else:\n",
    "#         print(\"Need weight file, exiting.\")\n",
    "#         exit()\n",
    "#     if args.backbone:\n",
    "#         backbone = args.backbone\n",
    "#     else:\n",
    "#         print(\"Need to specify backbone, exiting.\")\n",
    "#         exit()\n",
    "\n",
    "#     # Selecting mode\n",
    "#     if TRAIN: \n",
    "#         train_setup(raster_files, vector_files, out_width)\n",
    "#         train(backbone, weight_file)\n",
    "#     if TEST:\n",
    "#         test_setup(raster_files, out_width)\n",
    "#         test(backbone, weight_file)\n",
    "#         #test_optimized(backbone, weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "#delete all training/images, /images/images, /images/labels, images/masks, \n",
    "#images/m, images/nm, anotations\n",
    "\n",
    "#delete all files with extension\n",
    "folderpath = \"../dataset/training/\"\n",
    "for file_name in os.listdir(folderpath + \"images/images\"):\n",
    "    if file_name.endswith('.jpg'):\n",
    "        os.remove(folderpath + \"images/images/\" + file_name)\n",
    "for file_name in os.listdir(folderpath + \"images\"):\n",
    "    if file_name.endswith('.jpg'):\n",
    "        os.remove(folderpath + \"images/\" + file_name)\n",
    "for file_name in os.listdir(folderpath + \"images/labels\"):\n",
    "    if file_name.endswith('.jpg'):\n",
    "        os.remove(folderpath + \"images/labels/\" + file_name)\n",
    "# shutil.rmtree(\"../dataset/testing/output\")\n",
    "# shutil.rmtree(\"../dataset/testing/images\")\n",
    "shutil.rmtree(folderpath + \"vectors/masks\")\n",
    "shutil.rmtree(folderpath + \"vectors/nm\")  #removng directories\n",
    "shutil.rmtree(folderpath + \"vectors/m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_tensorflow",
   "language": "python",
   "name": "conda-env-py37_tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
