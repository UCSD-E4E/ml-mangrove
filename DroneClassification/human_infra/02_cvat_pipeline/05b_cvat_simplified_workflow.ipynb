{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28995067",
   "metadata": {},
   "source": [
    "# Model-Assisted Labeling (MAL) Workflow with DeepLab & CVAT (Later ML Paint for comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e7d3e",
   "metadata": {},
   "source": [
    "# Installing CVAT on Windows\n",
    "\n",
    "## Step 1: Install Git for Windows\n",
    "\n",
    "1. Download Git for Windows from [https://gitforwindows.org/](https://gitforwindows.org/).\n",
    "2. Install Git, keeping all options as default.\n",
    "3. Open the command prompt (`cmd`) and type the following command to check the Git version:\n",
    "    ```bash\n",
    "    git --version\n",
    "    ```\n",
    "\n",
    "## Step 2: Install Docker Desktop for Windows\n",
    "\n",
    "1. Download [Docker Desktop for Windows](https://desktop.docker.com/win/main/amd64/Docker%20Desktop%20Installer.exe).\n",
    "2. Double-click the Docker for Windows Installer to run the installer.\n",
    "3. Follow the instructions for installation, and reboot the system after installation is complete.\n",
    "4. Open the command prompt and check the Docker version:\n",
    "    ```bash\n",
    "    docker --version\n",
    "    ```\n",
    "5. Check the Docker Compose version:\n",
    "    ```bash\n",
    "    docker compose version\n",
    "    ```\n",
    "\n",
    "## Step 3: Install Google Chrome\n",
    "\n",
    "1. Download and install [Google Chrome](https://www.google.com/chrome/), as it is the only browser supported by CVAT.\n",
    "\n",
    "## Step 4: Clone CVAT Source Code\n",
    "\n",
    "1. Clone CVAT source code from the [GitHub repository](https://github.com/opencv/cvat):\n",
    "    ```bash\n",
    "    git clone https://github.com/opencv/cvat\n",
    "    cd cvat\n",
    "    ```\n",
    "2. Alternatively, check [alternatives](https://opencv.github.io/cvat/docs/administration/basics/installation/#how-to-get-cvat-source-code) for downloading specific release versions.\n",
    "\n",
    "## Step 5: Run Docker Containers for CVAT\n",
    "\n",
    "1. Run the following command to start Docker containers. This will download the latest CVAT release and other required images:\n",
    "    ```bash\n",
    "    docker compose up -d\n",
    "    ```\n",
    "2. Optionally, specify the CVAT version using the CVAT_VERSION environment variable:\n",
    "    ```bash\n",
    "    CVAT_VERSION=dev docker compose up -d\n",
    "    ```\n",
    "3. Check the status of the containers:\n",
    "    ```bash\n",
    "    docker ps\n",
    "    ```\n",
    "4. Wait until the CVAT server is up and running:\n",
    "    ```bash\n",
    "    docker logs cvat_server -f\n",
    "    ```\n",
    "5. Run the CVAT server:\n",
    "    ```bash\n",
    "    docker exec -it cvat_server bash\n",
    "    ```\n",
    "6. For the first-time setup, create a superuser account:\n",
    "    ```bash\n",
    "    python3 manage.py createsuperuser\n",
    "    ```\n",
    "    Choose a username and password for the admin account.\n",
    "\n",
    "## Step 6: Access CVAT in Google Chrome\n",
    "\n",
    "1. Open Google Chrome and go to `localhost:8080`.\n",
    "2. Log in with the superuser credentials created earlier.\n",
    "3. You should now be able to create a new annotation task.\n",
    "\n",
    "## Workflow\n",
    "To stop and remove the container, simply type, \n",
    "```bash\n",
    "docker compose down\n",
    "```\n",
    "And to start cvat again, simply type \n",
    "```bash\n",
    "docker compose up -d\n",
    "```\n",
    "Make sure you're in the correct directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b3963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import shutil\n",
    "from typing import Dict, List\n",
    "\n",
    "# Add project to path\n",
    "sys.path.insert(0, '../../')\n",
    "from models import DeepLab\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "# Configuration\n",
    "CLASS_NAMES = ['background', 'building', 'woodland', 'water', 'road']\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "TILE_SIZE = 512\n",
    "\n",
    "# Define workspace directories\n",
    "WORKSPACE_DIR = Path(\"mal_workspace\")\n",
    "TILES_DIR = WORKSPACE_DIR / \"01_tiles\"\n",
    "PREDICTIONS_DIR = WORKSPACE_DIR / \"02_predictions\"\n",
    "CORRECTED_MASKS_DIR = WORKSPACE_DIR / \"03_corrected_masks\"\n",
    "RECONSTRUCTED_DIR = WORKSPACE_DIR / \"04_reconstructed\"\n",
    "\n",
    "print(f\"‚úì Setup complete | Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e87b020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== UPDATE THESE PATHS =====\n",
    "INPUT_GEOTIFF = \"cvat_test/images/M-33-7-A-d-2-3.tif\"  # Change this path as needed\n",
    "CHECKPOINT_PATH = \"experiments/Deeplab_Landcover_Edited/best_model.pth\"  # Change this to your model\n",
    "# ==============================\n",
    "\n",
    "# Verify files exist\n",
    "if not Path(INPUT_GEOTIFF).exists():\n",
    "    print(f\"‚ö†Ô∏è  WARNING: {INPUT_GEOTIFF} not found!\")\n",
    "else:\n",
    "    print(f\"‚úì Input GeoTIFF: {INPUT_GEOTIFF}\")\n",
    "\n",
    "if not Path(CHECKPOINT_PATH).exists():\n",
    "    print(f\"‚ö†Ô∏è  WARNING: {CHECKPOINT_PATH} not found!\")\n",
    "else:\n",
    "    print(f\"‚úì Checkpoint: {CHECKPOINT_PATH}\")\n",
    "\n",
    "print(f\"\\nWorkspace: {WORKSPACE_DIR}\")\n",
    "print(f\"Classes: {CLASS_NAMES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a2e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path: str) -> DeepLab:\n",
    "    \"\"\"Load trained DeepLab model.\"\"\"\n",
    "    model = DeepLab(\n",
    "        num_classes=NUM_CLASSES,\n",
    "        input_image_size=TILE_SIZE,\n",
    "        backbone='resnet50',\n",
    "        output_stride=4\n",
    "    ).to(device)\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    if isinstance(checkpoint, dict) and 'model' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Ensure all dropout and batch norm layers are in eval mode\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, (nn.Dropout, nn.BatchNorm2d)):\n",
    "            module.eval()\n",
    "    \n",
    "    print(f\"‚úì Model loaded from {Path(checkpoint_path).name}\\n\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def normalize_image(image: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\"Normalize image to [-2, 2] range using ImageNet stats.\"\"\"\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    image = torch.from_numpy(image).permute(2, 0, 1)\n",
    "    \n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    image = (image - mean) / std\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "def predict_single_tile(model: DeepLab, image_path: str, debug: bool = False) -> np.ndarray:\n",
    "    \"\"\"Generate prediction mask for one tile.\"\"\"\n",
    "    image = np.array(Image.open(image_path).convert('RGB'))\n",
    "    image_tensor = normalize_image(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Output shape: {output.shape}\")\n",
    "            print(f\"Output min/max: {output.min():.4f} / {output.max():.4f}\")\n",
    "            print(f\"Class logits sample (center pixel): {output[0, :, 256, 256]}\")\n",
    "        \n",
    "        mask = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Mask unique classes: {np.unique(mask)}\\n\")\n",
    "    \n",
    "    return mask.astype(np.uint8)\n",
    "\n",
    "\n",
    "def generate_all_predictions(model: DeepLab, tiles_dir: str, output_dir: str) -> List[str]:\n",
    "    \"\"\"Generate predictions for all tiles in a directory.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    tile_files = sorted([f for f in os.listdir(tiles_dir) if f.endswith('.png')])\n",
    "    print(f\"Generating predictions for {len(tile_files)} tiles...\")\n",
    "    \n",
    "    # Test first tile with debug info\n",
    "    if tile_files:\n",
    "        first_tile = Path(tiles_dir) / tile_files[0]\n",
    "        print(f\"\\nüîç Testing first tile: {tile_files[0]}\")\n",
    "        test_mask = predict_single_tile(model, str(first_tile), debug=True)\n",
    "    \n",
    "    prediction_paths = []\n",
    "    for tile_file in tqdm(tile_files, desc=\"Predicting\"):\n",
    "        tile_path = Path(tiles_dir) / tile_file\n",
    "        pred_mask = predict_single_tile(model, str(tile_path))\n",
    "        \n",
    "        pred_name = tile_file.replace('.png', '_pred.png')\n",
    "        pred_path = Path(output_dir) / pred_name\n",
    "        \n",
    "        Image.fromarray(pred_mask).save(pred_path)\n",
    "        prediction_paths.append(pred_path)\n",
    "    \n",
    "    print(f\"‚úì Generated {len(prediction_paths)} predictions\\n\")\n",
    "    return prediction_paths\n",
    "\n",
    "print(\"‚úì Inference functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbd57f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(tiles_dir: str, predictions_dir: str, num_samples: int = 4):\n",
    "    \"\"\"\n",
    "    Display sample tiles with predictions.\n",
    "    \n",
    "    Shows: Original Image | Predicted Mask | Overlay\n",
    "    \"\"\"\n",
    "    tile_files = sorted([f for f in os.listdir(tiles_dir) if f.endswith('.png')])\n",
    "    indices = np.random.choice(len(tile_files), min(num_samples, len(tile_files)), replace=False)\n",
    "    \n",
    "    class_colors = {\n",
    "        0: [0.0, 0.0, 0.0],       # background - black\n",
    "        1: [1.0, 0.0, 0.0],       # building - red\n",
    "        2: [0.0, 0.5, 0.0],       # woodland - green\n",
    "        3: [0.0, 0.0, 1.0],       # water - blue\n",
    "        4: [1.0, 1.0, 0.0],       # road - yellow\n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(len(indices), 3, figsize=(14, 4*len(indices)))\n",
    "    fig.suptitle('Sample Predictions (Image | Mask | Overlay)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    for row, idx in enumerate(indices):\n",
    "        tile_file = tile_files[idx]\n",
    "        tile_path = Path(tiles_dir) / tile_file\n",
    "        pred_file = tile_file.replace('.png', '_pred.png')\n",
    "        pred_path = Path(predictions_dir) / pred_file\n",
    "        \n",
    "        image = np.array(Image.open(tile_path))\n",
    "        pred_mask = np.array(Image.open(pred_path))\n",
    "        \n",
    "        # Column 1: Image\n",
    "        axes[row, 0].imshow(image)\n",
    "        axes[row, 0].set_title(f'Image', fontsize=10)\n",
    "        axes[row, 0].axis('off')\n",
    "        \n",
    "        # Column 2: Mask\n",
    "        axes[row, 1].imshow(pred_mask, cmap='tab10', vmin=0, vmax=4)\n",
    "        axes[row, 1].set_title(f'Mask (classes: {np.unique(pred_mask)})', fontsize=10)\n",
    "        axes[row, 1].axis('off')\n",
    "        \n",
    "        # Column 3: Overlay\n",
    "        mask_rgb = np.zeros((*pred_mask.shape, 3))\n",
    "        for class_id, color in class_colors.items():\n",
    "            mask_rgb[pred_mask == class_id] = color\n",
    "        overlay = 0.65 * (image / 255.0) + 0.35 * mask_rgb\n",
    "        axes[row, 2].imshow(overlay)\n",
    "        axes[row, 2].set_title('Overlay', fontsize=10)\n",
    "        axes[row, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4499d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cvat_labelmap(labelmap_txt_path: str) -> Dict[int, tuple]:\n",
    "    \"\"\"\n",
    "    Extract color mapping from labelmap.txt file.\n",
    "    \n",
    "    Args:\n",
    "        labelmap_txt_path: Path to labelmap.txt\n",
    "    \n",
    "    Returns:\n",
    "        Dict mapping class_id to RGB tuple: {0: (0, 0, 0), 1: (250, 50, 83), ...}\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    labelmap_path = Path(labelmap_txt_path)\n",
    "    \n",
    "    if not labelmap_path.exists():\n",
    "        print(f\"‚ö†Ô∏è  Warning: {labelmap_path} not found\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(labelmap_path, 'r') as f:\n",
    "            labelmap_content = f.read()\n",
    "        \n",
    "        color_map = {}\n",
    "        class_names = ['background', 'building', 'road', 'water', 'woodland']\n",
    "        \n",
    "        for line in labelmap_content.strip().split('\\n'):\n",
    "            if not line or line.startswith('#'):\n",
    "                continue\n",
    "            \n",
    "            # Parse: \"label:color_rgb:parts:actions\"\n",
    "            parts = line.split(':')\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "            \n",
    "            label = parts[0].strip()\n",
    "            color_str = parts[1].strip()\n",
    "            \n",
    "            # Parse color: \"R,G,B\"\n",
    "            try:\n",
    "                r, g, b = map(int, color_str.split(','))\n",
    "                if label in class_names:\n",
    "                    class_id = class_names.index(label)\n",
    "                    color_map[class_id] = (r, g, b)\n",
    "                    print(f\"  Loaded: class {class_id} ({label}) ‚Üí RGB({r}, {g}, {b})\")\n",
    "            except ValueError:\n",
    "                continue\n",
    "        \n",
    "        if not color_map:\n",
    "            print(f\"‚ö†Ô∏è  Could not parse any colors from {labelmap_path}\")\n",
    "            return None\n",
    "        \n",
    "        return color_map\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error reading labelmap: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"‚úì Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc76e65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cvat_segmentation_mask_export(\n",
    "    predictions_dir: str,\n",
    "    output_zip: str,\n",
    "    class_names: List[str],\n",
    "    class_colors: Dict[int, tuple] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a CVAT-compatible Segmentation Mask ZIP file for importing predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions_dir: Directory with prediction masks (*_pred.png files)\n",
    "        output_zip: Path to save the ZIP file (e.g., \"mal_workspace/cvat_export.zip\")\n",
    "        class_names: List of class names (e.g., ['background', 'building', 'woodland', 'water', 'road'])\n",
    "        class_colors: Dict mapping class_id to RGB tuple for labelmap.txt\n",
    "                     If None, uses default Pascal VOC colors\n",
    "\n",
    "    Creates this ZIP structure (Required by CVAT. More details check documentation https://docs.cvat.ai/docs/dataset_management/formats/):\n",
    "        archive.zip/\n",
    "        ‚îú‚îÄ‚îÄ labelmap.txt\n",
    "        ‚îú‚îÄ‚îÄ ImageSets/Segmentation/default.txt\n",
    "        ‚îú‚îÄ‚îÄ SegmentationClass/\n",
    "        ‚îÇ   ‚îú‚îÄ‚îÄ tile_00000_00000.png\n",
    "        ‚îÇ   ‚îú‚îÄ‚îÄ tile_00000_00512.png\n",
    "        ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "        ‚îî‚îÄ‚îÄ SegmentationObject/\n",
    "            ‚îú‚îÄ‚îÄ tile_00000_00000.png\n",
    "            ‚îú‚îÄ‚îÄ tile_00000_00512.png\n",
    "            ‚îî‚îÄ‚îÄ ...\n",
    "    \"\"\"\n",
    "    import zipfile\n",
    "    import tempfile\n",
    "\n",
    "    # Default Pascal VOC colors if not provided\n",
    "    if class_colors is None:\n",
    "        class_colors = {\n",
    "            0: (0, 0, 0),           # background - black\n",
    "            1: (128, 0, 0),         # building - maroon\n",
    "            2: (0, 128, 0),         # woodland - dark green\n",
    "            3: (0, 0, 128),         # water - navy blue\n",
    "            4: (128, 128, 0),       # road - olive\n",
    "        }\n",
    "\n",
    "    # Create temporary directory for ZIP contents\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        temp_path = Path(temp_dir)\n",
    "\n",
    "        # Create directory structure\n",
    "        segmentation_class_dir = temp_path / \"SegmentationClass\"\n",
    "        segmentation_object_dir = temp_path / \"SegmentationObject\"\n",
    "        imageset_dir = temp_path / \"ImageSets\" / \"Segmentation\"\n",
    "\n",
    "        segmentation_class_dir.mkdir(parents=True, exist_ok=True)\n",
    "        segmentation_object_dir.mkdir(parents=True, exist_ok=True)\n",
    "        imageset_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # ===== CREATE labelmap.txt =====\n",
    "        labelmap_path = temp_path / \"labelmap.txt\"\n",
    "        with open(labelmap_path, 'w') as f:\n",
    "            for class_id, class_name in enumerate(class_names):\n",
    "                color = class_colors.get(class_id, (0, 0, 0))\n",
    "                f.write(f\"{class_name}:{color[0]},{color[1]},{color[2]}::\\n\")\n",
    "\n",
    "        print(f\"Created labelmap.txt with {len(class_names)} classes\")\n",
    "\n",
    "        # ===== GET PREDICTION FILES =====\n",
    "        pred_files = sorted([f for f in os.listdir(predictions_dir) if f.endswith('.png')])\n",
    "        image_names = []\n",
    "\n",
    "        print(f\"Processing {len(pred_files)} prediction masks...\")\n",
    "\n",
    "        # ===== COPY MASKS TO BOTH DIRECTORIES =====\n",
    "        for pred_file in tqdm(pred_files, desc=\"Copying masks\"):\n",
    "            pred_path = Path(predictions_dir) / pred_file\n",
    "\n",
    "            # Extract base name (remove '_pred.png' or '.png')\n",
    "            if pred_file.endswith('_pred.png'):\n",
    "                base_name = pred_file.replace('_pred.png', '')\n",
    "            else:\n",
    "                base_name = pred_file.replace('.png', '')\n",
    "            \n",
    "            image_names.append(base_name)\n",
    "\n",
    "            # Load mask\n",
    "            mask = np.array(Image.open(pred_path))\n",
    "            \n",
    "            # For semantic segmentation, both directories get the same mask\n",
    "            # (SegmentationClass = class per pixel, SegmentationObject = same for semantic)\n",
    "            Image.fromarray(mask).save(segmentation_class_dir / f\"{base_name}.png\")\n",
    "            Image.fromarray(mask).save(segmentation_object_dir / f\"{base_name}.png\")\n",
    "\n",
    "        # ===== CREATE ImageSets/Segmentation/default.txt =====\n",
    "        default_txt = imageset_dir / \"default.txt\"\n",
    "        with open(default_txt, 'w') as f:\n",
    "            for name in image_names:\n",
    "                f.write(name + '\\n')\n",
    "\n",
    "        print(f\"Created ImageSets/Segmentation/default.txt with {len(image_names)} entries\")\n",
    "\n",
    "        # ===== CREATE ZIP ARCHIVE =====\n",
    "        os.makedirs(os.path.dirname(output_zip), exist_ok=True)\n",
    "        \n",
    "        with zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "            # Add all files from temp directory to ZIP\n",
    "            for root, dirs, files in os.walk(temp_path):\n",
    "                for file in files:\n",
    "                    file_path = Path(root) / file\n",
    "                    # Get relative path from temp_path for ZIP archive\n",
    "                    arcname = file_path.relative_to(temp_path)\n",
    "                    zipf.write(file_path, arcname)\n",
    "\n",
    "        # Print results\n",
    "        zip_size_mb = Path(output_zip).stat().st_size / (1024**2)\n",
    "        print(f\"\\n‚úì CVAT Segmentation Mask export created successfully!\")\n",
    "        print(f\"  Output ZIP: {output_zip}\")\n",
    "        print(f\"  Size: {zip_size_mb:.1f} MB\")\n",
    "        print(f\"  Classes: {len(class_names)}\")\n",
    "        print(f\"  Masks: {len(image_names)}\")\n",
    "\n",
    "print(\"‚úì CVAT export function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e576a824",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PHASE 1: GENERATE PREDICTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load model\n",
    "model = load_model(CHECKPOINT_PATH)\n",
    "\n",
    "# Generate predictions\n",
    "prediction_paths = generate_all_predictions(\n",
    "    model=model,\n",
    "    tiles_dir=str(TILES_DIR),\n",
    "    output_dir=str(PREDICTIONS_DIR)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7640d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CVAT-compatible export\n",
    "create_cvat_segmentation_mask_export(\n",
    "    predictions_dir=str(PREDICTIONS_DIR),\n",
    "    output_zip=\"mal_workspace/cvat_predictions_export.zip\",\n",
    "    class_names=CLASS_NAMES,\n",
    "    class_colors={\n",
    "        0: (0, 0, 0),         # background - black\n",
    "        1: (128, 0, 0),       # building - maroon\n",
    "        2: (0, 128, 0),       # woodland - dark green\n",
    "        3: (0, 0, 128),       # water - navy blue\n",
    "        4: (128, 128, 0),     # road - olive\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1179df43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PHASE 2: VISUALIZE PREDICTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig = visualize_predictions(\n",
    "    tiles_dir=str(TILES_DIR),\n",
    "    predictions_dir=str(PREDICTIONS_DIR),\n",
    "    num_samples=6  # Change to see more/fewer samples\n",
    ")\n",
    "\n",
    "plt.savefig(WORKSPACE_DIR / \"predictions_preview.png\", dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Preview saved: {WORKSPACE_DIR / 'predictions_preview.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79aae80",
   "metadata": {},
   "source": [
    "## Manual Annotation in CVAT\n",
    "\n",
    "**Important**: This step happens in the CVAT web UI (`http://localhost:8080`), not in this notebook.\n",
    "\n",
    "### Step 1: Prepare Files\n",
    "- Tiles are ready in: `mal_workspace/01_tiles/`\n",
    "- Predictions are ready in: `mal_workspace/02_predictions/`\n",
    "\n",
    "### Step 2: Create CVAT Project\n",
    "1. Open http://localhost:8080 in your browser\n",
    "2. Click \"Create a new project\"\n",
    "3. Name it however you want\n",
    "4. Add labels at constructor (Important for CVAT to recognize the annotations):\n",
    "   - `background` (class 0)\n",
    "   - `building` (class 1)\n",
    "   - `woodland` (class 2)\n",
    "   - `water` (class 3)\n",
    "   - `road` (class 4)\n",
    "5. Submit and open\n",
    "\n",
    "### Step 3: Create CVAT Task\n",
    "1. Create a task\n",
    "2. Name however you want\n",
    "3. Upload the images you want to annotate. It should be `mal_workspace/01_tiles/`\n",
    "4. Submit and open\n",
    "\n",
    "### Step 4: Import Initial Predictions\n",
    "1. Click Menu ‚Üí Upload annotations\n",
    "2. Choose Segmentation mask 1.1\n",
    "3. Upload the ZIP file `mal_workspace\\cvat_predictions_export.zip`\n",
    "\n",
    "### Step 5: Refine Annotations\n",
    "1. For each tile:\n",
    "   - Use **Brush** tool to add pixels\n",
    "   - Use **Eraser** to remove pixels\n",
    "2. Save changes frequently\n",
    "(There should be a better workflow. Investigating)\n",
    "\n",
    "### Step 6: Export Corrected Masks\n",
    "1. Click Menu ‚Üí Export task dataset\n",
    "2. Select \"Segmentation mask 1.1\" format\n",
    "3. Download ZIP file\n",
    "4. Extract SegmentationClass folder to: `mal_workspace/03_corrected_masks/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
