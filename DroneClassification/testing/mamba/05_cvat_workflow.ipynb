{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28995067",
   "metadata": {},
   "source": [
    "# Model-Assisted Labeling (MAL) Workflow with DeepLab & CVAT\n",
    "\n",
    "This notebook provides an end-to-end MAL pipeline:\n",
    "\n",
    "1. **Tile Split**: Break large GeoTIFF images into 512√ó512 tiles\n",
    "2. **Prediction**: Generate masks using trained DeepLab model\n",
    "3. **Visualization**: Preview predictions before CVAT annotation\n",
    "4. **CVAT Refinement**: Manual annotation/correction (in web UI, not here)\n",
    "5. **Reconstruction**: Stitch corrected masks back into full GeoTIFF\n",
    "\n",
    "**Workflow**: Phase 1 ‚Üí Phase 2 ‚Üí Phase 3 ‚Üí (CVAT in browser) ‚Üí Phase 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696e7d3e",
   "metadata": {},
   "source": [
    "# Installing CVAT on Windows\n",
    "CVAT (Computer Vision Annotation Tool) is a powerful open-source tool for image and video annotation, widely used in computer vision projects. This step-by-step guide provides detailed instructions on how to install CVAT on a Windows environment. By following these instructions, you'll set up the necessary dependencies, including Git, Docker, and Google Chrome, and then clone the CVAT source code from its GitHub repository. The guide takes you through the process of running Docker containers, creating a superuser account, and accessing CVAT through Google Chrome. Whether you're a developer, data scientist, or enthusiast in the field of computer vision, this guide equips you with the tools to seamlessly integrate CVAT into your Windows environment for efficient annotation tasks.\n",
    "\n",
    "## Step 1: Install Git for Windows\n",
    "\n",
    "1. Download Git for Windows from [https://gitforwindows.org/](https://gitforwindows.org/).\n",
    "2. Install Git, keeping all options as default.\n",
    "3. Open the command prompt (`cmd`) and type the following command to check the Git version:\n",
    "    ```bash\n",
    "    git --version\n",
    "    ```\n",
    "\n",
    "## Step 2: Install Docker Desktop for Windows\n",
    "\n",
    "1. Download [Docker Desktop for Windows](https://desktop.docker.com/win/main/amd64/Docker%20Desktop%20Installer.exe).\n",
    "2. Double-click the Docker for Windows Installer to run the installer.\n",
    "3. Follow the instructions for installation, and reboot the system after installation is complete.\n",
    "4. Open the command prompt and check the Docker version:\n",
    "    ```bash\n",
    "    docker --version\n",
    "    ```\n",
    "5. Check the Docker Compose version:\n",
    "    ```bash\n",
    "    docker compose version\n",
    "    ```\n",
    "\n",
    "## Step 3: Install Google Chrome\n",
    "\n",
    "1. Download and install [Google Chrome](https://www.google.com/chrome/), as it is the only browser supported by CVAT.\n",
    "\n",
    "## Step 4: Clone CVAT Source Code\n",
    "\n",
    "1. Clone CVAT source code from the [GitHub repository](https://github.com/opencv/cvat):\n",
    "    ```bash\n",
    "    git clone https://github.com/opencv/cvat\n",
    "    cd cvat\n",
    "    ```\n",
    "2. Alternatively, check [alternatives](https://opencv.github.io/cvat/docs/administration/basics/installation/#how-to-get-cvat-source-code) for downloading specific release versions.\n",
    "\n",
    "## Step 5: Run Docker Containers for CVAT\n",
    "\n",
    "1. Run the following command to start Docker containers. This will download the latest CVAT release and other required images:\n",
    "    ```bash\n",
    "    docker compose up -d\n",
    "    ```\n",
    "2. Optionally, specify the CVAT version using the CVAT_VERSION environment variable:\n",
    "    ```bash\n",
    "    CVAT_VERSION=dev docker compose up -d\n",
    "    ```\n",
    "3. Check the status of the containers:\n",
    "    ```bash\n",
    "    docker ps\n",
    "    ```\n",
    "4. Wait until the CVAT server is up and running:\n",
    "    ```bash\n",
    "    docker logs cvat_server -f\n",
    "    ```\n",
    "5. Run the CVAT server:\n",
    "    ```bash\n",
    "    docker exec -it cvat_server bash\n",
    "    ```\n",
    "6. For the first-time setup, create a superuser account:\n",
    "    ```bash\n",
    "    python3 manage.py createsuperuser\n",
    "    ```\n",
    "    Choose a username and password for the admin account.\n",
    "\n",
    "## Step 6: Access CVAT in Google Chrome\n",
    "\n",
    "1. Open Google Chrome and go to `localhost:8080`.\n",
    "2. Log in with the superuser credentials created earlier.\n",
    "3. You should now be able to create a new annotation task.\n",
    "\n",
    "## Workflow\n",
    "To stop and remove the container, simply type, \n",
    "```bash\n",
    "docker compose down\n",
    "```\n",
    "And to start cvat again, simply type \n",
    "```bash\n",
    "docker compose up -d\n",
    "```\n",
    "Make sure you're in the correct directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b3963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import shutil\n",
    "from typing import Dict, List\n",
    "\n",
    "# Add project to path\n",
    "sys.path.insert(0, '../../')\n",
    "from models import DeepLab\n",
    "\n",
    "# Check GPU\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "# Configuration\n",
    "CLASS_NAMES = ['background', 'building', 'woodland', 'water', 'road']\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "TILE_SIZE = 512\n",
    "\n",
    "# Define workspace directories\n",
    "WORKSPACE_DIR = Path(\"mal_workspace\")\n",
    "TILES_DIR = WORKSPACE_DIR / \"01_tiles\"\n",
    "PREDICTIONS_DIR = WORKSPACE_DIR / \"02_predictions\"\n",
    "CORRECTED_MASKS_DIR = WORKSPACE_DIR / \"03_corrected_masks\"\n",
    "RECONSTRUCTED_DIR = WORKSPACE_DIR / \"04_reconstructed\"\n",
    "\n",
    "print(f\"‚úì Setup complete | Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e87b020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== UPDATE THESE PATHS =====\n",
    "INPUT_GEOTIFF = \"DroneClassification/mamba/cvat_test/images\"  # Change this path as needed\n",
    "CHECKPOINT_PATH = \"DroneClassificatiom/testing/mamba/experiments/Deeplab_Landcover_Edited/deeplab_checkpoint.pth\"  # Change this to your model\n",
    "# ==============================\n",
    "\n",
    "# Verify files exist\n",
    "if not Path(INPUT_GEOTIFF).exists():\n",
    "    print(f\"‚ö†Ô∏è  WARNING: {INPUT_GEOTIFF} not found!\")\n",
    "else:\n",
    "    print(f\"‚úì Input GeoTIFF: {INPUT_GEOTIFF}\")\n",
    "\n",
    "if not Path(CHECKPOINT_PATH).exists():\n",
    "    print(f\"‚ö†Ô∏è  WARNING: {CHECKPOINT_PATH} not found!\")\n",
    "else:\n",
    "    print(f\"‚úì Checkpoint: {CHECKPOINT_PATH}\")\n",
    "\n",
    "print(f\"\\nWorkspace: {WORKSPACE_DIR}\")\n",
    "print(f\"Classes: {CLASS_NAMES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bdd0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_geotiff(input_path: str, output_dir: str, tile_size: int = 512, overlap: int = 0) -> Dict:\n",
    "    \"\"\"\n",
    "    Break a large GeoTIFF into non-overlapping or overlapping tiles.\n",
    "    \n",
    "    Args:\n",
    "        input_path: Path to input GeoTIFF\n",
    "        output_dir: Directory to save tiles\n",
    "        tile_size: Tile size (default 512)\n",
    "        overlap: Overlap in pixels between tiles (default 0)\n",
    "    \n",
    "    Returns:\n",
    "        Metadata dict with tile info and original shape\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load image\n",
    "    img = Image.open(input_path)\n",
    "    img_array = np.array(img)\n",
    "    height, width = img_array.shape[:2]\n",
    "    \n",
    "    print(f\"Tiling {Path(input_path).name}\")\n",
    "    print(f\"  Original shape: {img_array.shape}\")\n",
    "    print(f\"  Tile size: {tile_size}√ó{tile_size}, Overlap: {overlap}px\")\n",
    "    \n",
    "    tiles_metadata = []\n",
    "    stride = tile_size - overlap\n",
    "    \n",
    "    # Extract tiles\n",
    "    for y in range(0, height - tile_size + 1, stride):\n",
    "        for x in range(0, width - tile_size + 1, stride):\n",
    "            tile = img_array[y:y+tile_size, x:x+tile_size]\n",
    "            \n",
    "            if tile.shape[0] != tile_size or tile.shape[1] != tile_size:\n",
    "                continue\n",
    "            \n",
    "            tile_name = f\"tile_{y:05d}_{x:05d}\"\n",
    "            tile_path = Path(output_dir) / f\"{tile_name}.png\"\n",
    "            \n",
    "            Image.fromarray(tile.astype(np.uint8)).save(tile_path)\n",
    "            \n",
    "            tiles_metadata.append({\n",
    "                \"tile_id\": tile_name,\n",
    "                \"filename\": f\"{tile_name}.png\",\n",
    "                \"original_x\": int(x),\n",
    "                \"original_y\": int(y),\n",
    "                \"width\": tile_size,\n",
    "                \"height\": tile_size\n",
    "            })\n",
    "    \n",
    "    metadata = {\n",
    "        \"original_shape\": list(img_array.shape),\n",
    "        \"tile_size\": tile_size,\n",
    "        \"tile_count\": len(tiles_metadata),\n",
    "        \"tiles\": tiles_metadata\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úì Created {len(tiles_metadata)} tiles\\n\")\n",
    "    return metadata\n",
    "\n",
    "print(\"‚úì Tiling function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a2e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path: str) -> DeepLab:\n",
    "    \"\"\"Load trained DeepLab model.\"\"\"\n",
    "    model = DeepLab(\n",
    "        num_classes=NUM_CLASSES,\n",
    "        input_image_size=TILE_SIZE,\n",
    "        backbone='resnet50',\n",
    "        output_stride=4\n",
    "    ).to(device)\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    if isinstance(checkpoint, dict) and 'model' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "    \n",
    "    model.eval()\n",
    "    print(f\"‚úì Model loaded from {Path(checkpoint_path).name}\\n\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def normalize_image(image: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\"Normalize image to [-2, 2] range using ImageNet stats.\"\"\"\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    image = torch.from_numpy(image).permute(2, 0, 1)\n",
    "    \n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    image = (image - mean) / std\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "def predict_single_tile(model: DeepLab, image_path: str) -> np.ndarray:\n",
    "    \"\"\"Generate prediction mask for one tile.\"\"\"\n",
    "    image = np.array(Image.open(image_path).convert('RGB'))\n",
    "    image_tensor = normalize_image(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        mask = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()\n",
    "    \n",
    "    return mask.astype(np.uint8)\n",
    "\n",
    "\n",
    "def generate_all_predictions(model: DeepLab, tiles_dir: str, output_dir: str) -> List[str]:\n",
    "    \"\"\"Generate predictions for all tiles in a directory.\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    tile_files = sorted([f for f in os.listdir(tiles_dir) if f.endswith('.png')])\n",
    "    print(f\"Generating predictions for {len(tile_files)} tiles...\")\n",
    "    \n",
    "    prediction_paths = []\n",
    "    for tile_file in tqdm(tile_files, desc=\"Predicting\"):\n",
    "        tile_path = Path(tiles_dir) / tile_file\n",
    "        pred_mask = predict_single_tile(model, str(tile_path))\n",
    "        \n",
    "        pred_name = tile_file.replace('.png', '_pred.png')\n",
    "        pred_path = Path(output_dir) / pred_name\n",
    "        \n",
    "        Image.fromarray(pred_mask, mode='L').save(pred_path)\n",
    "        prediction_paths.append(pred_path)\n",
    "    \n",
    "    print(f\"‚úì Generated {len(prediction_paths)} predictions\\n\")\n",
    "    return prediction_paths\n",
    "\n",
    "print(\"‚úì Inference functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbd57f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(tiles_dir: str, predictions_dir: str, num_samples: int = 4):\n",
    "    \"\"\"\n",
    "    Display sample tiles with predictions.\n",
    "    \n",
    "    Shows: Original Image | Predicted Mask | Overlay\n",
    "    \"\"\"\n",
    "    tile_files = sorted([f for f in os.listdir(tiles_dir) if f.endswith('.png')])\n",
    "    indices = np.random.choice(len(tile_files), min(num_samples, len(tile_files)), replace=False)\n",
    "    \n",
    "    class_colors = {\n",
    "        0: [0.0, 0.0, 0.0],       # background - black\n",
    "        1: [1.0, 0.0, 0.0],       # building - red\n",
    "        2: [0.0, 0.5, 0.0],       # woodland - green\n",
    "        3: [0.0, 0.0, 1.0],       # water - blue\n",
    "        4: [1.0, 1.0, 0.0],       # road - yellow\n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(len(indices), 3, figsize=(14, 4*len(indices)))\n",
    "    fig.suptitle('Sample Predictions (Image | Mask | Overlay)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    for row, idx in enumerate(indices):\n",
    "        tile_file = tile_files[idx]\n",
    "        tile_path = Path(tiles_dir) / tile_file\n",
    "        pred_file = tile_file.replace('.png', '_pred.png')\n",
    "        pred_path = Path(predictions_dir) / pred_file\n",
    "        \n",
    "        image = np.array(Image.open(tile_path))\n",
    "        pred_mask = np.array(Image.open(pred_path))\n",
    "        \n",
    "        # Column 1: Image\n",
    "        axes[row, 0].imshow(image)\n",
    "        axes[row, 0].set_title(f'Image', fontsize=10)\n",
    "        axes[row, 0].axis('off')\n",
    "        \n",
    "        # Column 2: Mask\n",
    "        axes[row, 1].imshow(pred_mask, cmap='tab10', vmin=0, vmax=4)\n",
    "        axes[row, 1].set_title(f'Mask (classes: {np.unique(pred_mask)})', fontsize=10)\n",
    "        axes[row, 1].axis('off')\n",
    "        \n",
    "        # Column 3: Overlay\n",
    "        mask_rgb = np.zeros((*pred_mask.shape, 3))\n",
    "        for class_id, color in class_colors.items():\n",
    "            mask_rgb[pred_mask == class_id] = color\n",
    "        overlay = 0.65 * (image / 255.0) + 0.35 * mask_rgb\n",
    "        axes[row, 2].imshow(overlay)\n",
    "        axes[row, 2].set_title('Overlay', fontsize=10)\n",
    "        axes[row, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8148df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_geotiff(tiles_metadata: Dict, corrected_masks_dir: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Stitch corrected tile masks back into a single GeoTIFF.\n",
    "    \n",
    "    Args:\n",
    "        tiles_metadata: Output from tile_geotiff() function\n",
    "        corrected_masks_dir: Directory containing corrected masks from CVAT\n",
    "        output_path: Path to save final GeoTIFF\n",
    "    \"\"\"\n",
    "    original_shape = tiles_metadata['original_shape']\n",
    "    tile_size = tiles_metadata['tile_size']\n",
    "    \n",
    "    output_mask = np.zeros(original_shape[:2], dtype=np.uint8)\n",
    "    \n",
    "    print(f\"Reconstructing GeoTIFF...\")\n",
    "    print(f\"  Output shape: {output_mask.shape}\")\n",
    "    print(f\"  Processing {len(tiles_metadata['tiles'])} tiles...\")\n",
    "    \n",
    "    found_count = 0\n",
    "    for tile_info in tqdm(tiles_metadata['tiles'], desc=\"Stitching\"):\n",
    "        tile_id = tile_info['tile_id']\n",
    "        x = tile_info['original_x']\n",
    "        y = tile_info['original_y']\n",
    "        \n",
    "        # Try multiple filename formats (CVAT may export differently)\n",
    "        candidates = [\n",
    "            f\"{tile_id}_pred.png\",\n",
    "            f\"{tile_id}.png\",\n",
    "            Path(corrected_masks_dir) / f\"{tile_id}_pred.png\",\n",
    "            Path(corrected_masks_dir) / f\"{tile_id}.png\"\n",
    "        ]\n",
    "        \n",
    "        mask_path = None\n",
    "        for candidate in candidates:\n",
    "            if isinstance(candidate, str):\n",
    "                candidate = Path(corrected_masks_dir) / candidate\n",
    "            if candidate.exists():\n",
    "                mask_path = candidate\n",
    "                break\n",
    "        \n",
    "        if mask_path is None:\n",
    "            print(f\"  ‚ö†Ô∏è  Could not find mask for {tile_id}\")\n",
    "            continue\n",
    "        \n",
    "        mask = np.array(Image.open(mask_path))\n",
    "        \n",
    "        # Handle multi-channel masks\n",
    "        if mask.ndim == 3:\n",
    "            mask = mask[:, :, 0]\n",
    "        \n",
    "        output_mask[y:y+tile_size, x:x+tile_size] = mask\n",
    "        found_count += 1\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    Image.fromarray(output_mask, mode='L').save(output_path)\n",
    "    \n",
    "    print(f\"‚úì Reconstructed GeoTIFF saved\")\n",
    "    print(f\"  Output: {output_path}\")\n",
    "    print(f\"  Tiles found: {found_count}/{len(tiles_metadata['tiles'])}\")\n",
    "    print(f\"  Classes: {np.unique(output_mask)}\\n\")\n",
    "\n",
    "print(\"‚úì Helper functions loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfdf8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PHASE 1: TILE SPLITTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create tiles\n",
    "tiles_metadata = tile_geotiff(\n",
    "    input_path=INPUT_GEOTIFF,\n",
    "    output_dir=str(TILES_DIR),\n",
    "    tile_size=TILE_SIZE,\n",
    "    overlap=0  # Change to 64 for 64px overlap if desired\n",
    ")\n",
    "\n",
    "# Save metadata for later use\n",
    "metadata_path = WORKSPACE_DIR / \"tiles_metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(tiles_metadata, f, indent=2)\n",
    "\n",
    "print(f\"Metadata saved: {metadata_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e576a824",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PHASE 2: GENERATE PREDICTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load model\n",
    "model = load_model(CHECKPOINT_PATH)\n",
    "\n",
    "# Generate predictions\n",
    "prediction_paths = generate_all_predictions(\n",
    "    model=model,\n",
    "    tiles_dir=str(TILES_DIR),\n",
    "    output_dir=str(PREDICTIONS_DIR)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1179df43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PHASE 3: VISUALIZE PREDICTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig = visualize_predictions(\n",
    "    tiles_dir=str(TILES_DIR),\n",
    "    predictions_dir=str(PREDICTIONS_DIR),\n",
    "    num_samples=6  # Change to see more/fewer samples\n",
    ")\n",
    "\n",
    "plt.savefig(WORKSPACE_DIR / \"predictions_preview.png\", dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Preview saved: {WORKSPACE_DIR / 'predictions_preview.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79aae80",
   "metadata": {},
   "source": [
    "## Phase 4: Manual Annotation in CVAT\n",
    "\n",
    "**Important**: This step happens in the CVAT web UI (`http://localhost:8080`), not in this notebook.\n",
    "\n",
    "### Step 1: Prepare Files\n",
    "- Tiles are ready in: `mal_workspace/01_tiles/`\n",
    "- Predictions are ready in: `mal_workspace/02_predictions/`\n",
    "\n",
    "### Step 2: Create CVAT Task\n",
    "1. Open http://localhost:8080 in your browser\n",
    "2. Click \"Create new task\"\n",
    "3. Name it: \"Landcover MAL Refinement\"\n",
    "4. Create labels:\n",
    "   - `background` (class 0)\n",
    "   - `building` (class 1)\n",
    "   - `woodland` (class 2)\n",
    "   - `water` (class 3)\n",
    "   - `road` (class 4)\n",
    "\n",
    "### Step 3: Upload Images\n",
    "1. Select all PNG files from `01_tiles/` directory\n",
    "2. Click \"Submit & Open\"\n",
    "\n",
    "### Step 4: Import Initial Predictions\n",
    "1. Click Menu ‚Üí Annotations ‚Üí Import annotations\n",
    "2. Choose semantic segmentation format\n",
    "3. Upload ZIP file containing all `*_pred.png` files from `02_predictions/`\n",
    "\n",
    "### Step 5: Refine Annotations\n",
    "1. For each tile:\n",
    "   - Use **Brush** tool to add pixels\n",
    "   - Use **Eraser** to remove pixels\n",
    "   - Use **Magic Wand** for connected regions\n",
    "2. Save changes frequently\n",
    "\n",
    "### Step 6: Export Corrected Masks\n",
    "1. Click Menu ‚Üí Export task dataset\n",
    "2. Select \"Semantic Segmentation\" format\n",
    "3. Download ZIP file\n",
    "4. Extract to: `mal_workspace/03_corrected_masks/`\n",
    "\n",
    "Once done, proceed to Phase 5 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a060a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PHASE 5: RECONSTRUCT GEOTIFF\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load metadata\n",
    "metadata_path = WORKSPACE_DIR / \"tiles_metadata.json\"\n",
    "with open(metadata_path, 'r') as f:\n",
    "    tiles_metadata = json.load(f)\n",
    "\n",
    "# Reconstruct\n",
    "output_geotiff = RECONSTRUCTED_DIR / \"final_landcover_corrected.tif\"\n",
    "reconstruct_geotiff(\n",
    "    tiles_metadata=tiles_metadata,\n",
    "    corrected_masks_dir=str(CORRECTED_MASKS_DIR),\n",
    "    output_path=str(output_geotiff)\n",
    ")\n",
    "\n",
    "print(f\"\\nüéâ MAL Workflow Complete!\")\n",
    "print(f\"Final GeoTIFF: {output_geotiff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ec3f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"WORKFLOW SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìÅ Workspace Structure:\")\n",
    "print(f\"  {WORKSPACE_DIR}/\")\n",
    "print(f\"  ‚îú‚îÄ‚îÄ 01_tiles/                    ({len(list(TILES_DIR.glob('*.png')))} tiles)\")\n",
    "print(f\"  ‚îú‚îÄ‚îÄ 02_predictions/              ({len(list(PREDICTIONS_DIR.glob('*.png')))} predictions)\")\n",
    "print(f\"  ‚îú‚îÄ‚îÄ 03_corrected_masks/          (from CVAT)\")\n",
    "print(f\"  ‚îú‚îÄ‚îÄ 04_reconstructed/            (final output)\")\n",
    "print(f\"  ‚îî‚îÄ‚îÄ tiles_metadata.json          (tile positions)\")\n",
    "\n",
    "print(f\"\\nüìä Dataset Info:\")\n",
    "print(f\"  Original shape: {tiles_metadata['original_shape']}\")\n",
    "print(f\"  Tile size: {tiles_metadata['tile_size']}√ó{tiles_metadata['tile_size']}\")\n",
    "print(f\"  Total tiles: {tiles_metadata['tile_count']}\")\n",
    "print(f\"  Classes: {', '.join(CLASS_NAMES)}\")\n",
    "\n",
    "final_output = list(RECONSTRUCTED_DIR.glob(\"*.tif\"))\n",
    "if final_output:\n",
    "    print(f\"\\n‚úì Final output: {final_output[0].name}\")\n",
    "else:\n",
    "    print(f\"\\n‚è≥ Waiting for CVAT export to Phase 5...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
