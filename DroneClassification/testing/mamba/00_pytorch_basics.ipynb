{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Basics: Complete Fundamentals\n",
    "## Building Foundation for Mamba Integration\n",
    "\n",
    "**Goal**: Learn PyTorch by implementing core concepts hands-on\n",
    "\n",
    "**Modules Covered**:\n",
    "1. Tensors & Basic Operations\n",
    "2. Building Custom nn.Module Classes\n",
    "3. Residual Connections & Normalization\n",
    "4. Working with Image Data\n",
    "5. Convolutional Layers\n",
    "6. Backpropagation & Gradients\n",
    "7. Building Multi-Stage Encoders\n",
    "8. Understanding UNet Architecture\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries\n",
    "\n",
    "First, let's import everything we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.7-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp311-cp311-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.60.1-cp311-cp311-win_amd64.whl.metadata (114 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp311-cp311-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\adytc\\anaconda3\\envs\\mamba-env\\lib\\site-packages (from matplotlib) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\adytc\\anaconda3\\envs\\mamba-env\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\adytc\\anaconda3\\envs\\mamba-env\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\adytc\\anaconda3\\envs\\mamba-env\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\adytc\\anaconda3\\envs\\mamba-env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.7-cp311-cp311-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 2.6/8.1 MB 15.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 5.5/8.1 MB 14.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 14.0 MB/s  0:00:00\n",
      "Downloading contourpy-1.3.3-cp311-cp311-win_amd64.whl (225 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.60.1-cp311-cp311-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.3/2.3 MB 14.3 MB/s  0:00:00\n",
      "Downloading kiwisolver-1.4.9-cp311-cp311-win_amd64.whl (73 kB)\n",
      "Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   ---------------------------------------- 6/6 [matplotlib]\n",
      "\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.60.1 kiwisolver-1.4.9 matplotlib-3.10.7 pyparsing-3.2.5\n"
     ]
    }
   ],
   "source": [
    "# Install packages as needed\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.4.1+cu124\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 4060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check PyTorch version and CUDA availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Module 0.1: Tensors & Basic Operations\n",
    "\n",
    "**Goal**: Understand PyTorch tensors (the foundation of everything)\n",
    "\n",
    "**What is a tensor?**\n",
    "- Like a NumPy array, but can run on GPU\n",
    "- Can automatically compute gradients (for backpropagation)\n",
    "- The basic data structure for all neural networks\n",
    "\n",
    "**Analogy from scikit-learn**:\n",
    "- In sklearn: You work with NumPy arrays (X, y)\n",
    "- In PyTorch: You work with tensors (same idea, but GPU-enabled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.1: Creating Tensors\n",
    "\n",
    "Let's create tensors in different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From list: tensor([1, 2, 3, 4, 5])\n",
      "Shape: torch.Size([5])\n",
      "Data type: torch.int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Method 1: From a Python list\n",
    "tensor_from_list = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(f\"From list: {tensor_from_list}\")\n",
    "print(f\"Shape: {tensor_from_list.shape}\")\n",
    "print(f\"Data type: {tensor_from_list.dtype}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random tensor (3x4):\n",
      "tensor([[-2.5413,  0.4841,  1.3195,  0.8582],\n",
      "        [ 0.6416, -0.6697, -0.8900, -0.4335],\n",
      "        [ 0.5710,  0.3317,  0.3666, -1.1197]])\n",
      "Shape: torch.Size([3, 4])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Random tensor (most common for initialization)\n",
    "random_tensor = torch.randn(3, 4)  # 3 rows, 4 columns (like np.random.randn)\n",
    "print(f\"Random tensor (3x4):\")\n",
    "print(random_tensor)\n",
    "print(f\"Shape: {random_tensor.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeros (2x3):\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "Ones (2x3):\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Method 3: Zeros and ones\n",
    "zeros = torch.zeros(2, 3)\n",
    "ones = torch.ones(2, 3)\n",
    "print(f\"Zeros (2x3):\")\n",
    "print(zeros)\n",
    "print(f\"\\nOnes (2x3):\")\n",
    "print(ones)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From NumPy:\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "Shape: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Method 4: From NumPy array (useful when converting existing data)\n",
    "numpy_array = np.array([[1, 2], [3, 4]])\n",
    "tensor_from_numpy = torch.from_numpy(numpy_array)\n",
    "print(f\"From NumPy:\")\n",
    "print(tensor_from_numpy)\n",
    "print(f\"Shape: {tensor_from_numpy.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.2: Tensor Shapes and Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image tensor shape: torch.Size([3, 512, 512])\n",
      "This represents: 3 channels (RGB), 512 height, 512 width\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor representing an image\n",
    "# Format: (Channels, Height, Width)\n",
    "image = torch.randn(3, 512, 512)  # RGB image, 512x512\n",
    "print(f\"Image tensor shape: {image.shape}\")\n",
    "print(f\"This represents: 3 channels (RGB), 512 height, 512 width\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch size: 16x16\n",
      "Number of patches per side: 32\n",
      "Total patches: 1024\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reshape to simulate extracting 16x16 patches\n",
    "# This is what PatchEmbedding does!\n",
    "patch_size = 16\n",
    "num_patches_per_side = 512 // patch_size  # 32 patches per side\n",
    "print(f\"Patch size: {patch_size}x{patch_size}\")\n",
    "print(f\"Number of patches per side: {num_patches_per_side}\")\n",
    "print(f\"Total patches: {num_patches_per_side * num_patches_per_side}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After reshape: torch.Size([3, 32, 16, 32, 16])\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Manual reshaping (understand the concept)\n",
    "# Reshape: (3, 512, 512) â†’ (3, 32, 16, 32, 16)\n",
    "#          [C,  H,   W ]    [C, #H, pH, #W, pW]\n",
    "# Where #H = number of patches in height, pH = patch height\n",
    "reshaped = image.reshape(3, num_patches_per_side, patch_size, \n",
    "                          num_patches_per_side, patch_size)\n",
    "print(f\"After reshape: {reshaped.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After permute: torch.Size([32, 32, 3, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "# Now rearrange to (num_patches, channels, patch_height, patch_width)\n",
    "# This makes each patch a separate \"item\"\n",
    "patches = reshaped.permute(1, 3, 0, 2, 4)  # Rearrange dimensions\n",
    "print(f\"After permute: {patches.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened patches: torch.Size([1024, 768])\n",
      "This is 1024 patches, each with 768 features\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Flatten to get (num_patches, channels*patch_height*patch_width)\n",
    "patches_flat = patches.reshape(num_patches_per_side * num_patches_per_side, -1)\n",
    "print(f\"Flattened patches: {patches_flat.shape}\")\n",
    "print(f\"This is {patches_flat.shape[0]} patches, each with {patches_flat.shape[1]} features\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Key Reshape Operations ===\n",
      "Original: torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Key operations you'll use constantly:\n",
    "print(\"=== Key Reshape Operations ===\")\n",
    "x = torch.randn(2, 3, 4)\n",
    "print(f\"Original: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After reshape(2, 12): torch.Size([2, 12])\n"
     ]
    }
   ],
   "source": [
    "# .reshape() - change shape (must have same total elements)\n",
    "y = x.reshape(2, 12)\n",
    "print(f\"After reshape(2, 12): {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After view(6, 4): torch.Size([6, 4])\n"
     ]
    }
   ],
   "source": [
    "# .view() - similar to reshape, but has stricter memory requirements\n",
    "z = x.view(6, 4)\n",
    "print(f\"After view(6, 4): {z.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After permute(2, 0, 1): torch.Size([4, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# .permute() - rearrange dimensions\n",
    "w = x.permute(2, 0, 1)  # (4, 2, 3) - swap dimensions\n",
    "print(f\"After permute(2, 0, 1): {w.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After unsqueeze(0): torch.Size([1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# .unsqueeze() - add a dimension\n",
    "u = x.unsqueeze(0)  # Add batch dimension\n",
    "print(f\"After unsqueeze(0): {u.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After squeeze(0): torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# .squeeze() - remove dimensions of size 1\n",
    "s = u.squeeze(0)  # Remove batch dimension\n",
    "print(f\"After squeeze(0): {s.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.3: GPU Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ CUDA is available!\n",
      "Device name: NVIDIA GeForce RTX 4060\n",
      "\n",
      "CPU tensor device: cpu\n",
      "GPU tensor device: cuda:0\n",
      "GPU tensor device: cuda:0\n",
      "\n",
      "Result device: cuda:0\n",
      "âœ“ GPU operations work!\n",
      "\n",
      "Back to CPU: cpu\n",
      "\n",
      "âœ— Error when mixing CPU and GPU tensors:\n",
      "  Expected all tensors to be on the same device, but found at least two devices, c...\n",
      "  â†’ Always ensure tensors are on the same device!\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA (GPU) is available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ“ CUDA is available!\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\\n\")\n",
    "    \n",
    "    # Create tensor on CPU (default)\n",
    "    cpu_tensor = torch.randn(3, 3)\n",
    "    print(f\"CPU tensor device: {cpu_tensor.device}\")\n",
    "    \n",
    "    # Move to GPU - Method 1: .cuda()\n",
    "    gpu_tensor = cpu_tensor.cuda()\n",
    "    print(f\"GPU tensor device: {gpu_tensor.device}\")\n",
    "    \n",
    "    # Move to GPU - Method 2: .to('cuda') (preferred, more flexible)\n",
    "    gpu_tensor2 = cpu_tensor.to('cuda')\n",
    "    print(f\"GPU tensor device: {gpu_tensor2.device}\\n\")\n",
    "    \n",
    "    # Operations on GPU tensors\n",
    "    result = gpu_tensor + gpu_tensor2\n",
    "    print(f\"Result device: {result.device}\")\n",
    "    print(\"âœ“ GPU operations work!\\n\")\n",
    "    \n",
    "    # Move back to CPU (needed for plotting, NumPy conversion)\n",
    "    back_to_cpu = result.cpu()\n",
    "    print(f\"Back to CPU: {back_to_cpu.device}\")\n",
    "    \n",
    "    # IMPORTANT: Can't mix CPU and GPU tensors!\n",
    "    try:\n",
    "        mixed = cpu_tensor + gpu_tensor  # This will error!\n",
    "    except RuntimeError as e:\n",
    "        print(f\"\\nâœ— Error when mixing CPU and GPU tensors:\")\n",
    "        print(f\"  {str(e)[:80]}...\")\n",
    "        print(\"  â†’ Always ensure tensors are on the same device!\")\n",
    "        \n",
    "else:\n",
    "    print(\"âœ— CUDA not available - will use CPU\")\n",
    "    print(\"(For Mamba, you'll need GPU via WSL2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1.4: Practical Example - Image Tensor to Patches\n",
    "\n",
    "This is what PatchEmbedding will do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input images shape: torch.Size([2, 3, 512, 512])\n",
      "This is 2 RGB images, each 512x512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simulate a batch of images\n",
    "batch_size = 2\n",
    "channels = 3\n",
    "height = 512\n",
    "width = 512\n",
    "\n",
    "images = torch.randn(batch_size, channels, height, width)\n",
    "print(f\"Input images shape: {images.shape}\")\n",
    "print(f\"This is {batch_size} RGB images, each {height}x{width}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After unfolding: torch.Size([2, 3, 32, 32, 16, 16])\n",
      "Shape breakdown:\n",
      "  Dimension 0: batch_size = 2\n",
      "  Dimension 1: channels = 3\n",
      "  Dimension 2: num_patches_height = 32\n",
      "  Dimension 3: num_patches_width = 32\n",
      "  Dimension 4: patch_height = 16\n",
      "  Dimension 5: patch_width = 16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Method: Using unfold (efficient way to extract patches)\n",
    "patch_size = 16\n",
    "\n",
    "# unfold extracts sliding windows\n",
    "# unfold(dimension, size, step)\n",
    "patches_h = images.unfold(2, patch_size, patch_size)  # Unfold height\n",
    "patches_hw = patches_h.unfold(3, patch_size, patch_size)  # Unfold width\n",
    "\n",
    "print(f\"After unfolding: {patches_hw.shape}\")\n",
    "print(\"Shape breakdown:\")\n",
    "print(f\"  Dimension 0: batch_size = {patches_hw.shape[0]}\")\n",
    "print(f\"  Dimension 1: channels = {patches_hw.shape[1]}\")\n",
    "print(f\"  Dimension 2: num_patches_height = {patches_hw.shape[2]}\")\n",
    "print(f\"  Dimension 3: num_patches_width = {patches_hw.shape[3]}\")\n",
    "print(f\"  Dimension 4: patch_height = {patches_hw.shape[4]}\")\n",
    "print(f\"  Dimension 5: patch_width = {patches_hw.shape[5]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After permute: torch.Size([2, 32, 32, 3, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "# Rearrange to (batch, num_patches, channels * patch_height * patch_width)\n",
    "num_patches_h = patches_hw.shape[2]\n",
    "num_patches_w = patches_hw.shape[3]\n",
    "total_patches = num_patches_h * num_patches_w\n",
    "\n",
    "# Rearrange dimensions\n",
    "patches_rearranged = patches_hw.permute(0, 2, 3, 1, 4, 5)\n",
    "print(f\"After permute: {patches_rearranged.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final patches shape: torch.Size([2, 1024, 768])\n",
      "\n",
      "Meaning:\n",
      "  - 2 images\n",
      "  - Each has 1024 patches\n",
      "  - Each patch has 768 features\n",
      "\n",
      "This is exactly what PatchEmbedding outputs!\n",
      "(Except PatchEmbedding also projects to a different dimension)\n"
     ]
    }
   ],
   "source": [
    "# Flatten each patch\n",
    "patches_final = patches_rearranged.reshape(batch_size, total_patches, -1)\n",
    "print(f\"Final patches shape: {patches_final.shape}\")\n",
    "print(f\"\\nMeaning:\")\n",
    "print(f\"  - {batch_size} images\")\n",
    "print(f\"  - Each has {total_patches} patches\")\n",
    "print(f\"  - Each patch has {patches_final.shape[2]} features\")\n",
    "print(f\"\\nThis is exactly what PatchEmbedding outputs!\")\n",
    "print(\"(Except PatchEmbedding also projects to a different dimension)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Module 0.2: Building Your First nn.Module\n",
    "\n",
    "**Goal**: Learn how to create custom PyTorch models\n",
    "\n",
    "**Key concept**: All PyTorch models inherit from `nn.Module`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1: Simple Linear Layer Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Multi-Layer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Module 0.3: Residual Connections & Normalization\n",
    "\n",
    "**Goal**: Understand the building pattern used in modern architectures (including Mamba!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1: Simple Residual Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2: LayerNorm + Pre-norm Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Module 0.4: Working with Image Data\n",
    "\n",
    "**Goal**: Understand how images are represented as tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: Load and Convert Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Image Tensor Format (C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Module 0.5: Convolutional Layers\n",
    "\n",
    "**Goal**: Understand Conv2d (used in PatchEmbedding!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.1: Basic Conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.2: Using Conv2d for Patch Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Module 0.6: Backpropagation & Gradients\n",
    "\n",
    "**Goal**: Understand gradient flow (crucial for debugging!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.1: Forward + Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6.2: Understanding Leaf vs Non-Leaf Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Module 0.7: Building Multi-Stage Encoders\n",
    "\n",
    "**Goal**: Combine components into complex architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7.1: Single Encoder Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7.2: Multi-Stage Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Module 0.8: Understanding UNet Architecture\n",
    "\n",
    "**Goal**: Learn the encoder-decoder pattern with skip connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8.1: Simple UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8.2: Visualize Skip Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "## What You've Learned\n",
    "âœ“ PyTorch tensors and operations\n",
    "âœ“ Building custom nn.Module classes\n",
    "âœ“ Residual connections and normalization\n",
    "âœ“ Working with image data\n",
    "âœ“ Convolutional layers\n",
    "âœ“ Gradient flow and backpropagation\n",
    "âœ“ Multi-stage encoder architectures\n",
    "âœ“ UNet encoder-decoder pattern\n",
    "\n",
    "## Next Steps\n",
    "After completing this notebook:\n",
    "1. Phase 1 Deep Dive: What is Mamba?\n",
    "2. Phase 2 Deep Dive: Why each building block?\n",
    "3. Build MambaUNet from scratch!\n",
    "\n",
    "---\n",
    "**Great job working through the fundamentals!** ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
