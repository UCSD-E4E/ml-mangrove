{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mamba Installation and Functionality Test\n",
    "\n",
    "**Purpose**: Verify that Mamba-SSM can be installed and works correctly on Windows with CUDA.\n",
    "\n",
    "**Expected Outcome**: \n",
    "- Mamba packages installed successfully\n",
    "- Basic forward pass works on CUDA\n",
    "- No errors or compatibility issues\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Mamba Packages\n",
    "\n",
    "This will install:\n",
    "- `mamba-ssm`: The core Mamba state-space model library\n",
    "- `causal-conv1d`: Required dependency for efficient causal convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install mamba-ssm and dependencies\n",
    "!pip install mamba-ssm causal-conv1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Verify Installation\n",
    "\n",
    "Check that packages can be imported successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test imports\n",
    "try:\n",
    "    import mamba_ssm\n",
    "    print(\"✓ mamba_ssm imported successfully\")\n",
    "    print(f\"  Version: {mamba_ssm.__version__ if hasattr(mamba_ssm, '__version__') else 'Unknown'}\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Failed to import mamba_ssm: {e}\")\n",
    "\n",
    "try:\n",
    "    import causal_conv1d\n",
    "    print(\"✓ causal_conv1d imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Failed to import causal_conv1d: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Check PyTorch and CUDA Availability\n",
    "\n",
    "Verify that PyTorch is installed and CUDA is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"⚠️  CUDA not available - Mamba may not work optimally\")\n",
    "    print(\"   Consider using a machine with CUDA support for best performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test Basic Mamba Forward Pass\n",
    "\n",
    "Create a simple Mamba layer and test a forward pass with random data.\n",
    "\n",
    "**Test Configuration**:\n",
    "- Input: (batch=2, sequence_length=64, dimension=128)\n",
    "- Model: Mamba with d_model=128, d_state=16, d_conv=4, expand=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mamba_ssm import Mamba\n",
    "\n",
    "# Test configuration\n",
    "batch_size = 2\n",
    "sequence_length = 64\n",
    "dim = 128\n",
    "\n",
    "# Create random input tensor\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "x = torch.randn(batch_size, sequence_length, dim).to(device)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\" if torch.cuda.is_available() else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Mamba model\n",
    "model = Mamba(\n",
    "    d_model=dim,      # Model dimension\n",
    "    d_state=16,       # SSM state dimension\n",
    "    d_conv=4,         # Local convolution width\n",
    "    expand=2          # Expansion factor\n",
    ").to(device)\n",
    "\n",
    "print(\"✓ Mamba model created successfully\")\n",
    "print(f\"  Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        y = model(x)\n",
    "    \n",
    "    print(\"✓ Forward pass successful!\")\n",
    "    print(f\"  Input shape:  {x.shape}\")\n",
    "    print(f\"  Output shape: {y.shape}\")\n",
    "    print(f\"  Output dtype: {y.dtype}\")\n",
    "    print(f\"  Output device: {y.device}\")\n",
    "    \n",
    "    # Check for NaN or Inf\n",
    "    if torch.isnan(y).any():\n",
    "        print(\"⚠️  Warning: Output contains NaN values\")\n",
    "    elif torch.isinf(y).any():\n",
    "        print(\"⚠️  Warning: Output contains Inf values\")\n",
    "    else:\n",
    "        print(\"✓ Output is numerically stable (no NaN/Inf)\")\n",
    "    \n",
    "    # Show output statistics\n",
    "    print(f\"\\nOutput statistics:\")\n",
    "    print(f\"  Mean: {y.mean().item():.4f}\")\n",
    "    print(f\"  Std:  {y.std().item():.4f}\")\n",
    "    print(f\"  Min:  {y.min().item():.4f}\")\n",
    "    print(f\"  Max:  {y.max().item():.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Forward pass failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test Gradient Flow (Training Readiness)\n",
    "\n",
    "Verify that gradients can be computed (important for training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test gradient computation\n",
    "try:\n",
    "    # Forward pass with gradients enabled\n",
    "    x_grad = torch.randn(batch_size, sequence_length, dim, requires_grad=True).to(device)\n",
    "    y_grad = model(x_grad)\n",
    "    \n",
    "    # Compute a dummy loss and backpropagate\n",
    "    loss = y_grad.mean()\n",
    "    loss.backward()\n",
    "    \n",
    "    print(\"✓ Gradient computation successful!\")\n",
    "    print(f\"  Loss value: {loss.item():.4f}\")\n",
    "    print(f\"  Input gradient shape: {x_grad.grad.shape}\")\n",
    "    print(f\"  Input gradient mean: {x_grad.grad.mean().item():.6f}\")\n",
    "    \n",
    "    # Check model parameter gradients\n",
    "    grad_params = [p for p in model.parameters() if p.grad is not None]\n",
    "    print(f\"  Parameters with gradients: {len(grad_params)}/{len(list(model.parameters()))}\")\n",
    "    \n",
    "    if len(grad_params) > 0:\n",
    "        avg_grad = torch.stack([p.grad.abs().mean() for p in grad_params]).mean()\n",
    "        print(f\"  Average gradient magnitude: {avg_grad.item():.6f}\")\n",
    "    \n",
    "    print(\"✓ Model is ready for training!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Gradient computation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Memory Usage Test\n",
    "\n",
    "Check GPU memory usage for different sequence lengths (important for understanding limitations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Testing memory usage with different sequence lengths...\\n\")\n",
    "    \n",
    "    sequence_lengths = [64, 128, 256, 512, 1024]\n",
    "    \n",
    "    for seq_len in sequence_lengths:\n",
    "        # Clear cache\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        try:\n",
    "            # Create input\n",
    "            x_test = torch.randn(1, seq_len, dim).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                y_test = model(x_test)\n",
    "            \n",
    "            # Get memory stats\n",
    "            mem_allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "            mem_reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "            mem_peak = torch.cuda.max_memory_allocated() / 1024**2\n",
    "            \n",
    "            print(f\"Sequence length {seq_len:4d}: \"\n",
    "                  f\"Allocated: {mem_allocated:6.2f} MB, \"\n",
    "                  f\"Reserved: {mem_reserved:6.2f} MB, \"\n",
    "                  f\"Peak: {mem_peak:6.2f} MB\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"Sequence length {seq_len:4d}: ✗ Out of memory\")\n",
    "                break\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    # Final cleanup\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"Skipping memory test (CUDA not available)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
