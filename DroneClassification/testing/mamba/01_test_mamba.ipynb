{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Mamba Installation and Functionality Test (WSL2)\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "This notebook is designed to run in **WSL2 with the `mamba-env` conda environment**.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Follow the setup guide at `docs/WSL2_SETUP_GUIDE.md`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Installation\n",
    "\n",
    "Check that packages can be imported successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ mamba_ssm imported successfully\n",
      "  Version: 2.2.6.post3\n",
      "✓ causal_conv1d imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Test imports\n",
    "try:\n",
    "    import mamba_ssm\n",
    "    print(\"✓ mamba_ssm imported successfully\")\n",
    "    print(f\"  Version: {mamba_ssm.__version__ if hasattr(mamba_ssm, '__version__') else 'Unknown'}\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Failed to import mamba_ssm: {e}\")\n",
    "\n",
    "try:\n",
    "    import causal_conv1d\n",
    "    print(\"✓ causal_conv1d imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Failed to import causal_conv1d: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check PyTorch and CUDA Availability\n",
    "\n",
    "Verify that PyTorch is installed and CUDA is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.4.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 4060\n",
      "Device count: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"⚠️  CUDA not available - Mamba may not work optimally\")\n",
    "    print(\"   Consider using a machine with CUDA support for best performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Basic Mamba Forward Pass\n",
    "\n",
    "Create a simple Mamba layer and test a forward pass with random data.\n",
    "\n",
    "**Test Configuration**:\n",
    "- Input: (batch=2, sequence_length=64, dimension=128)\n",
    "- Model: Mamba with d_model=128, d_state=16, d_conv=4, expand=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 64, 128])\n",
      "Device: cuda\n",
      "Memory allocated: 0.06 MB\n"
     ]
    }
   ],
   "source": [
    "from mamba_ssm import Mamba\n",
    "\n",
    "# Test configuration\n",
    "batch_size = 2\n",
    "sequence_length = 64\n",
    "dim = 128\n",
    "\n",
    "# Create random input tensor\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "x = torch.randn(batch_size, sequence_length, dim).to(device)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\" if torch.cuda.is_available() else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Mamba model created successfully\n",
      "  Model parameters: 116,480\n",
      "  Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Create Mamba model\n",
    "model = Mamba(\n",
    "    d_model=dim,      # Model dimension\n",
    "    d_state=16,       # SSM state dimension\n",
    "    d_conv=4,         # Local convolution width\n",
    "    expand=2          # Expansion factor\n",
    ").to(device)\n",
    "\n",
    "print(\"✓ Mamba model created successfully\")\n",
    "print(f\"  Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Forward pass successful!\n",
      "  Input shape:  torch.Size([2, 64, 128])\n",
      "  Output shape: torch.Size([2, 64, 128])\n",
      "  Output dtype: torch.float32\n",
      "  Output device: cuda:0\n",
      "✓ Output is numerically stable (no NaN/Inf)\n",
      "\n",
      "Output statistics:\n",
      "  Mean: -0.0002\n",
      "  Std:  0.0421\n",
      "  Min:  -0.1791\n",
      "  Max:  0.1942\n"
     ]
    }
   ],
   "source": [
    "# Test forward pass\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        y = model(x)\n",
    "    \n",
    "    print(\"✓ Forward pass successful!\")\n",
    "    print(f\"  Input shape:  {x.shape}\")\n",
    "    print(f\"  Output shape: {y.shape}\")\n",
    "    print(f\"  Output dtype: {y.dtype}\")\n",
    "    print(f\"  Output device: {y.device}\")\n",
    "    \n",
    "    # Check for NaN or Inf\n",
    "    if torch.isnan(y).any():\n",
    "        print(\"⚠️  Warning: Output contains NaN values\")\n",
    "    elif torch.isinf(y).any():\n",
    "        print(\"⚠️  Warning: Output contains Inf values\")\n",
    "    else:\n",
    "        print(\"✓ Output is numerically stable (no NaN/Inf)\")\n",
    "    \n",
    "    # Show output statistics\n",
    "    print(f\"\\nOutput statistics:\")\n",
    "    print(f\"  Mean: {y.mean().item():.4f}\")\n",
    "    print(f\"  Std:  {y.std().item():.4f}\")\n",
    "    print(f\"  Min:  {y.min().item():.4f}\")\n",
    "    print(f\"  Max:  {y.max().item():.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Forward pass failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Gradient Flow (Training Readiness)\n",
    "\n",
    "Verify that gradients can be computed (important for training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Gradient computation successful!\n",
      "  Loss value: 0.0004\n",
      "  Parameters with gradients: 9/9\n",
      "  Average gradient magnitude: 0.000026\n",
      "✓ Model is ready for training!\n"
     ]
    }
   ],
   "source": [
    "# Test gradient computation\n",
    "try:\n",
    "    # Forward pass with gradients enabled\n",
    "    x_grad = torch.randn(batch_size, sequence_length, dim, requires_grad=True).to(device)\n",
    "    y_grad = model(x_grad)\n",
    "    \n",
    "    # Compute a dummy loss and backpropagate\n",
    "    loss = y_grad.mean()\n",
    "    loss.backward()\n",
    "    \n",
    "    print(\"✓ Gradient computation successful!\")\n",
    "    print(f\"  Loss value: {loss.item():.4f}\")\n",
    "    \n",
    "    # Check model parameter gradients\n",
    "    grad_params = [p for p in model.parameters() if p.grad is not None]\n",
    "    print(f\"  Parameters with gradients: {len(grad_params)}/{len(list(model.parameters()))}\")\n",
    "    \n",
    "    if len(grad_params) > 0:\n",
    "        avg_grad = torch.stack([p.grad.abs().mean() for p in grad_params]).mean()\n",
    "        print(f\"  Average gradient magnitude: {avg_grad.item():.6f}\")\n",
    "    \n",
    "    print(\"✓ Model is ready for training!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Gradient computation failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Summary\n",
    "\n",
    "Display the complete working environment configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "WORKING ENVIRONMENT CONFIGURATION\n",
      "============================================================\n",
      "\n",
      "Python:\n",
      "  Version: 3.10.19\n",
      "  Executable: /home/jason/miniconda3/envs/mamba-env/bin/python\n",
      "\n",
      "PyTorch:\n",
      "  Version: 2.4.1+cu121\n",
      "  CUDA available: True\n",
      "  CUDA version: 12.1\n",
      "  cuDNN version: 90100\n",
      "\n",
      "NumPy:\n",
      "  Version: 1.26.4\n",
      "\n",
      "Mamba-SSM:\n",
      "  Version: 2.2.6.post3\n",
      "\n",
      "CUDA Toolkit:\n",
      "  Cuda compilation tools, release 12.6, V12.6.20\n",
      "\n",
      "GPU:\n",
      "  Device: NVIDIA GeForce RTX 4060\n",
      "  Memory: 8.0 GB\n",
      "\n",
      "============================================================\n",
      "✓ All components configured correctly for Mamba training!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"WORKING ENVIRONMENT CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Python info\n",
    "print(f\"\\nPython:\")\n",
    "print(f\"  Version: {sys.version.split()[0]}\")\n",
    "print(f\"  Executable: {sys.executable}\")\n",
    "\n",
    "# PyTorch info\n",
    "print(f\"\\nPyTorch:\")\n",
    "print(f\"  Version: {torch.__version__}\")\n",
    "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"  CUDA version: {torch.version.cuda}\")\n",
    "print(f\"  cuDNN version: {torch.backends.cudnn.version()}\")\n",
    "\n",
    "# NumPy info\n",
    "import numpy as np\n",
    "print(f\"\\nNumPy:\")\n",
    "print(f\"  Version: {np.__version__}\")\n",
    "\n",
    "# Mamba info\n",
    "import mamba_ssm\n",
    "print(f\"\\nMamba-SSM:\")\n",
    "print(f\"  Version: {mamba_ssm.__version__ if hasattr(mamba_ssm, '__version__') else '2.2.6.post3'}\")\n",
    "\n",
    "# CUDA toolkit info (from system)\n",
    "try:\n",
    "    nvcc_out = subprocess.check_output(['nvcc', '--version'], stderr=subprocess.STDOUT, text=True)\n",
    "    cuda_version = [line for line in nvcc_out.split('\\n') if 'release' in line.lower()][0]\n",
    "    print(f\"\\nCUDA Toolkit:\")\n",
    "    print(f\"  {cuda_version.strip()}\")\n",
    "except:\n",
    "    print(\"\\nCUDA Toolkit: Unable to determine\")\n",
    "\n",
    "# GPU info\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU:\")\n",
    "    print(f\"  Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"✓ All components configured correctly for Mamba training!\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
