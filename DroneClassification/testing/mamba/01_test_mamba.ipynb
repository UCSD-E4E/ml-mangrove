{
 "cells": [
  {
   "cell_type": "code",
   "source": "import sys\nimport subprocess\n\nprint(\"=\" * 60)\nprint(\"WORKING ENVIRONMENT CONFIGURATION\")\nprint(\"=\" * 60)\n\n# Python info\nprint(f\"\\nPython:\")\nprint(f\"  Version: {sys.version.split()[0]}\")\nprint(f\"  Executable: {sys.executable}\")\n\n# PyTorch info\nprint(f\"\\nPyTorch:\")\nprint(f\"  Version: {torch.__version__}\")\nprint(f\"  CUDA available: {torch.cuda.is_available()}\")\nprint(f\"  CUDA version: {torch.version.cuda}\")\nprint(f\"  cuDNN version: {torch.backends.cudnn.version()}\")\n\n# NumPy info\nimport numpy as np\nprint(f\"\\nNumPy:\")\nprint(f\"  Version: {np.__version__}\")\n\n# Mamba info\nimport mamba_ssm\nprint(f\"\\nMamba-SSM:\")\nprint(f\"  Version: {mamba_ssm.__version__ if hasattr(mamba_ssm, '__version__') else '2.2.6.post3'}\")\n\n# CUDA toolkit info (from system)\ntry:\n    nvcc_out = subprocess.check_output(['nvcc', '--version'], stderr=subprocess.STDOUT, text=True)\n    cuda_version = [line for line in nvcc_out.split('\\n') if 'release' in line.lower()][0]\n    print(f\"\\nCUDA Toolkit:\")\n    print(f\"  {cuda_version.strip()}\")\nexcept:\n    print(\"\\nCUDA Toolkit: Unable to determine\")\n\n# GPU info\nif torch.cuda.is_available():\n    print(f\"\\nGPU:\")\n    print(f\"  Device: {torch.cuda.get_device_name(0)}\")\n    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"✓ All components configured correctly for Mamba training!\")\nprint(\"=\" * 60)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Mamba Installation and Functionality Test (WSL2)\n\n**Purpose**: Verify that Mamba-SSM works correctly in WSL2 environment with CUDA support.\n\n**Expected Outcome**: \n- Mamba packages imported successfully\n- Basic forward pass works on CUDA\n- No errors or compatibility issues\n\n---\n\n## Environment Setup\n\nThis notebook is designed to run in **WSL2 with the `mamba-env` conda environment**.\n\n### Prerequisites (Should Already Be Completed)\n\nFollow the setup guide at `docs/WSL2_SETUP_GUIDE.md` to install:\n\n1. **WSL2** with Ubuntu\n2. **CUDA Toolkit 12.6** in WSL\n3. **Conda environment** named `mamba-env` with:\n   - Python 3.10\n   - PyTorch 2.4.1+cu121\n   - NumPy < 2.0\n   - mamba-ssm (built from source)\n\n### Running This Notebook\n\n**In VSCode with WSL Extension:**\n1. Open VSCode → Connect to WSL (green button bottom-left)\n2. Open this notebook\n3. Select kernel: **Python 3.10.19 ('mamba-env')** or **Python (Mamba-WSL2)**\n4. Run cells\n\n**Important**: Do NOT run the installation cells below if you've already followed the setup guide. Jump to Step 2 (Verify Installation).\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Install Mamba Packages (Optional - Only if Not Installed)\n\n**SKIP THIS STEP** if you've already followed the WSL2 setup guide.\n\nThis installs:\n- `torch==2.4.1+cu121`: PyTorch with CUDA 12.1 support\n- `numpy<2.0`: NumPy 1.x for compatibility\n- `causal-conv1d`: Required dependency (built from source)\n- `mamba-ssm`: The core Mamba library (built from source)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# OPTIONAL: Only run if you haven't completed the WSL2 setup guide\n# This will take 10-15 minutes to compile from source\n\n# Install PyTorch 2.4.1 with CUDA 12.1\n!pip install torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu121\n\n# Install NumPy 1.x\n!pip install \"numpy<2.0\"\n\n# Update C++ standard library\n!conda install -c conda-forge libstdcxx-ng -y\n\n# Build mamba-ssm from source (critical for compatibility)\n!pip cache purge\n!pip install causal-conv1d --no-binary causal-conv1d --no-build-isolation\n!pip install mamba-ssm --no-binary mamba-ssm --no-build-isolation"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Verify Installation\n",
    "\n",
    "Check that packages can be imported successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ mamba_ssm imported successfully\n",
      "  Version: 2.2.6.post3\n",
      "✓ causal_conv1d imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Test imports\n",
    "try:\n",
    "    import mamba_ssm\n",
    "    print(\"✓ mamba_ssm imported successfully\")\n",
    "    print(f\"  Version: {mamba_ssm.__version__ if hasattr(mamba_ssm, '__version__') else 'Unknown'}\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Failed to import mamba_ssm: {e}\")\n",
    "\n",
    "try:\n",
    "    import causal_conv1d\n",
    "    print(\"✓ causal_conv1d imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Failed to import causal_conv1d: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Check PyTorch and CUDA Availability\n",
    "\n",
    "Verify that PyTorch is installed and CUDA is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.4.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 4060\n",
      "Device count: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"⚠️  CUDA not available - Mamba may not work optimally\")\n",
    "    print(\"   Consider using a machine with CUDA support for best performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test Basic Mamba Forward Pass\n",
    "\n",
    "Create a simple Mamba layer and test a forward pass with random data.\n",
    "\n",
    "**Test Configuration**:\n",
    "- Input: (batch=2, sequence_length=64, dimension=128)\n",
    "- Model: Mamba with d_model=128, d_state=16, d_conv=4, expand=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 64, 128])\n",
      "Device: cuda\n",
      "Memory allocated: 0.06 MB\n"
     ]
    }
   ],
   "source": [
    "from mamba_ssm import Mamba\n",
    "\n",
    "# Test configuration\n",
    "batch_size = 2\n",
    "sequence_length = 64\n",
    "dim = 128\n",
    "\n",
    "# Create random input tensor\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "x = torch.randn(batch_size, sequence_length, dim).to(device)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\" if torch.cuda.is_available() else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Mamba model created successfully\n",
      "  Model parameters: 116,480\n",
      "  Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Create Mamba model\n",
    "model = Mamba(\n",
    "    d_model=dim,      # Model dimension\n",
    "    d_state=16,       # SSM state dimension\n",
    "    d_conv=4,         # Local convolution width\n",
    "    expand=2          # Expansion factor\n",
    ").to(device)\n",
    "\n",
    "print(\"✓ Mamba model created successfully\")\n",
    "print(f\"  Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Forward pass successful!\n",
      "  Input shape:  torch.Size([2, 64, 128])\n",
      "  Output shape: torch.Size([2, 64, 128])\n",
      "  Output dtype: torch.float32\n",
      "  Output device: cuda:0\n",
      "✓ Output is numerically stable (no NaN/Inf)\n",
      "\n",
      "Output statistics:\n",
      "  Mean: -0.0001\n",
      "  Std:  0.0411\n",
      "  Min:  -0.2012\n",
      "  Max:  0.1837\n"
     ]
    }
   ],
   "source": [
    "# Test forward pass\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        y = model(x)\n",
    "    \n",
    "    print(\"✓ Forward pass successful!\")\n",
    "    print(f\"  Input shape:  {x.shape}\")\n",
    "    print(f\"  Output shape: {y.shape}\")\n",
    "    print(f\"  Output dtype: {y.dtype}\")\n",
    "    print(f\"  Output device: {y.device}\")\n",
    "    \n",
    "    # Check for NaN or Inf\n",
    "    if torch.isnan(y).any():\n",
    "        print(\"⚠️  Warning: Output contains NaN values\")\n",
    "    elif torch.isinf(y).any():\n",
    "        print(\"⚠️  Warning: Output contains Inf values\")\n",
    "    else:\n",
    "        print(\"✓ Output is numerically stable (no NaN/Inf)\")\n",
    "    \n",
    "    # Show output statistics\n",
    "    print(f\"\\nOutput statistics:\")\n",
    "    print(f\"  Mean: {y.mean().item():.4f}\")\n",
    "    print(f\"  Std:  {y.std().item():.4f}\")\n",
    "    print(f\"  Min:  {y.min().item():.4f}\")\n",
    "    print(f\"  Max:  {y.max().item():.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Forward pass failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test Gradient Flow (Training Readiness)\n",
    "\n",
    "Verify that gradients can be computed (important for training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test gradient computation\ntry:\n    # Forward pass with gradients enabled\n    x_grad = torch.randn(batch_size, sequence_length, dim, requires_grad=True).to(device)\n    y_grad = model(x_grad)\n    \n    # Compute a dummy loss and backpropagate\n    loss = y_grad.mean()\n    loss.backward()\n    \n    print(\"✓ Gradient computation successful!\")\n    print(f\"  Loss value: {loss.item():.4f}\")\n    \n    # Check model parameter gradients\n    grad_params = [p for p in model.parameters() if p.grad is not None]\n    print(f\"  Parameters with gradients: {len(grad_params)}/{len(list(model.parameters()))}\")\n    \n    if len(grad_params) > 0:\n        avg_grad = torch.stack([p.grad.abs().mean() for p in grad_params]).mean()\n        print(f\"  Average gradient magnitude: {avg_grad.item():.6f}\")\n    \n    print(\"✓ Model is ready for training!\")\n    \nexcept Exception as e:\n    print(f\"✗ Gradient computation failed: {e}\")\n    import traceback\n    traceback.print_exc()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Memory Usage Test\n",
    "\n",
    "Check GPU memory usage for different sequence lengths (important for understanding limitations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing memory usage with different sequence lengths...\n",
      "\n",
      "Sequence length   64: Allocated:  17.45 MB, Reserved:  24.00 MB, Peak:  17.86 MB\n",
      "Sequence length  128: Allocated:  17.51 MB, Reserved:  24.00 MB, Peak:  18.31 MB\n",
      "Sequence length  256: Allocated:  17.64 MB, Reserved:  24.00 MB, Peak:  19.19 MB\n",
      "Sequence length  512: Allocated:  17.89 MB, Reserved:  26.00 MB, Peak:  20.95 MB\n",
      "Sequence length 1024: Allocated:  18.39 MB, Reserved:  28.00 MB, Peak:  24.47 MB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Testing memory usage with different sequence lengths...\\n\")\n",
    "    \n",
    "    sequence_lengths = [64, 128, 256, 512, 1024]\n",
    "    \n",
    "    for seq_len in sequence_lengths:\n",
    "        # Clear cache\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        try:\n",
    "            # Create input\n",
    "            x_test = torch.randn(1, seq_len, dim).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            with torch.no_grad():\n",
    "                y_test = model(x_test)\n",
    "            \n",
    "            # Get memory stats\n",
    "            mem_allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "            mem_reserved = torch.cuda.memory_reserved() / 1024**2\n",
    "            mem_peak = torch.cuda.max_memory_allocated() / 1024**2\n",
    "            \n",
    "            print(f\"Sequence length {seq_len:4d}: \"\n",
    "                  f\"Allocated: {mem_allocated:6.2f} MB, \"\n",
    "                  f\"Reserved: {mem_reserved:6.2f} MB, \"\n",
    "                  f\"Peak: {mem_peak:6.2f} MB\")\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                print(f\"Sequence length {seq_len:4d}: ✗ Out of memory\")\n",
    "                break\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    # Final cleanup\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"Skipping memory test (CUDA not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Step 7: Environment Summary\n\nDisplay the complete working environment configuration.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}