{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training: 0.5m Mangrove Dataset\n",
    "\n",
    "Train semantic segmentation models on the 0.5m resolution mangrove dataset.\n",
    "\n",
    "**Dataset**: 0.5m resolution aerial imagery\n",
    "- Pre-tiled 512x512 tiles stored as .npy\n",
    "- Binary classification: Not Mangrove (0) vs Mangrove (1)\n",
    "- Value 255 = ignore/no-data pixels\n",
    "\n",
    "**Models**: DeepLab, ResNet-UNet, SegFormer\n",
    "\n",
    "**Prerequisites**:\n",
    "- Run `02_preprocess_0_5m.ipynb` first to verify data and generate class weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.10.0+cu126\n",
      "CUDA available: True\n",
      "Device: NVIDIA GeForce RTX 4060\n",
      "Memory: 8.0 GB\n",
      "\n",
      "Using device: cuda\n",
      "\n",
      "Data root: c:\\vscode workspace\\ml-mangrove\\DroneClassification\\human_infra\\03_model_training\\..\\data\\0_5m\n",
      "Model: deeplab\n",
      "Experiment: deeplab_mangrove_0_5m\n",
      "Classes: ['not_mangrove', 'mangrove'] (ignore=255)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torchvision import tv_tensors\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION - Edit these paths and hyperparameters\n",
    "# ============================================================\n",
    "\n",
    "# Paths\n",
    "DATA_ROOT = Path('../data/0_5m')\n",
    "WEIGHTS_DIR = Path('../weights')\n",
    "PLOTS_DIR = Path('../plots/0_5m')\n",
    "EXPERIMENTS_DIR = Path('./experiments')\n",
    "\n",
    "# Data files\n",
    "IMAGES_FILE = DATA_ROOT / '512dataset_images.npy'\n",
    "LABELS_FILE = DATA_ROOT / '512dataset_labels.npy'\n",
    "\n",
    "# Ensure directories exist\n",
    "WEIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EXPERIMENTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 50\n",
    "INIT_LR = 5e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "NUM_WORKERS = 0  # Set >0 for multiprocessing (may cause issues on Windows)\n",
    "TRAIN_SPLIT = 0.8  # 80% train, 20% val\n",
    "\n",
    "# Model selection: 'deeplab', 'resnet_unet', 'segformer'\n",
    "MODEL_NAME = 'deeplab'\n",
    "EXPERIMENT_NAME = f'{MODEL_NAME}_mangrove_0_5m'\n",
    "\n",
    "# Class definitions (binary)\n",
    "CLASS_NAMES = ['not_mangrove', 'mangrove']\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "IGNORE_INDEX = 255  # No-data pixels\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "print(f\"\\nData root: {DATA_ROOT.absolute()}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Experiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"Classes: {CLASS_NAMES} (ignore={IGNORE_INDEX})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation pipeline:\n",
      "  - Random horizontal flip (p=0.5)\n",
      "  - Random vertical flip (p=0.5)\n",
      "  - Random 90-degree rotation\n"
     ]
    }
   ],
   "source": [
    "class Rotate90Only(v2.Transform):\n",
    "    \"\"\"Random 90-degree rotations (0, 90, 180, 270 degrees).\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform_image(self, img: torch.Tensor, k):\n",
    "        if k == 0:\n",
    "            return img\n",
    "        hdim, wdim = -2, -1\n",
    "        if k == 1:   # 90 degrees\n",
    "            return img.transpose(hdim, wdim).flip(wdim)\n",
    "        elif k == 2: # 180 degrees\n",
    "            return img.flip(hdim).flip(wdim)\n",
    "        elif k == 3: # 270 degrees\n",
    "            return img.transpose(hdim, wdim).flip(hdim)\n",
    "        return img\n",
    "\n",
    "    def forward(self, img: torch.Tensor, mask=None):\n",
    "        k = random.randint(0, 3)\n",
    "        img = self._transform_image(img, k)\n",
    "        if mask is not None:\n",
    "            mask = self._transform_image(mask, k)\n",
    "            return img, mask\n",
    "        return img\n",
    "\n",
    "\n",
    "# Training augmentation pipeline\n",
    "train_augmentation = v2.Compose([\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomVerticalFlip(p=0.5),\n",
    "    Rotate90Only(),\n",
    "])\n",
    "\n",
    "print(\"Augmentation pipeline:\")\n",
    "print(\"  - Random horizontal flip (p=0.5)\")\n",
    "print(\"  - Random vertical flip (p=0.5)\")\n",
    "print(\"  - Random 90-degree rotation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MangroveDataset class defined\n"
     ]
    }
   ],
   "source": [
    "class MangroveDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for 0.5m mangrove .npy data.\n",
    "    \n",
    "    Args:\n",
    "        images_path: Path to images .npy file\n",
    "        labels_path: Path to labels .npy file\n",
    "        indices: Optional subset of indices to use\n",
    "        augment: Whether to apply augmentation\n",
    "    \"\"\"\n",
    "    \n",
    "    # ImageNet normalization\n",
    "    MEAN = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    STD = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    \n",
    "    def __init__(self, images_path, labels_path, indices=None, augment=False):\n",
    "        # Load as memory-mapped for efficiency\n",
    "        self.images = np.load(images_path, mmap_mode='r')\n",
    "        self.labels = np.load(labels_path, mmap_mode='r')\n",
    "        self.indices = indices if indices is not None else np.arange(len(self.images))\n",
    "        self.augment = augment\n",
    "        \n",
    "        print(f\"Loaded {len(self.indices)} samples (augment={augment})\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = self.indices[idx]\n",
    "        \n",
    "        # Load image and label\n",
    "        image = self.images[real_idx].copy()  # Copy from mmap\n",
    "        label = self.labels[real_idx].copy()\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        # Image is (C, H, W) uint8\n",
    "        image = torch.from_numpy(image).float()\n",
    "        if image.max() > 1.5:\n",
    "            image = image / 255.0\n",
    "        \n",
    "        # Normalize with ImageNet stats\n",
    "        image = (image - self.MEAN) / self.STD\n",
    "        \n",
    "        # Label is (1, H, W) or (H, W)\n",
    "        label = torch.from_numpy(label).long()\n",
    "        if label.dim() == 3:\n",
    "            label = label.squeeze(0)\n",
    "        \n",
    "        # Apply augmentation\n",
    "        if self.augment:\n",
    "            image, label = train_augmentation(image, label)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "print(\"MangroveDataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading Data ===\n",
      "\n",
      "Images file: ..\\data\\0_5m\\512dataset_images.npy\n",
      "  exists: True\n",
      "Labels file: ..\\data\\0_5m\\512dataset_labels.npy\n",
      "  exists: True\n",
      "\n",
      "Images shape: (573, 3, 512, 512)\n",
      "Labels shape: (573, 1, 512, 512)\n",
      "Total samples: 573\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Loading Data ===\")\n",
    "print()\n",
    "\n",
    "# Verify files exist\n",
    "print(f\"Images file: {IMAGES_FILE}\")\n",
    "print(f\"  exists: {IMAGES_FILE.exists()}\")\n",
    "print(f\"Labels file: {LABELS_FILE}\")\n",
    "print(f\"  exists: {LABELS_FILE.exists()}\")\n",
    "print()\n",
    "\n",
    "if not IMAGES_FILE.exists() or not LABELS_FILE.exists():\n",
    "    raise FileNotFoundError(\"Data files not found!\")\n",
    "\n",
    "# Load to check shape\n",
    "images = np.load(IMAGES_FILE, mmap_mode='r')\n",
    "labels = np.load(LABELS_FILE, mmap_mode='r')\n",
    "\n",
    "print(f\"Images shape: {images.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print(f\"Total samples: {len(images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating Train/Val Split ===\n",
      "\n",
      "Loading existing split...\n",
      "\n",
      "Train: 458 samples\n",
      "Val:   115 samples\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Creating Train/Val Split ===\")\n",
    "print()\n",
    "\n",
    "# Check for existing split files\n",
    "train_split_file = DATA_ROOT / 'train_indices.npy'\n",
    "val_split_file = DATA_ROOT / 'val_indices.npy'\n",
    "\n",
    "if train_split_file.exists() and val_split_file.exists():\n",
    "    print(\"Loading existing split...\")\n",
    "    train_indices = np.load(train_split_file)\n",
    "    val_indices = np.load(val_split_file)\n",
    "else:\n",
    "    print(f\"Creating new {TRAIN_SPLIT*100:.0f}/{(1-TRAIN_SPLIT)*100:.0f} split...\")\n",
    "    n_samples = len(images)\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    split_idx = int(n_samples * TRAIN_SPLIT)\n",
    "    train_indices = indices[:split_idx]\n",
    "    val_indices = indices[split_idx:]\n",
    "    \n",
    "    # Save for reproducibility\n",
    "    np.save(train_split_file, train_indices)\n",
    "    np.save(val_split_file, val_indices)\n",
    "    print(f\"Saved: {train_split_file.name}, {val_split_file.name}\")\n",
    "\n",
    "print(f\"\\nTrain: {len(train_indices):,} samples\")\n",
    "print(f\"Val:   {len(val_indices):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating Datasets ===\n",
      "\n",
      "Loaded 458 samples (augment=True)\n",
      "Loaded 115 samples (augment=False)\n",
      "\n",
      "Sample shapes: Image torch.Size([3, 512, 512]), Mask torch.Size([512, 512])\n",
      "Mask unique values: [0, 1, 255]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Creating Datasets ===\")\n",
    "print()\n",
    "\n",
    "train_dataset = MangroveDataset(\n",
    "    IMAGES_FILE, LABELS_FILE,\n",
    "    indices=train_indices,\n",
    "    augment=True\n",
    ")\n",
    "\n",
    "val_dataset = MangroveDataset(\n",
    "    IMAGES_FILE, LABELS_FILE,\n",
    "    indices=val_indices,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "# Verify sample\n",
    "img, mask = train_dataset[0]\n",
    "print(f\"\\nSample shapes: Image {img.shape}, Mask {mask.shape}\")\n",
    "print(f\"Mask unique values: {torch.unique(mask).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating DataLoaders ===\n",
      "\n",
      "Batch size: 16\n",
      "Num workers: 0\n",
      "\n",
      "Batches per epoch:\n",
      "  Train: 29\n",
      "  Val:   8\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Creating DataLoaders ===\")\n",
    "print()\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Num workers: {NUM_WORKERS}\")\n",
    "print(f\"\\nBatches per epoch:\")\n",
    "print(f\"  Train: {len(train_loader):,}\")\n",
    "print(f\"  Val:   {len(val_loader):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Verifying Data ===\n",
      "\n",
      "Image batch: torch.Size([16, 3, 512, 512]), dtype=torch.float32\n",
      "Mask batch:  torch.Size([16, 512, 512]), dtype=torch.int64\n",
      "\n",
      "Image range: [-2.118, 2.640]\n",
      "Mask values: [0, 1, 255]\n",
      "Ignore pixels (255): 47.6%\n",
      "\n",
      "Data quality check passed\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Verifying Data ===\")\n",
    "print()\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "x, y = batch\n",
    "\n",
    "print(f\"Image batch: {x.shape}, dtype={x.dtype}\")\n",
    "print(f\"Mask batch:  {y.shape}, dtype={y.dtype}\")\n",
    "print(f\"\\nImage range: [{x.min():.3f}, {x.max():.3f}]\")\n",
    "print(f\"Mask values: {sorted(torch.unique(y).tolist())}\")\n",
    "\n",
    "# Count ignore pixels\n",
    "ignore_pct = (y == IGNORE_INDEX).float().mean() * 100\n",
    "print(f\"Ignore pixels (255): {ignore_pct:.1f}%\")\n",
    "\n",
    "# Check for NaN/Inf\n",
    "has_nan = torch.isnan(x).any()\n",
    "has_inf = torch.isinf(x).any()\n",
    "\n",
    "if not has_nan and not has_inf:\n",
    "    print(\"\\nData quality check passed\")\n",
    "else:\n",
    "    print(f\"\\nWARNING: NaN={has_nan}, Inf={has_inf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading Class Weights ===\n",
      "\n",
      "Loaded from: class_weights.json\n",
      "\n",
      "Class frequencies:\n",
      "  not_mangrove: 0.6267\n",
      "  mangrove    : 0.3733\n",
      "\n",
      "Class weights (inverse sqrt):\n",
      "  not_mangrove: 0.8711\n",
      "  mangrove    : 1.1289\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Loading Class Weights ===\")\n",
    "print()\n",
    "\n",
    "weights_file = DATA_ROOT / 'class_weights.json'\n",
    "\n",
    "if weights_file.exists():\n",
    "    with open(weights_file) as f:\n",
    "        weights_dict = json.load(f)\n",
    "    \n",
    "    class_frequencies = torch.tensor(weights_dict['class_frequencies'])\n",
    "    class_weights = torch.tensor(weights_dict['weights_inverse_sqrt']).to(device)\n",
    "    \n",
    "    print(f\"Loaded from: {weights_file.name}\")\n",
    "    print(f\"\\nClass frequencies:\")\n",
    "    for i, name in enumerate(CLASS_NAMES):\n",
    "        print(f\"  {name:12s}: {class_frequencies[i]:.4f}\")\n",
    "    print(f\"\\nClass weights (inverse sqrt):\")\n",
    "    for i, name in enumerate(CLASS_NAMES):\n",
    "        print(f\"  {name:12s}: {class_weights[i]:.4f}\")\n",
    "else:\n",
    "    # Fallback: balanced weights for binary\n",
    "    print(\"class_weights.json not found, using balanced weights\")\n",
    "    class_weights = torch.tensor([1.0, 1.0]).to(device)\n",
    "    print(f\"Class weights: {class_weights.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Import Models and Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adytc\\anaconda3\\envs\\mangrove\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported:\n",
      "  Models: DeepLab, ResNet_UNet, SegFormer\n",
      "  Losses: JaccardLoss, DiceLoss\n",
      "  Training: TrainingSession\n"
     ]
    }
   ],
   "source": [
    "# Add project root to path\n",
    "sys.path.insert(0, '../../')\n",
    "\n",
    "from models import DeepLab, ResNet_UNet, SegFormer, JaccardLoss, DiceLoss\n",
    "from training_utils import TrainingSession\n",
    "\n",
    "print(\"Imported:\")\n",
    "print(\"  Models: DeepLab, ResNet_UNet, SegFormer\")\n",
    "print(\"  Losses: JaccardLoss, DiceLoss\")\n",
    "print(\"  Training: TrainingSession\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Initializing Model: deeplab ===\n",
      "\n",
      "Model: deeplab\n",
      "Num classes: 2 (binary)\n",
      "Total parameters: 41,999,191\n",
      "Trainable parameters: 41,999,191\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"=== Initializing Model: {MODEL_NAME} ===\")\n",
    "print()\n",
    "\n",
    "if MODEL_NAME == 'deeplab':\n",
    "    model = DeepLab(\n",
    "        num_classes=NUM_CLASSES,\n",
    "        input_image_size=512,\n",
    "        backbone='resnet50',\n",
    "        output_stride=4\n",
    "    ).to(device)\n",
    "elif MODEL_NAME == 'resnet_unet':\n",
    "    model = ResNet_UNet(\n",
    "        num_classes=NUM_CLASSES\n",
    "    ).to(device)\n",
    "elif MODEL_NAME == 'segformer':\n",
    "    model = SegFormer(\n",
    "        num_classes=NUM_CLASSES\n",
    "    ).to(device)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model: {MODEL_NAME}\")\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Num classes: {NUM_CLASSES} (binary)\")\n",
    "print(f\"Total parameters: {num_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Setup Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Setting Up Loss Function ===\n",
      "\n",
      "Loss: JaccardLoss (CE + IoU + Boundary)\n",
      "  alpha (IoU weight): 0.3\n",
      "  boundary_weight: 0.3\n",
      "  ignore_index: 255\n",
      "  class_weights: ['0.87', '1.13']\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Setting Up Loss Function ===\")\n",
    "print()\n",
    "\n",
    "# JaccardLoss with ignore_index for 255 pixels\n",
    "loss_fn = JaccardLoss(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    weight=class_weights,\n",
    "    alpha=0.3,\n",
    "    boundary_weight=0.3,\n",
    "    ignore_index=IGNORE_INDEX  # Important: ignore 255 pixels\n",
    ")\n",
    "\n",
    "print(\"Loss: JaccardLoss (CE + IoU + Boundary)\")\n",
    "print(f\"  alpha (IoU weight): 0.3\")\n",
    "print(f\"  boundary_weight: 0.3\")\n",
    "print(f\"  ignore_index: {IGNORE_INDEX}\")\n",
    "print(f\"  class_weights: {[f'{w:.2f}' for w in class_weights.tolist()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Setup Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Setting Up Optimizer ===\n",
      "\n",
      "Optimizer: AdamW\n",
      "  Initial LR: 5e-05\n",
      "  Weight decay: 0.01\n",
      "\n",
      "Scheduler: CosineAnnealingLR\n",
      "  Total steps: 1,450\n",
      "  Steps per epoch: 29\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Setting Up Optimizer ===\")\n",
    "print()\n",
    "\n",
    "steps_per_epoch = len(train_loader)\n",
    "num_training_steps = NUM_EPOCHS * steps_per_epoch\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=INIT_LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=num_training_steps,\n",
    "    eta_min=0\n",
    ")\n",
    "\n",
    "print(f\"Optimizer: AdamW\")\n",
    "print(f\"  Initial LR: {INIT_LR}\")\n",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"\\nScheduler: CosineAnnealingLR\")\n",
    "print(f\"  Total steps: {num_training_steps:,}\")\n",
    "print(f\"  Steps per epoch: {steps_per_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Create Training Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating Training Session ===\n",
      "\n",
      "Using CUDA device.\n",
      "Experiment: deeplab_mangrove_0_5m\n",
      "Epochs: 50\n",
      "Ignore index: 255\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Creating Training Session ===\")\n",
    "print()\n",
    "\n",
    "trainer = TrainingSession(\n",
    "    model=model,\n",
    "    trainLoader=train_loader,\n",
    "    testLoader=val_loader,\n",
    "    lossFunc=loss_fn,\n",
    "    init_lr=INIT_LR,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    optimizer=optimizer,\n",
    "    class_names=CLASS_NAMES,\n",
    "    scheduler=scheduler,\n",
    "    ignore_index=IGNORE_INDEX  # Important for metrics calculation\n",
    ")\n",
    "\n",
    "print(f\"Experiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Ignore index: {IGNORE_INDEX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Starting Training ===\")\n",
    "print()\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Dataset: 0.5m Mangrove ({len(train_dataset):,} training samples)\")\n",
    "print(f\"Classes: {CLASS_NAMES}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print()\n",
    "\n",
    "# Start training\n",
    "trainer.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Evaluate on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Evaluating on Validation Set ===\")\n",
    "print()\n",
    "\n",
    "val_metrics = trainer.evaluate(val_loader)\n",
    "\n",
    "print(f\"\\nValidation Results:\")\n",
    "print(f\"  Pixel Accuracy: {val_metrics['Pixel_Accuracy']:.4f}\")\n",
    "print(f\"  Mean IoU: {val_metrics['IoU']:.4f}\")\n",
    "\n",
    "# Plot per-class IoU\n",
    "trainer.plot_metrics(\"Class IoU\", metrics_wanted=[\"class_ious\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Saving Model ===\")\n",
    "print()\n",
    "\n",
    "# Save to weights directory\n",
    "model_path = WEIGHTS_DIR / f'{EXPERIMENT_NAME}_final.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Saved: {model_path}\")\n",
    "\n",
    "# Also save training config\n",
    "config = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'class_names': CLASS_NAMES,\n",
    "    'ignore_index': IGNORE_INDEX,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'num_epochs': NUM_EPOCHS,\n",
    "    'init_lr': INIT_LR,\n",
    "    'val_pixel_accuracy': val_metrics['Pixel_Accuracy'],\n",
    "    'val_miou': val_metrics['IoU']\n",
    "}\n",
    "\n",
    "config_path = WEIGHTS_DIR / f'{EXPERIMENT_NAME}_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(f\"Saved: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(img):\n",
    "    \"\"\"Reverse ImageNet normalization.\"\"\"\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n",
    "    return torch.clamp(img * std + mean, 0, 1)\n",
    "\n",
    "\n",
    "# Class colors for visualization (binary)\n",
    "CLASS_COLORS = {\n",
    "    0: [0.8, 0.8, 0.8],  # Not mangrove - gray\n",
    "    1: [0.0, 0.7, 0.0],  # Mangrove - green\n",
    "}\n",
    "\n",
    "\n",
    "def visualize_predictions(model, dataset, indices, save_path=None):\n",
    "    \"\"\"Visualize model predictions on sample images.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(len(indices), 4, figsize=(16, 4*len(indices)))\n",
    "    fig.suptitle('0.5m Mangrove Predictions', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for row, idx in enumerate(indices):\n",
    "            img, mask = dataset[idx]\n",
    "            \n",
    "            # Get prediction\n",
    "            pred = model(img.unsqueeze(0).to(device))\n",
    "            pred_mask = torch.argmax(pred, dim=1).squeeze().cpu().numpy()\n",
    "            \n",
    "            # Prepare for display\n",
    "            img_np = denormalize(img).numpy().transpose(1, 2, 0)\n",
    "            mask_np = mask.numpy()\n",
    "            \n",
    "            # Image\n",
    "            axes[row, 0].imshow(img_np)\n",
    "            axes[row, 0].set_title('Image')\n",
    "            axes[row, 0].axis('off')\n",
    "            \n",
    "            # Ground truth (show ignore as white)\n",
    "            mask_display = np.zeros((*mask_np.shape, 3))\n",
    "            mask_display[mask_np == 0] = CLASS_COLORS[0]\n",
    "            mask_display[mask_np == 1] = CLASS_COLORS[1]\n",
    "            mask_display[mask_np == IGNORE_INDEX] = [1.0, 1.0, 1.0]  # White for ignore\n",
    "            axes[row, 1].imshow(mask_display)\n",
    "            axes[row, 1].set_title('Ground Truth')\n",
    "            axes[row, 1].axis('off')\n",
    "            \n",
    "            # Prediction\n",
    "            pred_display = np.zeros((*pred_mask.shape, 3))\n",
    "            pred_display[pred_mask == 0] = CLASS_COLORS[0]\n",
    "            pred_display[pred_mask == 1] = CLASS_COLORS[1]\n",
    "            axes[row, 2].imshow(pred_display)\n",
    "            axes[row, 2].set_title('Prediction')\n",
    "            axes[row, 2].axis('off')\n",
    "            \n",
    "            # Overlay\n",
    "            overlay = 0.6 * img_np + 0.4 * pred_display\n",
    "            axes[row, 3].imshow(np.clip(overlay, 0, 1))\n",
    "            axes[row, 3].set_title('Overlay')\n",
    "            axes[row, 3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=100, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize on validation set\n",
    "val_indices = np.random.choice(len(val_dataset), 4, replace=False).tolist()\n",
    "print(f\"Visualizing validation samples: {val_indices}\")\n",
    "\n",
    "visualize_predictions(\n",
    "    model,\n",
    "    val_dataset,\n",
    "    val_indices,\n",
    "    save_path=PLOTS_DIR / f'{EXPERIMENT_NAME}_predictions.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Training Complete\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Dataset: 0.5m Mangrove (binary)\")\n",
    "print(f\"Classes: {CLASS_NAMES}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print()\n",
    "print(f\"Validation Results:\")\n",
    "print(f\"  Pixel Accuracy: {val_metrics['Pixel_Accuracy']:.4f}\")\n",
    "print(f\"  Mean IoU: {val_metrics['IoU']:.4f}\")\n",
    "print()\n",
    "print(f\"Saved Files:\")\n",
    "print(f\"  Model: {WEIGHTS_DIR / f'{EXPERIMENT_NAME}_final.pth'}\")\n",
    "print(f\"  Config: {WEIGHTS_DIR / f'{EXPERIMENT_NAME}_config.json'}\")\n",
    "print(f\"  Plots: {PLOTS_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mangrove",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
