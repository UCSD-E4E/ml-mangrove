{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training: Landcover.ai Dataset\n",
    "\n",
    "Train semantic segmentation models on the Landcover.ai v1 dataset.\n",
    "\n",
    "**Dataset**: Landcover.ai v1\n",
    "- 41 high-resolution GeoTIFFs tiled to 512Ã—512\n",
    "- 5 classes: Background, Building, Woodland, Water, Road\n",
    "- Train/Val/Test splits provided\n",
    "\n",
    "**Models**: DeepLab, ResNet-UNet, SegFormer\n",
    "\n",
    "**Prerequisites**:\n",
    "- Run `01_preprocess_landcover.ipynb` first to generate tiles and class weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.10.0+cu126\n",
      "CUDA available: True\n",
      "Device: NVIDIA GeForce RTX 4060\n",
      "Memory: 8.0 GB\n",
      "\n",
      "Using device: cuda\n",
      "\n",
      "Data root: c:\\vscode workspace\\ml-mangrove\\DroneClassification\\human_infra\\03_model_training\\..\\data\\landcover.ai.v1\n",
      "Model: deeplab\n",
      "Experiment: deeplab_landcover_v1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torchvision import tv_tensors\n",
    "\n",
    "# ============================================================\n",
    "# CONFIGURATION - Edit these paths and hyperparameters\n",
    "# ============================================================\n",
    "\n",
    "# Paths\n",
    "DATA_ROOT = Path('../data/landcover.ai.v1')\n",
    "WEIGHTS_DIR = Path('../weights')\n",
    "PLOTS_DIR = Path('../plots/landcover')\n",
    "EXPERIMENTS_DIR = Path('./experiments')\n",
    "\n",
    "# Ensure directories exist\n",
    "WEIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EXPERIMENTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 50\n",
    "INIT_LR = 5e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "NUM_WORKERS = 0  # Set >0 for multiprocessing (may cause issues on Windows)\n",
    "\n",
    "# Model selection: 'deeplab', 'resnet_unet', 'segformer'\n",
    "MODEL_NAME = 'deeplab'\n",
    "EXPERIMENT_NAME = f'{MODEL_NAME}_landcover_v1'\n",
    "\n",
    "# Class definitions\n",
    "CLASS_NAMES = ['background', 'building', 'woodland', 'water', 'road']\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "print(f\"\\nData root: {DATA_ROOT.absolute()}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Experiment: {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation pipeline:\n",
      "  - Random horizontal flip (p=0.5)\n",
      "  - Random vertical flip (p=0.5)\n",
      "  - Random resized crop (scale 0.8-2.0)\n",
      "  - Random 90-degree rotation\n"
     ]
    }
   ],
   "source": [
    "class Rotate90Only(v2.Transform):\n",
    "    \"\"\"Random 90-degree rotations (0, 90, 180, 270 degrees).\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _transform_image(self, img: torch.Tensor, k):\n",
    "        if k == 0:\n",
    "            return img\n",
    "        hdim, wdim = -2, -1\n",
    "        if k == 1:   # 90 degrees\n",
    "            return img.transpose(hdim, wdim).flip(wdim)\n",
    "        elif k == 2: # 180 degrees\n",
    "            return img.flip(hdim).flip(wdim)\n",
    "        elif k == 3: # 270 degrees\n",
    "            return img.transpose(hdim, wdim).flip(hdim)\n",
    "        return img\n",
    "\n",
    "    def forward(self, img: torch.Tensor, mask=None):\n",
    "        k = random.randint(0, 3)\n",
    "        img = self._transform_image(img, k)\n",
    "        if mask is not None:\n",
    "            mask = self._transform_image(mask, k)\n",
    "            return img, mask\n",
    "        return img\n",
    "\n",
    "\n",
    "# Training augmentation pipeline\n",
    "train_augmentation = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.RandomVerticalFlip(p=0.5),\n",
    "    v2.RandomResizedCrop(size=512, scale=(0.8, 2.0), ratio=(0.9, 1.1)),\n",
    "    Rotate90Only(),\n",
    "])\n",
    "\n",
    "print(\"Augmentation pipeline:\")\n",
    "print(\"  - Random horizontal flip (p=0.5)\")\n",
    "print(\"  - Random vertical flip (p=0.5)\")\n",
    "print(\"  - Random resized crop (scale 0.8-2.0)\")\n",
    "print(\"  - Random 90-degree rotation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LandcoverAIDataset class defined\n"
     ]
    }
   ],
   "source": [
    "class LandcoverAIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Landcover.ai dataset loader for 512x512 tiles.\n",
    "    \n",
    "    Args:\n",
    "        root_dir: Path to dataset root containing 'output' folder\n",
    "        split_file: Path to text file listing tile names\n",
    "        augment: Whether to apply augmentation (True for training)\n",
    "    \"\"\"\n",
    "    \n",
    "    # ImageNet normalization\n",
    "    MEAN = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    STD = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    \n",
    "    def __init__(self, root_dir, split_file, augment=False):\n",
    "        self.output_dir = Path(root_dir) / 'output'\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Load tile names from split file\n",
    "        with open(split_file, 'r') as f:\n",
    "            self.tile_names = [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "        print(f\"Loaded {len(self.tile_names)} tiles from {Path(split_file).name}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tile_names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tile_name = self.tile_names[idx]\n",
    "        image_path = self.output_dir / f\"{tile_name}.jpg\"\n",
    "        mask_path = self.output_dir / f\"{tile_name}_m.png\"\n",
    "        \n",
    "        # Load image\n",
    "        image = np.array(Image.open(image_path))\n",
    "        if image.ndim == 3 and image.shape[2] == 4:\n",
    "            image = image[:, :, :3]  # Drop alpha channel\n",
    "        \n",
    "        # Load mask\n",
    "        mask = np.array(Image.open(mask_path))\n",
    "        if mask.ndim == 3:\n",
    "            mask = mask[:, :, 0]\n",
    "        mask = mask.astype(np.uint8)\n",
    "        \n",
    "        # Convert to PIL for transforms\n",
    "        image_pil = Image.fromarray(image)\n",
    "        mask_pil = Image.fromarray(mask)\n",
    "        \n",
    "        # Apply augmentation or just convert to tensor\n",
    "        if self.augment:\n",
    "            image_t, mask_t = train_augmentation(image_pil, mask_pil)\n",
    "        else:\n",
    "            image_t = v2.ToImage()(image_pil)\n",
    "            mask_t = v2.ToImage()(mask_pil)\n",
    "        \n",
    "        # Normalize image\n",
    "        image_t = image_t.float()\n",
    "        if image_t.max() > 1.5:\n",
    "            image_t = image_t / 255.0\n",
    "        image_t = (image_t - self.MEAN) / self.STD\n",
    "        \n",
    "        # Process mask\n",
    "        mask_t = mask_t.long()\n",
    "        if mask_t.dim() == 3 and mask_t.size(0) == 1:\n",
    "            mask_t = mask_t.squeeze(0)\n",
    "        \n",
    "        return image_t, mask_t\n",
    "\n",
    "\n",
    "print(\"LandcoverAIDataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading Datasets ===\n",
      "\n",
      "Dataset root: ..\\data\\landcover.ai.v1\n",
      "  output/ exists: True\n",
      "  train.txt exists: True\n",
      "  val.txt exists: True\n",
      "  test.txt exists: True\n",
      "\n",
      "Loaded 7470 tiles from train.txt\n",
      "Loaded 1602 tiles from val.txt\n",
      "Loaded 1602 tiles from test.txt\n",
      "\n",
      "Dataset sizes:\n",
      "  Train: 7,470 tiles (with augmentation)\n",
      "  Val:   1,602 tiles\n",
      "  Test:  1,602 tiles\n",
      "\n",
      "Sample shapes: Image torch.Size([3, 512, 512]), Mask torch.Size([512, 512])\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Loading Datasets ===\")\n",
    "print()\n",
    "\n",
    "# Verify paths exist\n",
    "print(f\"Dataset root: {DATA_ROOT}\")\n",
    "print(f\"  output/ exists: {(DATA_ROOT / 'output').exists()}\")\n",
    "print(f\"  train.txt exists: {(DATA_ROOT / 'train.txt').exists()}\")\n",
    "print(f\"  val.txt exists: {(DATA_ROOT / 'val.txt').exists()}\")\n",
    "print(f\"  test.txt exists: {(DATA_ROOT / 'test.txt').exists()}\")\n",
    "print()\n",
    "\n",
    "if not (DATA_ROOT / 'output').exists():\n",
    "    print(\"ERROR: Output directory not found!\")\n",
    "    print(\"Please run the preprocessing notebook first to generate tiles.\")\n",
    "else:\n",
    "    # Load datasets\n",
    "    train_dataset = LandcoverAIDataset(DATA_ROOT, DATA_ROOT / 'train.txt', augment=True)\n",
    "    val_dataset = LandcoverAIDataset(DATA_ROOT, DATA_ROOT / 'val.txt', augment=False)\n",
    "    test_dataset = LandcoverAIDataset(DATA_ROOT, DATA_ROOT / 'test.txt', augment=False)\n",
    "    \n",
    "    print(f\"\\nDataset sizes:\")\n",
    "    print(f\"  Train: {len(train_dataset):,} tiles (with augmentation)\")\n",
    "    print(f\"  Val:   {len(val_dataset):,} tiles\")\n",
    "    print(f\"  Test:  {len(test_dataset):,} tiles\")\n",
    "    \n",
    "    # Verify sample\n",
    "    img, mask = train_dataset[0]\n",
    "    print(f\"\\nSample shapes: Image {img.shape}, Mask {mask.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating DataLoaders ===\n",
      "\n",
      "Batch size: 16\n",
      "Num workers: 0\n",
      "\n",
      "Batches per epoch:\n",
      "  Train: 467\n",
      "  Val:   101\n",
      "  Test:  101\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Creating DataLoaders ===\")\n",
    "print()\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Num workers: {NUM_WORKERS}\")\n",
    "print(f\"\\nBatches per epoch:\")\n",
    "print(f\"  Train: {len(train_loader):,}\")\n",
    "print(f\"  Val:   {len(val_loader):,}\")\n",
    "print(f\"  Test:  {len(test_loader):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Verifying Data ===\n",
      "\n",
      "Image batch: torch.Size([16, 3, 512, 512]), dtype=torch.float32\n",
      "Mask batch:  torch.Size([16, 512, 512]), dtype=torch.int64\n",
      "\n",
      "Image range: [-2.118, 1.872]\n",
      "Mask values: [0, 1, 2, 3, 4]\n",
      "\n",
      "Data quality check passed\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Verifying Data ===\")\n",
    "print()\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "x, y = batch\n",
    "\n",
    "print(f\"Image batch: {x.shape}, dtype={x.dtype}\")\n",
    "print(f\"Mask batch:  {y.shape}, dtype={y.dtype}\")\n",
    "print(f\"\\nImage range: [{x.min():.3f}, {x.max():.3f}]\")\n",
    "print(f\"Mask values: {sorted(torch.unique(y).tolist())}\")\n",
    "\n",
    "# Check for NaN/Inf\n",
    "has_nan = torch.isnan(x).any()\n",
    "has_inf = torch.isinf(x).any()\n",
    "\n",
    "if not has_nan and not has_inf:\n",
    "    print(\"\\nData quality check passed\")\n",
    "else:\n",
    "    print(f\"\\nWARNING: NaN={has_nan}, Inf={has_inf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading Class Weights ===\n",
      "\n",
      "Loaded from: class_weights.json\n",
      "\n",
      "Class frequencies:\n",
      "  background  : 0.5729\n",
      "  building    : 0.0087\n",
      "  woodland    : 0.3298\n",
      "  water       : 0.0718\n",
      "  road        : 0.0167\n",
      "\n",
      "Class weights (inverse sqrt):\n",
      "  background  : 0.2618\n",
      "  building    : 2.1208\n",
      "  woodland    : 0.3450\n",
      "  water       : 0.7393\n",
      "  road        : 1.5330\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Loading Class Weights ===\")\n",
    "print()\n",
    "\n",
    "weights_file = DATA_ROOT / 'class_weights.json'\n",
    "\n",
    "if weights_file.exists():\n",
    "    with open(weights_file) as f:\n",
    "        weights_dict = json.load(f)\n",
    "    \n",
    "    class_frequencies = torch.tensor(weights_dict['class_frequencies'])\n",
    "    class_weights = torch.tensor(weights_dict['weights_inverse_sqrt']).to(device)\n",
    "    \n",
    "    print(f\"Loaded from: {weights_file.name}\")\n",
    "    print(f\"\\nClass frequencies:\")\n",
    "    for i, name in enumerate(CLASS_NAMES):\n",
    "        print(f\"  {name:12s}: {class_frequencies[i]:.4f}\")\n",
    "    print(f\"\\nClass weights (inverse sqrt):\")\n",
    "    for i, name in enumerate(CLASS_NAMES):\n",
    "        print(f\"  {name:12s}: {class_weights[i]:.4f}\")\n",
    "else:\n",
    "    # Fallback: use hardcoded weights from paper\n",
    "    print(\"class_weights.json not found, using default values\")\n",
    "    class_dist = torch.tensor([0.579, 0.015, 0.331, 0.065, 0.02])\n",
    "    class_weights = (1.0 / torch.sqrt(class_dist)).to(device)\n",
    "    \n",
    "    print(f\"\\nDefault class weights: {class_weights.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Import Models and Training Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported:\n",
      "  Models: DeepLab, ResNet_UNet, SegFormer\n",
      "  Losses: JaccardLoss, DiceLoss\n",
      "  Training: TrainingSession\n"
     ]
    }
   ],
   "source": [
    "# Add project root to path\n",
    "sys.path.insert(0, '../../')\n",
    "\n",
    "from models import DeepLab, ResNet_UNet, SegFormer, JaccardLoss, DiceLoss\n",
    "from training_utils import TrainingSession\n",
    "\n",
    "print(\"Imported:\")\n",
    "print(\"  Models: DeepLab, ResNet_UNet, SegFormer\")\n",
    "print(\"  Losses: JaccardLoss, DiceLoss\")\n",
    "print(\"  Training: TrainingSession\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Initializing Model: deeplab ===\n",
      "\n",
      "Model: deeplab\n",
      "Total parameters: 41,999,962\n",
      "Trainable parameters: 41,999,962\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"=== Initializing Model: {MODEL_NAME} ===\")\n",
    "print()\n",
    "\n",
    "if MODEL_NAME == 'deeplab':\n",
    "    model = DeepLab(\n",
    "        num_classes=NUM_CLASSES,\n",
    "        input_image_size=512,\n",
    "        backbone='resnet50',\n",
    "        output_stride=4\n",
    "    ).to(device)\n",
    "elif MODEL_NAME == 'resnet_unet':\n",
    "    model = ResNet_UNet(\n",
    "        num_classes=NUM_CLASSES\n",
    "    ).to(device)\n",
    "elif MODEL_NAME == 'segformer':\n",
    "    model = SegFormer(\n",
    "        num_classes=NUM_CLASSES\n",
    "    ).to(device)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model: {MODEL_NAME}\")\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Total parameters: {num_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Setup Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Setting Up Loss Function ===\n",
      "\n",
      "Loss: JaccardLoss (CE + IoU + Boundary)\n",
      "  alpha (IoU weight): 0.3\n",
      "  boundary_weight: 0.3\n",
      "  class_weights: ['0.26', '2.12', '0.35', '0.74', '1.53']\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Setting Up Loss Function ===\")\n",
    "print()\n",
    "\n",
    "# JaccardLoss combines CE + IoU + Boundary loss\n",
    "loss_fn = JaccardLoss(\n",
    "    num_classes=NUM_CLASSES,\n",
    "    weight=class_weights,\n",
    "    alpha=0.3,          # Weight for IoU component\n",
    "    boundary_weight=0.3  # Weight for boundary loss\n",
    ")\n",
    "\n",
    "print(\"Loss: JaccardLoss (CE + IoU + Boundary)\")\n",
    "print(f\"  alpha (IoU weight): 0.3\")\n",
    "print(f\"  boundary_weight: 0.3\")\n",
    "print(f\"  class_weights: {[f'{w:.2f}' for w in class_weights.tolist()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Setup Optimizer and Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Setting Up Optimizer ===\n",
      "\n",
      "Optimizer: AdamW\n",
      "  Initial LR: 5e-05\n",
      "  Weight decay: 0.01\n",
      "\n",
      "Scheduler: CosineAnnealingLR\n",
      "  Total steps: 23,350\n",
      "  Steps per epoch: 467\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Setting Up Optimizer ===\")\n",
    "print()\n",
    "\n",
    "steps_per_epoch = len(train_loader)\n",
    "num_training_steps = NUM_EPOCHS * steps_per_epoch\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=INIT_LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=num_training_steps,\n",
    "    eta_min=0\n",
    ")\n",
    "\n",
    "print(f\"Optimizer: AdamW\")\n",
    "print(f\"  Initial LR: {INIT_LR}\")\n",
    "print(f\"  Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"\\nScheduler: CosineAnnealingLR\")\n",
    "print(f\"  Total steps: {num_training_steps:,}\")\n",
    "print(f\"  Steps per epoch: {steps_per_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Create Training Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating Training Session ===\n",
      "\n",
      "Using CUDA device.\n",
      "Experiment: deeplab_landcover_v1\n",
      "Epochs: 50\n",
      "Mixed precision: enabled\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Creating Training Session ===\")\n",
    "print()\n",
    "\n",
    "trainer = TrainingSession(\n",
    "    model=model,\n",
    "    trainLoader=train_loader,\n",
    "    testLoader=val_loader,\n",
    "    lossFunc=loss_fn,\n",
    "    init_lr=INIT_LR,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    optimizer=optimizer,\n",
    "    class_names=CLASS_NAMES,\n",
    "    scheduler=scheduler,\n",
    ")\n",
    "\n",
    "print(f\"Experiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Mixed precision: enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 17:20:43,527 - INFO - Starting training: 50 epochs\n",
      "2026-02-05 17:20:43,528 - INFO - Model parameters: 41,999,962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting Training ===\n",
      "\n",
      "Model: deeplab\n",
      "Dataset: Landcover.ai (7,470 training tiles)\n",
      "Epochs: 50\n",
      "Batch size: 16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 2/467 [00:50<3:15:42, 25.25s/it, loss=0.8162, lr=0.000050]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\vscode workspace\\ml-mangrove\\DroneClassification\\human_infra\\03_model_training\\../..\\training_utils\\training_utils.py:219\u001b[39m, in \u001b[36mTrainingSession.learn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    215\u001b[39m     \u001b[38;5;28mself\u001b[39m.logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p.numel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m.model.parameters())\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_epochs):\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# Training phase\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     epoch_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m     \u001b[38;5;28mself\u001b[39m.training_loss.append(epoch_loss)\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# Validation phase\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\vscode workspace\\ml-mangrove\\DroneClassification\\human_infra\\03_model_training\\../..\\training_utils\\training_utils.py:277\u001b[39m, in \u001b[36mTrainingSession._train_epoch\u001b[39m\u001b[34m(self, epoch)\u001b[39m\n\u001b[32m    275\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.model(x)\n\u001b[32m    276\u001b[39m     pred = pred[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pred, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m pred\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlossFunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler.scale(loss).backward()\n\u001b[32m    279\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler.step(\u001b[38;5;28mself\u001b[39m.optimizer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adytc\\anaconda3\\envs\\mangrove\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adytc\\anaconda3\\envs\\mangrove\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\vscode workspace\\ml-mangrove\\DroneClassification\\human_infra\\03_model_training\\../..\\models\\loss.py:125\u001b[39m, in \u001b[36mJaccardLoss.forward\u001b[39m\u001b[34m(self, logits, labels)\u001b[39m\n\u001b[32m    122\u001b[39m jaccard_loss = \u001b[32m1\u001b[39m - iou\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.boundary_weight > \u001b[32m0.0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     boundary_loss_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mboundary_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m boundary_loss_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m boundary_loss_value > \u001b[32m0\u001b[39m:\n\u001b[32m    127\u001b[39m         jaccard_loss = (\u001b[32m1\u001b[39m-\u001b[38;5;28mself\u001b[39m.boundary_weight) * jaccard_loss + \u001b[38;5;28mself\u001b[39m.boundary_weight * boundary_loss_value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adytc\\anaconda3\\envs\\mangrove\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adytc\\anaconda3\\envs\\mangrove\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\vscode workspace\\ml-mangrove\\DroneClassification\\human_infra\\03_model_training\\../..\\models\\loss.py:366\u001b[39m, in \u001b[36mActiveBoundaryLoss.forward\u001b[39m\u001b[34m(self, logits, target)\u001b[39m\n\u001b[32m    363\u001b[39m prob = logits.softmax(dim=\u001b[32m1\u001b[39m)\n\u001b[32m    365\u001b[39m gt_bnd  = \u001b[38;5;28mself\u001b[39m.gt2boundary(target)\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m pred_bnd = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlogits2boundary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprob\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[38;5;66;03m# nothing predicted\u001b[39;00m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pred_bnd.sum() == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\vscode workspace\\ml-mangrove\\DroneClassification\\human_infra\\03_model_training\\../..\\models\\loss.py:297\u001b[39m, in \u001b[36mActiveBoundaryLoss.logits2boundary\u001b[39m\u001b[34m(self, prob)\u001b[39m\n\u001b[32m    294\u001b[39m kl_ud = kl_div(prob[:, :, \u001b[32m1\u001b[39m:, :], prob[:, :, :-\u001b[32m1\u001b[39m, :]).sum(\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    295\u001b[39m kl_lr = kl_div(prob[:, :, :, \u001b[32m1\u001b[39m:], prob[:, :, :, :-\u001b[32m1\u001b[39m]).sum(\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m kl_map = F.pad(kl_ud, (\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m)) + \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkl_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# adaptive threshold\u001b[39;00m\n\u001b[32m    300\u001b[39m eps = \u001b[32m1e-5\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adytc\\anaconda3\\envs\\mangrove\\Lib\\site-packages\\torch\\nn\\functional.py:5418\u001b[39m, in \u001b[36mpad\u001b[39m\u001b[34m(input, pad, mode, value)\u001b[39m\n\u001b[32m   5411\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mreplicate\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   5412\u001b[39m             \u001b[38;5;66;03m# Use slow decomp whose backward will be in terms of index_put.\u001b[39;00m\n\u001b[32m   5413\u001b[39m             \u001b[38;5;66;03m# importlib is required because the import cannot be top level\u001b[39;00m\n\u001b[32m   5414\u001b[39m             \u001b[38;5;66;03m# (cycle) and cannot be nested (TS doesn't support)\u001b[39;00m\n\u001b[32m   5415\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\n\u001b[32m   5416\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtorch._decomp.decompositions\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5417\u001b[39m             )._replication_pad(\u001b[38;5;28minput\u001b[39m, pad)\n\u001b[32m-> \u001b[39m\u001b[32m5418\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"=== Starting Training ===\")\n",
    "print()\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Dataset: Landcover.ai ({len(train_dataset):,} training tiles)\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print()\n",
    "\n",
    "# Start training\n",
    "trainer.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluating on Test Set ===\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +=: 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Evaluating on Test Set ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m test_metrics = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTest Set Results:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Pixel Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_metrics[\u001b[33m'\u001b[39m\u001b[33mPixel_Accuracy\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adytc\\anaconda3\\envs\\mangrove\\Lib\\site-packages\\torch\\utils\\_contextlib.py:124\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\vscode workspace\\ml-mangrove\\DroneClassification\\human_infra\\03_model_training\\../..\\training_utils\\training_utils.py:321\u001b[39m, in \u001b[36mTrainingSession.evaluate\u001b[39m\u001b[34m(self, dataloader)\u001b[39m\n\u001b[32m    319\u001b[39m         all_metrics[key] = [a + b \u001b[38;5;28;01mfor\u001b[39;00m a, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(all_metrics[key], value)]\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m321\u001b[39m         \u001b[43mall_metrics\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    323\u001b[39m     all_metrics[key] = value\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for +=: 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "print(\"=== Evaluating on Test Set ===\")\n",
    "print()\n",
    "\n",
    "test_metrics = trainer.evaluate(test_loader)\n",
    "\n",
    "print(f\"\\nTest Set Results:\")\n",
    "print(f\"  Pixel Accuracy: {test_metrics['Pixel_Accuracy']:.4f}\")\n",
    "print(f\"  Mean IoU: {test_metrics['IoU']:.4f}\")\n",
    "\n",
    "# Plot per-class IoU\n",
    "trainer.plot_metrics(\"Class IoU\", metrics_wanted=[\"class_ious\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Saving Model ===\")\n",
    "print()\n",
    "\n",
    "# Save to weights directory\n",
    "model_path = WEIGHTS_DIR / f'{EXPERIMENT_NAME}_final.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Saved: {model_path}\")\n",
    "\n",
    "# Also save training config\n",
    "config = {\n",
    "    'model_name': MODEL_NAME,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'class_names': CLASS_NAMES,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'num_epochs': NUM_EPOCHS,\n",
    "    'init_lr': INIT_LR,\n",
    "    'test_pixel_accuracy': test_metrics['Pixel_Accuracy'],\n",
    "    'test_miou': test_metrics['IoU']\n",
    "}\n",
    "\n",
    "config_path = WEIGHTS_DIR / f'{EXPERIMENT_NAME}_config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "print(f\"Saved: {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(img):\n",
    "    \"\"\"Reverse ImageNet normalization.\"\"\"\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n",
    "    return torch.clamp(img * std + mean, 0, 1)\n",
    "\n",
    "\n",
    "# Class colors for visualization\n",
    "CLASS_COLORS = {\n",
    "    0: [0.8, 0.8, 0.8],  # Background - gray\n",
    "    1: [1.0, 0.0, 0.0],  # Building - red\n",
    "    2: [0.0, 0.5, 0.0],  # Woodland - dark green\n",
    "    3: [0.0, 0.0, 1.0],  # Water - blue\n",
    "    4: [1.0, 1.0, 0.0],  # Road - yellow\n",
    "}\n",
    "\n",
    "\n",
    "def visualize_predictions(model, dataset, indices, save_path=None):\n",
    "    \"\"\"Visualize model predictions on sample images.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(len(indices), 4, figsize=(16, 4*len(indices)))\n",
    "    fig.suptitle('Landcover.ai Predictions', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for row, idx in enumerate(indices):\n",
    "            img, mask = dataset[idx]\n",
    "            \n",
    "            # Get prediction\n",
    "            pred = model(img.unsqueeze(0).to(device))\n",
    "            pred_mask = torch.argmax(pred, dim=1).squeeze().cpu().numpy()\n",
    "            \n",
    "            # Prepare for display\n",
    "            img_np = denormalize(img).numpy().transpose(1, 2, 0)\n",
    "            mask_np = mask.numpy()\n",
    "            \n",
    "            # Image\n",
    "            axes[row, 0].imshow(img_np)\n",
    "            axes[row, 0].set_title('Image')\n",
    "            axes[row, 0].axis('off')\n",
    "            \n",
    "            # Ground truth\n",
    "            axes[row, 1].imshow(mask_np, cmap='tab10', vmin=0, vmax=5)\n",
    "            axes[row, 1].set_title('Ground Truth')\n",
    "            axes[row, 1].axis('off')\n",
    "            \n",
    "            # Prediction\n",
    "            axes[row, 2].imshow(pred_mask, cmap='tab10', vmin=0, vmax=5)\n",
    "            axes[row, 2].set_title('Prediction')\n",
    "            axes[row, 2].axis('off')\n",
    "            \n",
    "            # Overlay\n",
    "            pred_rgb = np.zeros((*pred_mask.shape, 3))\n",
    "            for class_id, color in CLASS_COLORS.items():\n",
    "                pred_rgb[pred_mask == class_id] = color\n",
    "            overlay = 0.6 * img_np + 0.4 * pred_rgb\n",
    "            axes[row, 3].imshow(np.clip(overlay, 0, 1))\n",
    "            axes[row, 3].set_title('Overlay')\n",
    "            axes[row, 3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=100, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize on test set\n",
    "test_indices = np.random.choice(len(test_dataset), 4, replace=False).tolist()\n",
    "print(f\"Visualizing test samples: {test_indices}\")\n",
    "\n",
    "visualize_predictions(\n",
    "    model,\n",
    "    test_dataset,\n",
    "    test_indices,\n",
    "    save_path=PLOTS_DIR / f'{EXPERIMENT_NAME}_predictions.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"Training Complete\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Dataset: Landcover.ai v1\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print()\n",
    "print(f\"Test Results:\")\n",
    "print(f\"  Pixel Accuracy: {test_metrics['Pixel_Accuracy']:.4f}\")\n",
    "print(f\"  Mean IoU: {test_metrics['IoU']:.4f}\")\n",
    "print()\n",
    "print(f\"Saved Files:\")\n",
    "print(f\"  Model: {WEIGHTS_DIR / f'{EXPERIMENT_NAME}_final.pth'}\")\n",
    "print(f\"  Config: {WEIGHTS_DIR / f'{EXPERIMENT_NAME}_config.json'}\")\n",
    "print(f\"  Plots: {PLOTS_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mangrove",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
