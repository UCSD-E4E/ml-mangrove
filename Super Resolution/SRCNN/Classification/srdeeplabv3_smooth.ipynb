{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktlc5DhciOrq"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArQa3MxxMZpH"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt-get update\n",
        "!apt-get install libgdal-dev -y\n",
        "!apt-get install python-gdal -y\n",
        "!apt-get install python-numpy python-scipy -y\n",
        "!pip install rasterio\n",
        "!pip install fiona\n",
        "!pip install geopandas\n",
        "!pip install PyCRS\n",
        "!pip install -i https://test.pypi.org/simple/ gis-utils-pkg-dillhicks==0.0.76\n",
        "\n",
        "\n",
        "import os\n",
        "from tqdm.autonotebook import tqdm\n",
        "import numpy as np\n",
        "import gis_utils.raster as raster\n",
        "from rasterio.plot import reshape_as_image\n",
        "import rasterio\n",
        "from rasterio.plot import reshape_as_raster, reshape_as_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXhBjkLJLuBP",
        "outputId": "0eb3c69d-d2d4-4a01-bd53-3fd4e0cda8b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "%cd /content\n",
        "!mkdir drive\n",
        "%cd drive\n",
        "!mkdir MyDrive\n",
        "%cd ..\n",
        "%cd ..\n",
        "!google-drive-ocamlfuse /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/bnsreenu/python_for_microscopists/master/229_smooth_predictions_by_blending_patches/smooth_tiled_predictions.py"
      ],
      "metadata": {
        "id": "X3-cZC-ApXu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqnaN4vukS-r"
      },
      "source": [
        "# Loading Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB7bw1a7L-4B"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/srcnn/seg_tiles.zip .\n",
        "!unzip seg_tiles.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlzRzHAjJsEb"
      },
      "outputs": [],
      "source": [
        "from cv2 import resize\n",
        "import cv2\n",
        "\n",
        "\n",
        "def load_data(input_dir):\n",
        "    lr_image_filenames = [os.path.join(input_dir, x) for x in os.listdir(input_dir) if x.endswith(\".tif\") and x.startswith(\"S2_\")]\n",
        "\n",
        "    hr_image_filenames = [os.path.join(input_dir,\"hr_\" + os.path.split(x)[1]) for x in lr_image_filenames]\n",
        "    #Loading LR tiles (getting filenames with \"hr_\" removed)\n",
        "    label_filenames = [os.path.join(input_dir,\"label_hr_\" + os.path.split(x)[1]) for x in lr_image_filenames]\n",
        "\n",
        "    print(label_filenames)\n",
        "    labels = []\n",
        "    hr_images = []\n",
        "    lr_images = []\n",
        "\n",
        "    label_meta = []\n",
        "    hr_image_meta = []\n",
        "    lr_image_meta = []\n",
        "\n",
        "    transforms = []\n",
        "    for index in tqdm(range(len(lr_image_filenames))):\n",
        "        \n",
        "\n",
        "        label, label_meta_cur = raster.load_image(label_filenames[index])\n",
        "        lr_image, lr_image_meta_cur = raster.load_image(lr_image_filenames[index])\n",
        "        hr_image, hr_image_meta_cur = raster.load_image(hr_image_filenames[index])\n",
        "                \n",
        "        labels.append(np.squeeze(reshape_as_image(label.read())))\n",
        "        lr_images.append(resize(reshape_as_image(lr_image.read()), (48,48), interpolation = cv2.INTER_AREA))\n",
        "        hr_images.append(np.squeeze(reshape_as_image(hr_image.read())))\n",
        "        \n",
        "        \n",
        "        label_meta.append(label_meta_cur)\n",
        "        lr_image_meta.append(lr_image_meta_cur)\n",
        "        hr_image_meta.append(hr_image_meta_cur)\n",
        "        transforms.append(hr_image.transform)\n",
        "\n",
        "    return labels, lr_images, hr_images, label_meta, lr_image_meta, hr_image_meta, lr_image_filenames, hr_image_filenames, label_filenames, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTgdcH5tdcZr"
      },
      "outputs": [],
      "source": [
        "labels_train, lr_images_train, hr_images_train,\\\n",
        "labels_train_meta, lr_images_train_meta, hr_images_train_meta,\\\n",
        "lr_images_train_filenames, hr_images_train_filenames,\\\n",
        "labels_train_filenames, hr_train_transforms = load_data(\"train\")\n",
        "\n",
        "labels_test, lr_images_test, hr_images_test,\\\n",
        "labels_test_meta, lr_images_test_meta, hr_images_test_meta,\\\n",
        "lr_images_test_filenames, hr_images_test_filenames,\\\n",
        "labels_test_filenames, hr_test_transforms = load_data(\"test\")\n",
        "\n",
        "labels_train = np.asarray(labels_train)\n",
        "lr_images_train = np.asarray(lr_images_train)\n",
        "hr_images_train = np.asarray(hr_images_train)\n",
        "\n",
        "labels_test = np.asarray(labels_test)\n",
        "lr_images_test = np.asarray(lr_images_test)\n",
        "hr_images_test = np.asarray(hr_images_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poKsPJwzOjyg"
      },
      "source": [
        "# Loading SRCNN + Generating Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8pXm4dCR9_U"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/srcnn/sr_checkpoint.h5 .\n",
        "\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras import Sequential\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, Input, regularizers\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, UpSampling2D, Add, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def PSNRLoss(y_true, y_pred):\n",
        "\n",
        "    max_pixel = 1.0\n",
        "    return (10.0 * K.log((max_pixel ** 2) / (K.mean(K.square(y_pred - y_true), axis=-1)))) / 2.303\n",
        "\n",
        "\n",
        "srcnn = Sequential()\n",
        "srcnn.add(Conv2D(64,9,padding='same',input_shape=(48,48,4)))\n",
        "srcnn.add(Activation('relu'))\n",
        "srcnn.add(Conv2D(32,1,padding='same'))\n",
        "srcnn.add(Activation('relu'))\n",
        "srcnn.add(Conv2D(4,5,padding='same'))\n",
        "srcnn.compile(optimizer='adam', loss='mean_squared_error', metrics=[PSNRLoss])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuWyaXV0d--o"
      },
      "outputs": [],
      "source": [
        "srcnn.load_weights('sr_checkpoint.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from smooth_tiled_predictions import predict_img_with_smooth_windowing\n"
      ],
      "metadata": {
        "id": "fCeLsW9NrLQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JkakecAadVf"
      },
      "outputs": [],
      "source": [
        "r_train = srcnn.predict(lr_images_train)\n",
        "r_test = srcnn.predict(lr_images_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajPLeCuOBtgR"
      },
      "source": [
        "# DeeplabV3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPAmSQaLBw8X"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def convolution_block(\n",
        "    block_input,\n",
        "    num_filters=256,\n",
        "    kernel_size=3,\n",
        "    dilation_rate=1,\n",
        "    padding=\"same\",\n",
        "    use_bias=False,\n",
        "):\n",
        "    x = layers.Conv2D(\n",
        "        num_filters,\n",
        "        kernel_size=kernel_size,\n",
        "        dilation_rate=dilation_rate,\n",
        "        padding=\"same\",\n",
        "        use_bias=use_bias,\n",
        "        kernel_initializer=tf.keras.initializers.HeNormal(),\n",
        "    )(block_input)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    return tf.nn.relu(x)\n",
        "\n",
        "\n",
        "def DilatedSpatialPyramidPooling(dspp_input):\n",
        "    dims = dspp_input.shape\n",
        "    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
        "    x = convolution_block(x, kernel_size=1, use_bias=True)\n",
        "    out_pool = layers.UpSampling2D(\n",
        "        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "\n",
        "    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n",
        "    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n",
        "    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n",
        "    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
        "    output = convolution_block(x, kernel_size=1)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8HUlDIiClL-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def DeeplabV3Plus(image_size, num_classes):\n",
        "    model_input = tf.keras.Input(shape=(image_size, image_size, 4))\n",
        "    resnet50 = tf.keras.applications.ResNet50(\n",
        "        weights=None, include_top=False, input_tensor=model_input\n",
        "    )\n",
        "    x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n",
        "    x = DilatedSpatialPyramidPooling(x)\n",
        "\n",
        "    input_a = layers.UpSampling2D(\n",
        "        size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n",
        "    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n",
        "\n",
        "    x = layers.Concatenate(axis=-1)([input_a, input_b])\n",
        "    x = convolution_block(x)\n",
        "    x = convolution_block(x)\n",
        "    x = layers.UpSampling2D(\n",
        "        size=(image_size // x.shape[1], image_size // x.shape[2]),\n",
        "        interpolation=\"bilinear\",\n",
        "    )(x)\n",
        "    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\")(x)\n",
        "    return tf.keras.Model(inputs=model_input, outputs=model_output)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hr_images_train.shape"
      ],
      "metadata": {
        "id": "vIiZ8wQk6m3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEDSZTIeFBvR"
      },
      "outputs": [],
      "source": [
        "from cv2 import resize\n",
        "import cv2\n",
        "\n",
        "dlab_hr = np.array([resize(image, (240,240), interpolation = cv2.INTER_AREA) for image in hr_images_train])\n",
        "dlab_hr_test = np.array([resize(image, (240,240), interpolation = cv2.INTER_AREA) for image in hr_images_test])\n",
        "dlab_labels = np.array([resize(image, (240,240), interpolation = cv2.INTER_AREA) for image in labels_train])\n",
        "dlab_labels_test = np.array([resize(image, (240,240), interpolation = cv2.INTER_AREA) for image in labels_test])\n",
        "\n",
        "dlab_hr.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwlDPttpCtGY"
      },
      "outputs": [],
      "source": [
        "num_classes = 2\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "sr_model = DeeplabV3Plus(image_size=240, num_classes=num_classes)\n",
        "\n",
        "\n",
        "sr_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=\"sparse_categorical_crossentropy\", metrics = ['accuracy'])\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "\n",
        "early_stopper = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=50, verbose=1, mode='min')\n",
        "model_checkpoint =  ModelCheckpoint('unet_checkpoint_aug.h5', save_best_only = True)\n",
        "\n",
        "history = sr_model.fit(dlab_hr, dlab_labels,\n",
        "            epochs=50,\n",
        "            validation_data=(dlab_hr_test, dlab_labels_test),\n",
        "            callbacks=[early_stopper, model_checkpoint])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading DeepLab\n"
      ],
      "metadata": {
        "id": "PO41iUl2L8_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/srcnn/deeplab_checkpoint.h5 .\n",
        "\n",
        "\n",
        "num_classes = 2\n",
        "\n",
        "sr_model = DeeplabV3Plus(image_size=240, num_classes=num_classes)\n",
        "sr_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=\"sparse_categorical_crossentropy\", metrics = ['accuracy'])\n",
        "srcnn.load_weights('deeplab_checkpoint.h5')"
      ],
      "metadata": {
        "id": "YviIUO-72P5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mA2PHJlabhvH"
      },
      "source": [
        "# Mosaicing Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oT7Nk7O4bnpQ"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/srcnn/tiles.zip .\n",
        "\n",
        "\n",
        "!unzip tiles.zip -d testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCIAubz-c81E"
      },
      "outputs": [],
      "source": [
        "from cv2 import resize\n",
        "import cv2\n",
        "\n",
        "\n",
        "\n",
        "def load_unresolved(input_dir):\n",
        "    target_filenames = [os.path.join(input_dir, x) for x in os.listdir(input_dir) if x.endswith(\".tif\")]\n",
        "    lr_images = []\n",
        "\n",
        "    lr_meta = []\n",
        "\n",
        "    bounds = []\n",
        "    for index in tqdm(range(len(target_filenames))):\n",
        "        \n",
        "        lr_image = rasterio.open(target_filenames[index])\n",
        "        print(lr_image.read().shape)\n",
        "\n",
        "                \n",
        "        lr_images.append(resize(reshape_as_image(lr_image.read()), (48,48), interpolation = cv2.INTER_AREA))\n",
        "        \n",
        "        lr_meta.append(lr_image.meta)\n",
        "        bounds.append(lr_image.bounds)\n",
        "\n",
        "    return lr_images, lr_meta, target_filenames, bounds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rasterio.transform import from_bounds\n",
        "\n",
        "!cp /content/drive/MyDrive/srcnn/site1.tif .\n"
      ],
      "metadata": {
        "id": "T4N3RaUZUG1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufTuTyUYdLRo"
      },
      "outputs": [],
      "source": [
        "\n",
        "multiple = 3\n",
        "\n",
        "lr_image = rasterio.open(\"site1.tif\")\n",
        "lr_array = resize(reshape_as_image(lr_image.read()), (lr_image.width * multiple, lr_image.height * multiple), interpolation = cv2.INTER_AREA)\n",
        "\n",
        "\n",
        "\n",
        "sr_input = predict_img_with_smooth_windowing(\n",
        "    lr_array,\n",
        "    window_size=48,\n",
        "    subdivisions=2,  # Minimal amount of overlap for windowing. Must be an even number.\n",
        "    nb_classes=4,\n",
        "    pred_func=(\n",
        "        lambda img_batch_subdiv: srcnn.predict((img_batch_subdiv))\n",
        "    )\n",
        ")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "predictions_smooth = np.argmax(predictions_smooth, axis = 2)\n",
        "\n",
        "plt.imshow(predictions_smooth)\n",
        "plt.figure(figsize=(20, 20))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "S2PUt_iMTsfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_smooth = predict_img_with_smooth_windowing(\n",
        "    input,\n",
        "    window_size=240,\n",
        "    subdivisions=6,  # Minimal amount of overlap for windowing. Must be an even number.\n",
        "    nb_classes=2,\n",
        "    pred_func=(\n",
        "        lambda img_batch_subdiv: sr_model.predict((img_batch_subdiv))\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "wx1YF0E32Cy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "predictions_smooth = np.argmax(predictions_smooth, axis = 2)\n",
        "\n",
        "plt.imshow(predictions_smooth)\n",
        "plt.figure(figsize=(20, 20))"
      ],
      "metadata": {
        "id": "kSeDtfta_7c1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "srdeeplabv3_smooth.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "XqnaN4vukS-r",
        "poKsPJwzOjyg",
        "ajPLeCuOBtgR",
        "PO41iUl2L8_I"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}