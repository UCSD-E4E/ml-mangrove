{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c00efb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evanwu/ml-mangrove/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from Datasets import *\n",
    "from models import *\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torchgeo.models import resnet18, resnet50, get_weight\n",
    "from typing import List\n",
    "from prefetch_generator import BackgroundGenerator\n",
    "import torch.nn.functional as F\n",
    "from i2sb.runner import Runner\n",
    "from rasterio.plot import show\n",
    "\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Optional (for reproducibility in CUDA):\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84480133",
   "metadata": {},
   "source": [
    "We test our final ResNet UNet Diffusion model after having:\n",
    "1. Trained our diffusion layer on NAIP data\n",
    "2. Tuned our decoder layer on 1m/pixel drone data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5025e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Metal Performance Shaders (MPS) device.\n",
      "\n",
      "RUNNER_CKPT.keys()=dict_keys(['net', 'ema', 'optimizer', 'sched'])\n",
      "tuned_decoder_ckpt.keys()=odict_keys(['encoder.0.weight', 'encoder.1.weight', 'encoder.1.bias', 'encoder.1.running_mean', 'encoder.1.running_var', 'encoder.1.num_batches_tracked', 'encoder.4.0.conv1.weight', 'encoder.4.0.bn1.weight', 'encoder.4.0.bn1.bias', 'encoder.4.0.bn1.running_mean', 'encoder.4.0.bn1.running_var', 'encoder.4.0.bn1.num_batches_tracked', 'encoder.4.0.conv2.weight', 'encoder.4.0.bn2.weight', 'encoder.4.0.bn2.bias', 'encoder.4.0.bn2.running_mean', 'encoder.4.0.bn2.running_var', 'encoder.4.0.bn2.num_batches_tracked', 'encoder.4.1.conv1.weight', 'encoder.4.1.bn1.weight', 'encoder.4.1.bn1.bias', 'encoder.4.1.bn1.running_mean', 'encoder.4.1.bn1.running_var', 'encoder.4.1.bn1.num_batches_tracked', 'encoder.4.1.conv2.weight', 'encoder.4.1.bn2.weight', 'encoder.4.1.bn2.bias', 'encoder.4.1.bn2.running_mean', 'encoder.4.1.bn2.running_var', 'encoder.4.1.bn2.num_batches_tracked', 'encoder.5.0.conv1.weight', 'encoder.5.0.bn1.weight', 'encoder.5.0.bn1.bias', 'encoder.5.0.bn1.running_mean', 'encoder.5.0.bn1.running_var', 'encoder.5.0.bn1.num_batches_tracked', 'encoder.5.0.conv2.weight', 'encoder.5.0.bn2.weight', 'encoder.5.0.bn2.bias', 'encoder.5.0.bn2.running_mean', 'encoder.5.0.bn2.running_var', 'encoder.5.0.bn2.num_batches_tracked', 'encoder.5.0.downsample.0.weight', 'encoder.5.0.downsample.1.weight', 'encoder.5.0.downsample.1.bias', 'encoder.5.0.downsample.1.running_mean', 'encoder.5.0.downsample.1.running_var', 'encoder.5.0.downsample.1.num_batches_tracked', 'encoder.5.1.conv1.weight', 'encoder.5.1.bn1.weight', 'encoder.5.1.bn1.bias', 'encoder.5.1.bn1.running_mean', 'encoder.5.1.bn1.running_var', 'encoder.5.1.bn1.num_batches_tracked', 'encoder.5.1.conv2.weight', 'encoder.5.1.bn2.weight', 'encoder.5.1.bn2.bias', 'encoder.5.1.bn2.running_mean', 'encoder.5.1.bn2.running_var', 'encoder.5.1.bn2.num_batches_tracked', 'encoder.6.0.conv1.weight', 'encoder.6.0.bn1.weight', 'encoder.6.0.bn1.bias', 'encoder.6.0.bn1.running_mean', 'encoder.6.0.bn1.running_var', 'encoder.6.0.bn1.num_batches_tracked', 'encoder.6.0.conv2.weight', 'encoder.6.0.bn2.weight', 'encoder.6.0.bn2.bias', 'encoder.6.0.bn2.running_mean', 'encoder.6.0.bn2.running_var', 'encoder.6.0.bn2.num_batches_tracked', 'encoder.6.0.downsample.0.weight', 'encoder.6.0.downsample.1.weight', 'encoder.6.0.downsample.1.bias', 'encoder.6.0.downsample.1.running_mean', 'encoder.6.0.downsample.1.running_var', 'encoder.6.0.downsample.1.num_batches_tracked', 'encoder.6.1.conv1.weight', 'encoder.6.1.bn1.weight', 'encoder.6.1.bn1.bias', 'encoder.6.1.bn1.running_mean', 'encoder.6.1.bn1.running_var', 'encoder.6.1.bn1.num_batches_tracked', 'encoder.6.1.conv2.weight', 'encoder.6.1.bn2.weight', 'encoder.6.1.bn2.bias', 'encoder.6.1.bn2.running_mean', 'encoder.6.1.bn2.running_var', 'encoder.6.1.bn2.num_batches_tracked', 'encoder.7.0.conv1.weight', 'encoder.7.0.bn1.weight', 'encoder.7.0.bn1.bias', 'encoder.7.0.bn1.running_mean', 'encoder.7.0.bn1.running_var', 'encoder.7.0.bn1.num_batches_tracked', 'encoder.7.0.conv2.weight', 'encoder.7.0.bn2.weight', 'encoder.7.0.bn2.bias', 'encoder.7.0.bn2.running_mean', 'encoder.7.0.bn2.running_var', 'encoder.7.0.bn2.num_batches_tracked', 'encoder.7.0.downsample.0.weight', 'encoder.7.0.downsample.1.weight', 'encoder.7.0.downsample.1.bias', 'encoder.7.0.downsample.1.running_mean', 'encoder.7.0.downsample.1.running_var', 'encoder.7.0.downsample.1.num_batches_tracked', 'encoder.7.1.conv1.weight', 'encoder.7.1.bn1.weight', 'encoder.7.1.bn1.bias', 'encoder.7.1.bn1.running_mean', 'encoder.7.1.bn1.running_var', 'encoder.7.1.bn1.num_batches_tracked', 'encoder.7.1.conv2.weight', 'encoder.7.1.bn2.weight', 'encoder.7.1.bn2.bias', 'encoder.7.1.bn2.running_mean', 'encoder.7.1.bn2.running_var', 'encoder.7.1.bn2.num_batches_tracked', 'center.decoder.0.weight', 'center.decoder.0.bias', 'center.decoder.1.weight', 'center.decoder.1.bias', 'center.decoder.1.running_mean', 'center.decoder.1.running_var', 'center.decoder.1.num_batches_tracked', 'center.decoder.4.weight', 'center.decoder.4.bias', 'center.decoder.5.weight', 'center.decoder.5.bias', 'center.decoder.5.running_mean', 'center.decoder.5.running_var', 'center.decoder.5.num_batches_tracked', 'center.decoder.7.weight', 'center.decoder.7.bias', 'classification_head.0.decoder.0.weight', 'classification_head.0.decoder.0.bias', 'classification_head.0.decoder.1.weight', 'classification_head.0.decoder.1.bias', 'classification_head.0.decoder.1.running_mean', 'classification_head.0.decoder.1.running_var', 'classification_head.0.decoder.1.num_batches_tracked', 'classification_head.0.decoder.4.weight', 'classification_head.0.decoder.4.bias', 'classification_head.0.decoder.5.weight', 'classification_head.0.decoder.5.bias', 'classification_head.0.decoder.5.running_mean', 'classification_head.0.decoder.5.running_var', 'classification_head.0.decoder.5.num_batches_tracked', 'classification_head.0.decoder.7.weight', 'classification_head.0.decoder.7.bias', 'classification_head.1.upsampler.0.weight', 'classification_head.1.upsampler.0.bias', 'classification_head.1.upsampler.1.weight', 'classification_head.1.upsampler.1.bias', 'classification_head.1.upsampler.1.running_mean', 'classification_head.1.upsampler.1.running_var', 'classification_head.1.upsampler.1.num_batches_tracked', 'classification_head.1.upsampler.3.weight', 'classification_head.1.upsampler.3.bias', 'classification_head.1.upsampler.4.weight', 'classification_head.1.upsampler.4.bias', 'classification_head.1.upsampler.4.running_mean', 'classification_head.1.upsampler.4.running_var', 'classification_head.1.upsampler.4.num_batches_tracked', 'classification_head.1.upsampler.6.weight', 'classification_head.1.upsampler.6.bias', 'classification_head.2.upsampler.0.weight', 'classification_head.2.upsampler.0.bias', 'classification_head.2.upsampler.1.weight', 'classification_head.2.upsampler.1.bias', 'classification_head.2.upsampler.1.running_mean', 'classification_head.2.upsampler.1.running_var', 'classification_head.2.upsampler.1.num_batches_tracked', 'classification_head.2.upsampler.3.weight', 'classification_head.2.upsampler.3.bias', 'classification_head.2.upsampler.4.weight', 'classification_head.2.upsampler.4.bias', 'classification_head.2.upsampler.4.running_mean', 'classification_head.2.upsampler.4.running_var', 'classification_head.2.upsampler.4.num_batches_tracked', 'classification_head.2.upsampler.6.weight', 'classification_head.2.upsampler.6.bias', 'classification_head.3.upsampler.0.weight', 'classification_head.3.upsampler.0.bias', 'classification_head.3.upsampler.1.weight', 'classification_head.3.upsampler.1.bias', 'classification_head.3.upsampler.1.running_mean', 'classification_head.3.upsampler.1.running_var', 'classification_head.3.upsampler.1.num_batches_tracked', 'classification_head.3.upsampler.3.weight', 'classification_head.3.upsampler.3.bias', 'classification_head.3.upsampler.4.weight', 'classification_head.3.upsampler.4.bias', 'classification_head.3.upsampler.4.running_mean', 'classification_head.3.upsampler.4.running_var', 'classification_head.3.upsampler.4.num_batches_tracked', 'classification_head.3.upsampler.6.weight', 'classification_head.3.upsampler.6.bias', 'classification_head.5.weight', 'classification_head.5.bias'])\n",
      "center.keys()=odict_keys(['encoder.0.weight', 'encoder.1.weight', 'encoder.1.bias', 'encoder.1.running_mean', 'encoder.1.running_var', 'encoder.1.num_batches_tracked', 'encoder.4.0.conv1.weight', 'encoder.4.0.bn1.weight', 'encoder.4.0.bn1.bias', 'encoder.4.0.bn1.running_mean', 'encoder.4.0.bn1.running_var', 'encoder.4.0.bn1.num_batches_tracked', 'encoder.4.0.conv2.weight', 'encoder.4.0.bn2.weight', 'encoder.4.0.bn2.bias', 'encoder.4.0.bn2.running_mean', 'encoder.4.0.bn2.running_var', 'encoder.4.0.bn2.num_batches_tracked', 'encoder.4.1.conv1.weight', 'encoder.4.1.bn1.weight', 'encoder.4.1.bn1.bias', 'encoder.4.1.bn1.running_mean', 'encoder.4.1.bn1.running_var', 'encoder.4.1.bn1.num_batches_tracked', 'encoder.4.1.conv2.weight', 'encoder.4.1.bn2.weight', 'encoder.4.1.bn2.bias', 'encoder.4.1.bn2.running_mean', 'encoder.4.1.bn2.running_var', 'encoder.4.1.bn2.num_batches_tracked', 'encoder.5.0.conv1.weight', 'encoder.5.0.bn1.weight', 'encoder.5.0.bn1.bias', 'encoder.5.0.bn1.running_mean', 'encoder.5.0.bn1.running_var', 'encoder.5.0.bn1.num_batches_tracked', 'encoder.5.0.conv2.weight', 'encoder.5.0.bn2.weight', 'encoder.5.0.bn2.bias', 'encoder.5.0.bn2.running_mean', 'encoder.5.0.bn2.running_var', 'encoder.5.0.bn2.num_batches_tracked', 'encoder.5.0.downsample.0.weight', 'encoder.5.0.downsample.1.weight', 'encoder.5.0.downsample.1.bias', 'encoder.5.0.downsample.1.running_mean', 'encoder.5.0.downsample.1.running_var', 'encoder.5.0.downsample.1.num_batches_tracked', 'encoder.5.1.conv1.weight', 'encoder.5.1.bn1.weight', 'encoder.5.1.bn1.bias', 'encoder.5.1.bn1.running_mean', 'encoder.5.1.bn1.running_var', 'encoder.5.1.bn1.num_batches_tracked', 'encoder.5.1.conv2.weight', 'encoder.5.1.bn2.weight', 'encoder.5.1.bn2.bias', 'encoder.5.1.bn2.running_mean', 'encoder.5.1.bn2.running_var', 'encoder.5.1.bn2.num_batches_tracked', 'encoder.6.0.conv1.weight', 'encoder.6.0.bn1.weight', 'encoder.6.0.bn1.bias', 'encoder.6.0.bn1.running_mean', 'encoder.6.0.bn1.running_var', 'encoder.6.0.bn1.num_batches_tracked', 'encoder.6.0.conv2.weight', 'encoder.6.0.bn2.weight', 'encoder.6.0.bn2.bias', 'encoder.6.0.bn2.running_mean', 'encoder.6.0.bn2.running_var', 'encoder.6.0.bn2.num_batches_tracked', 'encoder.6.0.downsample.0.weight', 'encoder.6.0.downsample.1.weight', 'encoder.6.0.downsample.1.bias', 'encoder.6.0.downsample.1.running_mean', 'encoder.6.0.downsample.1.running_var', 'encoder.6.0.downsample.1.num_batches_tracked', 'encoder.6.1.conv1.weight', 'encoder.6.1.bn1.weight', 'encoder.6.1.bn1.bias', 'encoder.6.1.bn1.running_mean', 'encoder.6.1.bn1.running_var', 'encoder.6.1.bn1.num_batches_tracked', 'encoder.6.1.conv2.weight', 'encoder.6.1.bn2.weight', 'encoder.6.1.bn2.bias', 'encoder.6.1.bn2.running_mean', 'encoder.6.1.bn2.running_var', 'encoder.6.1.bn2.num_batches_tracked', 'encoder.7.0.conv1.weight', 'encoder.7.0.bn1.weight', 'encoder.7.0.bn1.bias', 'encoder.7.0.bn1.running_mean', 'encoder.7.0.bn1.running_var', 'encoder.7.0.bn1.num_batches_tracked', 'encoder.7.0.conv2.weight', 'encoder.7.0.bn2.weight', 'encoder.7.0.bn2.bias', 'encoder.7.0.bn2.running_mean', 'encoder.7.0.bn2.running_var', 'encoder.7.0.bn2.num_batches_tracked', 'encoder.7.0.downsample.0.weight', 'encoder.7.0.downsample.1.weight', 'encoder.7.0.downsample.1.bias', 'encoder.7.0.downsample.1.running_mean', 'encoder.7.0.downsample.1.running_var', 'encoder.7.0.downsample.1.num_batches_tracked', 'encoder.7.1.conv1.weight', 'encoder.7.1.bn1.weight', 'encoder.7.1.bn1.bias', 'encoder.7.1.bn1.running_mean', 'encoder.7.1.bn1.running_var', 'encoder.7.1.bn1.num_batches_tracked', 'encoder.7.1.conv2.weight', 'encoder.7.1.bn2.weight', 'encoder.7.1.bn2.bias', 'encoder.7.1.bn2.running_mean', 'encoder.7.1.bn2.running_var', 'encoder.7.1.bn2.num_batches_tracked', 'center.decoder.0.weight', 'center.decoder.0.bias', 'center.decoder.1.weight', 'center.decoder.1.bias', 'center.decoder.1.running_mean', 'center.decoder.1.running_var', 'center.decoder.1.num_batches_tracked', 'center.decoder.4.weight', 'center.decoder.4.bias', 'center.decoder.5.weight', 'center.decoder.5.bias', 'center.decoder.5.running_mean', 'center.decoder.5.running_var', 'center.decoder.5.num_batches_tracked', 'center.decoder.7.weight', 'center.decoder.7.bias', 'classification_head.0.decoder.0.weight', 'classification_head.0.decoder.0.bias', 'classification_head.0.decoder.1.weight', 'classification_head.0.decoder.1.bias', 'classification_head.0.decoder.1.running_mean', 'classification_head.0.decoder.1.running_var', 'classification_head.0.decoder.1.num_batches_tracked', 'classification_head.0.decoder.4.weight', 'classification_head.0.decoder.4.bias', 'classification_head.0.decoder.5.weight', 'classification_head.0.decoder.5.bias', 'classification_head.0.decoder.5.running_mean', 'classification_head.0.decoder.5.running_var', 'classification_head.0.decoder.5.num_batches_tracked', 'classification_head.0.decoder.7.weight', 'classification_head.0.decoder.7.bias', 'classification_head.1.upsampler.0.weight', 'classification_head.1.upsampler.0.bias', 'classification_head.1.upsampler.1.weight', 'classification_head.1.upsampler.1.bias', 'classification_head.1.upsampler.1.running_mean', 'classification_head.1.upsampler.1.running_var', 'classification_head.1.upsampler.1.num_batches_tracked', 'classification_head.1.upsampler.3.weight', 'classification_head.1.upsampler.3.bias', 'classification_head.1.upsampler.4.weight', 'classification_head.1.upsampler.4.bias', 'classification_head.1.upsampler.4.running_mean', 'classification_head.1.upsampler.4.running_var', 'classification_head.1.upsampler.4.num_batches_tracked', 'classification_head.1.upsampler.6.weight', 'classification_head.1.upsampler.6.bias', 'classification_head.2.upsampler.0.weight', 'classification_head.2.upsampler.0.bias', 'classification_head.2.upsampler.1.weight', 'classification_head.2.upsampler.1.bias', 'classification_head.2.upsampler.1.running_mean', 'classification_head.2.upsampler.1.running_var', 'classification_head.2.upsampler.1.num_batches_tracked', 'classification_head.2.upsampler.3.weight', 'classification_head.2.upsampler.3.bias', 'classification_head.2.upsampler.4.weight', 'classification_head.2.upsampler.4.bias', 'classification_head.2.upsampler.4.running_mean', 'classification_head.2.upsampler.4.running_var', 'classification_head.2.upsampler.4.num_batches_tracked', 'classification_head.2.upsampler.6.weight', 'classification_head.2.upsampler.6.bias', 'classification_head.3.upsampler.0.weight', 'classification_head.3.upsampler.0.bias', 'classification_head.3.upsampler.1.weight', 'classification_head.3.upsampler.1.bias', 'classification_head.3.upsampler.1.running_mean', 'classification_head.3.upsampler.1.running_var', 'classification_head.3.upsampler.1.num_batches_tracked', 'classification_head.3.upsampler.3.weight', 'classification_head.3.upsampler.3.bias', 'classification_head.3.upsampler.4.weight', 'classification_head.3.upsampler.4.bias', 'classification_head.3.upsampler.4.running_mean', 'classification_head.3.upsampler.4.running_var', 'classification_head.3.upsampler.4.num_batches_tracked', 'classification_head.3.upsampler.6.weight', 'classification_head.3.upsampler.6.bias', 'classification_head.5.weight', 'classification_head.5.bias'])\n"
     ]
    }
   ],
   "source": [
    "LOSS = JaccardLoss()\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA device.\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"Using Apple Metal Performance Shaders (MPS) device.\\n\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"WARNING: No GPU found. Defaulting to CPU.\")\n",
    "\n",
    "PRETRAINED_CENTER_DECODER_0_WEIGHT_FIRST_VALUE = 0.6512\n",
    "PRETRAINED_RGB_MOCO_WEIGHT_FIRST_VALUE = 0.0596\n",
    "PRETRAINED_ALL_MOCO_WEIGHT_FIRST_VALUE = 0.0150\n",
    "\n",
    "# RGB and Satellite metrics\n",
    "RGB_MEANS = [0.485, 0.456, 0.406]\n",
    "RGB_SDS = [0.229, 0.224, 0.225]\n",
    "'''\n",
    "SATELLITE_MEANS, SATELLITE_SDS = calculate_band_stats(LIST_TILED_SATELLITE_FILE)\n",
    "SATELLITE_SDS[10] = 1e-6 # So we don't get divide by 0 error during normalization\n",
    "print(f\"SATELLITE_MEANS: {SATELLITE_MEANS}\")\n",
    "print(f\"SATELLITE_SDS: {SATELLITE_SDS}\")\n",
    "'''\n",
    "SATELLITE_MEANS = [0.026770996230905415, 0.09145129995394104, 0.1374701370800858, 0.12629982008698087, 0.08348537283615888, 0.15234747745903673, 0.1755682088182016, 0.19574084608396258, 0.1958724138455955, 0.19676238458042725, 0.0, 0.15115207971951478, 0.0945010388495151]\n",
    "SATELLITE_SDS = [0.035945900674540114, 0.09533489898570938, 0.1089656215582545, 0.11637876785009475, 0.0598930857979902, 0.10018866040486654, 0.11469476295117267, 0.1271611062494966, 0.1263377774611167, 0.12642673147611674, 1e-06, 0.11149914947615584, 0.07974405349672085]\n",
    "\n",
    "# reshape and remake means/sds arrays into tensors\n",
    "RGB_MEANS_TENSOR = torch.tensor(RGB_MEANS, dtype=torch.float32).view(3, 1, 1).to(DEVICE)\n",
    "RGB_SDS_TENSOR = torch.tensor(RGB_SDS, dtype=torch.float32).view(3, 1, 1).to(DEVICE)\n",
    "SATELLITE_MEANS_TENSOR = torch.tensor(SATELLITE_MEANS, dtype=torch.float32).view(13, 1, 1).to(DEVICE)\n",
    "SATELLITE_SDS_TENSOR = torch.tensor(SATELLITE_SDS, dtype=torch.float32).view(13, 1, 1).to(DEVICE)\n",
    "\n",
    "INTERVAL = 100\n",
    "\n",
    "# saved weights from runner (we want the diffusion layer weights specifically)\n",
    "# contains [net, ema, optimizer, sched] weights\n",
    "RUNNER_CKPT_PATH = '/Users/evanwu/ml-mangrove/Super Resolution/Schrodinger Diffusion/results/test/model_001000.pt'\n",
    "RUNNER_CKPT = torch.load(RUNNER_CKPT_PATH, map_location=DEVICE)\n",
    "print(f\"RUNNER_CKPT.keys()={RUNNER_CKPT.keys()}\")\n",
    "\n",
    "# saved weights from decoder tuning\n",
    "TUNED_DECODER_CKPT_PATH = '/Users/evanwu/Downloads/finat3_200.pth'\n",
    "TUNED_DECODER_CKPT = torch.load(TUNED_DECODER_CKPT_PATH, map_location=DEVICE)\n",
    "print(f\"TUNED_DECODER_CKPT.keys()={TUNED_DECODER_CKPT.keys()}\")\n",
    "\n",
    "# saved weights from pretrained\n",
    "CENTER_PATH = '/Users/evanwu/Downloads/224_moco_resnet18_noskip.pth'\n",
    "CENTER = torch.load(CENTER_PATH, map_location=DEVICE)\n",
    "print(f\"CENTER.keys()={CENTER.keys()}\")\n",
    "\n",
    "OM_SATELLITE_PATH = 'data/one_meter_drone/224dataset_satellite.npy'\n",
    "OM_LABEL_PATH = 'data/one_meter_drone/224dataset_label.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d316e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy input: latent space channels=512\n",
      "Loaded 'net' and 'ema' from checkpoint path\n",
      "Built schrodinger_bridge Diffusion Model with 100 steps and i2sb beta schedule!\n",
      "encoder.0.weight\n",
      "encoder.1.weight\n",
      "encoder.1.bias\n",
      "encoder.1.running_mean\n",
      "encoder.1.running_var\n",
      "encoder.1.num_batches_tracked\n",
      "encoder.4.0.conv1.weight\n",
      "encoder.4.0.bn1.weight\n",
      "encoder.4.0.bn1.bias\n",
      "encoder.4.0.bn1.running_mean\n",
      "encoder.4.0.bn1.running_var\n",
      "encoder.4.0.bn1.num_batches_tracked\n",
      "encoder.4.0.conv2.weight\n",
      "encoder.4.0.bn2.weight\n",
      "encoder.4.0.bn2.bias\n",
      "encoder.4.0.bn2.running_mean\n",
      "encoder.4.0.bn2.running_var\n",
      "encoder.4.0.bn2.num_batches_tracked\n",
      "encoder.4.1.conv1.weight\n",
      "encoder.4.1.bn1.weight\n",
      "encoder.4.1.bn1.bias\n",
      "encoder.4.1.bn1.running_mean\n",
      "encoder.4.1.bn1.running_var\n",
      "encoder.4.1.bn1.num_batches_tracked\n",
      "encoder.4.1.conv2.weight\n",
      "encoder.4.1.bn2.weight\n",
      "encoder.4.1.bn2.bias\n",
      "encoder.4.1.bn2.running_mean\n",
      "encoder.4.1.bn2.running_var\n",
      "encoder.4.1.bn2.num_batches_tracked\n",
      "encoder.5.0.conv1.weight\n",
      "encoder.5.0.bn1.weight\n",
      "encoder.5.0.bn1.bias\n",
      "encoder.5.0.bn1.running_mean\n",
      "encoder.5.0.bn1.running_var\n",
      "encoder.5.0.bn1.num_batches_tracked\n",
      "encoder.5.0.conv2.weight\n",
      "encoder.5.0.bn2.weight\n",
      "encoder.5.0.bn2.bias\n",
      "encoder.5.0.bn2.running_mean\n",
      "encoder.5.0.bn2.running_var\n",
      "encoder.5.0.bn2.num_batches_tracked\n",
      "encoder.5.0.downsample.0.weight\n",
      "encoder.5.0.downsample.1.weight\n",
      "encoder.5.0.downsample.1.bias\n",
      "encoder.5.0.downsample.1.running_mean\n",
      "encoder.5.0.downsample.1.running_var\n",
      "encoder.5.0.downsample.1.num_batches_tracked\n",
      "encoder.5.1.conv1.weight\n",
      "encoder.5.1.bn1.weight\n",
      "encoder.5.1.bn1.bias\n",
      "encoder.5.1.bn1.running_mean\n",
      "encoder.5.1.bn1.running_var\n",
      "encoder.5.1.bn1.num_batches_tracked\n",
      "encoder.5.1.conv2.weight\n",
      "encoder.5.1.bn2.weight\n",
      "encoder.5.1.bn2.bias\n",
      "encoder.5.1.bn2.running_mean\n",
      "encoder.5.1.bn2.running_var\n",
      "encoder.5.1.bn2.num_batches_tracked\n",
      "encoder.6.0.conv1.weight\n",
      "encoder.6.0.bn1.weight\n",
      "encoder.6.0.bn1.bias\n",
      "encoder.6.0.bn1.running_mean\n",
      "encoder.6.0.bn1.running_var\n",
      "encoder.6.0.bn1.num_batches_tracked\n",
      "encoder.6.0.conv2.weight\n",
      "encoder.6.0.bn2.weight\n",
      "encoder.6.0.bn2.bias\n",
      "encoder.6.0.bn2.running_mean\n",
      "encoder.6.0.bn2.running_var\n",
      "encoder.6.0.bn2.num_batches_tracked\n",
      "encoder.6.0.downsample.0.weight\n",
      "encoder.6.0.downsample.1.weight\n",
      "encoder.6.0.downsample.1.bias\n",
      "encoder.6.0.downsample.1.running_mean\n",
      "encoder.6.0.downsample.1.running_var\n",
      "encoder.6.0.downsample.1.num_batches_tracked\n",
      "encoder.6.1.conv1.weight\n",
      "encoder.6.1.bn1.weight\n",
      "encoder.6.1.bn1.bias\n",
      "encoder.6.1.bn1.running_mean\n",
      "encoder.6.1.bn1.running_var\n",
      "encoder.6.1.bn1.num_batches_tracked\n",
      "encoder.6.1.conv2.weight\n",
      "encoder.6.1.bn2.weight\n",
      "encoder.6.1.bn2.bias\n",
      "encoder.6.1.bn2.running_mean\n",
      "encoder.6.1.bn2.running_var\n",
      "encoder.6.1.bn2.num_batches_tracked\n",
      "encoder.7.0.conv1.weight\n",
      "encoder.7.0.bn1.weight\n",
      "encoder.7.0.bn1.bias\n",
      "encoder.7.0.bn1.running_mean\n",
      "encoder.7.0.bn1.running_var\n",
      "encoder.7.0.bn1.num_batches_tracked\n",
      "encoder.7.0.conv2.weight\n",
      "encoder.7.0.bn2.weight\n",
      "encoder.7.0.bn2.bias\n",
      "encoder.7.0.bn2.running_mean\n",
      "encoder.7.0.bn2.running_var\n",
      "encoder.7.0.bn2.num_batches_tracked\n",
      "encoder.7.0.downsample.0.weight\n",
      "encoder.7.0.downsample.1.weight\n",
      "encoder.7.0.downsample.1.bias\n",
      "encoder.7.0.downsample.1.running_mean\n",
      "encoder.7.0.downsample.1.running_var\n",
      "encoder.7.0.downsample.1.num_batches_tracked\n",
      "encoder.7.1.conv1.weight\n",
      "encoder.7.1.bn1.weight\n",
      "encoder.7.1.bn1.bias\n",
      "encoder.7.1.bn1.running_mean\n",
      "encoder.7.1.bn1.running_var\n",
      "encoder.7.1.bn1.num_batches_tracked\n",
      "encoder.7.1.conv2.weight\n",
      "encoder.7.1.bn2.weight\n",
      "encoder.7.1.bn2.bias\n",
      "encoder.7.1.bn2.running_mean\n",
      "encoder.7.1.bn2.running_var\n",
      "encoder.7.1.bn2.num_batches_tracked\n",
      "center.decoder.0.weight\n",
      "center.decoder.0.bias\n",
      "center.decoder.1.weight\n",
      "center.decoder.1.bias\n",
      "center.decoder.1.running_mean\n",
      "center.decoder.1.running_var\n",
      "center.decoder.1.num_batches_tracked\n",
      "center.decoder.4.weight\n",
      "center.decoder.4.bias\n",
      "center.decoder.5.weight\n",
      "center.decoder.5.bias\n",
      "center.decoder.5.running_mean\n",
      "center.decoder.5.running_var\n",
      "center.decoder.5.num_batches_tracked\n",
      "center.decoder.7.weight\n",
      "center.decoder.7.bias\n",
      "decoder.0.decoder.0.weight\n",
      "decoder.0.decoder.0.bias\n",
      "decoder.0.decoder.1.weight\n",
      "decoder.0.decoder.1.bias\n",
      "decoder.0.decoder.1.running_mean\n",
      "decoder.0.decoder.1.running_var\n",
      "decoder.0.decoder.1.num_batches_tracked\n",
      "decoder.0.decoder.4.weight\n",
      "decoder.0.decoder.4.bias\n",
      "decoder.0.decoder.5.weight\n",
      "decoder.0.decoder.5.bias\n",
      "decoder.0.decoder.5.running_mean\n",
      "decoder.0.decoder.5.running_var\n",
      "decoder.0.decoder.5.num_batches_tracked\n",
      "decoder.0.decoder.7.weight\n",
      "decoder.0.decoder.7.bias\n",
      "decoder.1.upsampler.0.weight\n",
      "decoder.1.upsampler.0.bias\n",
      "decoder.1.upsampler.1.weight\n",
      "decoder.1.upsampler.1.bias\n",
      "decoder.1.upsampler.1.running_mean\n",
      "decoder.1.upsampler.1.running_var\n",
      "decoder.1.upsampler.1.num_batches_tracked\n",
      "decoder.1.upsampler.3.weight\n",
      "decoder.1.upsampler.3.bias\n",
      "decoder.1.upsampler.4.weight\n",
      "decoder.1.upsampler.4.bias\n",
      "decoder.1.upsampler.4.running_mean\n",
      "decoder.1.upsampler.4.running_var\n",
      "decoder.1.upsampler.4.num_batches_tracked\n",
      "decoder.1.upsampler.6.weight\n",
      "decoder.1.upsampler.6.bias\n",
      "decoder.2.upsampler.0.weight\n",
      "decoder.2.upsampler.0.bias\n",
      "decoder.2.upsampler.1.weight\n",
      "decoder.2.upsampler.1.bias\n",
      "decoder.2.upsampler.1.running_mean\n",
      "decoder.2.upsampler.1.running_var\n",
      "decoder.2.upsampler.1.num_batches_tracked\n",
      "decoder.2.upsampler.3.weight\n",
      "decoder.2.upsampler.3.bias\n",
      "decoder.2.upsampler.4.weight\n",
      "decoder.2.upsampler.4.bias\n",
      "decoder.2.upsampler.4.running_mean\n",
      "decoder.2.upsampler.4.running_var\n",
      "decoder.2.upsampler.4.num_batches_tracked\n",
      "decoder.2.upsampler.6.weight\n",
      "decoder.2.upsampler.6.bias\n",
      "decoder.3.upsampler.0.weight\n",
      "decoder.3.upsampler.0.bias\n",
      "decoder.3.upsampler.1.weight\n",
      "decoder.3.upsampler.1.bias\n",
      "decoder.3.upsampler.1.running_mean\n",
      "decoder.3.upsampler.1.running_var\n",
      "decoder.3.upsampler.1.num_batches_tracked\n",
      "decoder.3.upsampler.3.weight\n",
      "decoder.3.upsampler.3.bias\n",
      "decoder.3.upsampler.4.weight\n",
      "decoder.3.upsampler.4.bias\n",
      "decoder.3.upsampler.4.running_mean\n",
      "decoder.3.upsampler.4.running_var\n",
      "decoder.3.upsampler.4.num_batches_tracked\n",
      "decoder.3.upsampler.6.weight\n",
      "decoder.3.upsampler.6.bias\n",
      "decoder.5.weight\n",
      "decoder.5.bias\n",
      "diffuser.timestep_embed.0.weight\n",
      "diffuser.timestep_embed.0.bias\n",
      "diffuser.timestep_embed.2.weight\n",
      "diffuser.timestep_embed.2.bias\n",
      "diffuser.diffusion_step.0.weight\n",
      "diffuser.diffusion_step.0.bias\n",
      "diffuser.diffusion_step.2.weight\n",
      "diffuser.diffusion_step.2.bias\n",
      "_________\n",
      "Load center.decoder.0.weight in to ResNet UNet Diffusion\n",
      "Load center.decoder.0.bias in to ResNet UNet Diffusion\n",
      "Load center.decoder.1.weight in to ResNet UNet Diffusion\n",
      "Load center.decoder.1.bias in to ResNet UNet Diffusion\n",
      "Load center.decoder.1.running_mean in to ResNet UNet Diffusion\n",
      "Load center.decoder.1.running_var in to ResNet UNet Diffusion\n",
      "Load center.decoder.1.num_batches_tracked in to ResNet UNet Diffusion\n",
      "Load center.decoder.4.weight in to ResNet UNet Diffusion\n",
      "Load center.decoder.4.bias in to ResNet UNet Diffusion\n",
      "Load center.decoder.5.weight in to ResNet UNet Diffusion\n",
      "Load center.decoder.5.bias in to ResNet UNet Diffusion\n",
      "Load center.decoder.5.running_mean in to ResNet UNet Diffusion\n",
      "Load center.decoder.5.running_var in to ResNet UNet Diffusion\n",
      "Load center.decoder.5.num_batches_tracked in to ResNet UNet Diffusion\n",
      "Load center.decoder.7.weight in to ResNet UNet Diffusion\n",
      "Load center.decoder.7.bias in to ResNet UNet Diffusion\n",
      "Load decoder.0.decoder.0.weight in to ResNet UNet Diffusion\n",
      "Load decoder.0.decoder.0.bias in to ResNet UNet Diffusion\n",
      "Load decoder.0.decoder.1.weight in to ResNet UNet Diffusion\n",
      "Load decoder.0.decoder.1.bias in to ResNet UNet Diffusion\n",
      "Load decoder.0.decoder.1.running_mean in to ResNet UNet Diffusion\n",
      "Load decoder.0.decoder.1.running_var in to ResNet UNet Diffusion\n",
      "Load decoder.0.decoder.1.num_batches_tracked in to ResNet UNet Diffusion\n",
      "Load decoder.0.decoder.4.weight in to ResNet UNet Diffusion\n",
      "Load decoder.0.decoder.4.bias in to ResNet UNet Diffusion\n",
      "Load decoder.0.decoder.5.weight in to ResNet UNet Diffusion\n",
      "Load decoder.0.decoder.5.bias in to ResNet UNet Diffusion\n",
      "Load decoder.0.decoder.5.running_mean in to ResNet UNet Diffusion\n",
      "Load decoder.0.decoder.5.running_var in to ResNet UNet Diffusion\n",
      "Load decoder.0.decoder.5.num_batches_tracked in to ResNet UNet Diffusion\n",
      "Load decoder.0.decoder.7.weight in to ResNet UNet Diffusion\n",
      "Load decoder.0.decoder.7.bias in to ResNet UNet Diffusion\n",
      "Load decoder.1.upsampler.0.weight in to ResNet UNet Diffusion\n",
      "Load decoder.1.upsampler.0.bias in to ResNet UNet Diffusion\n",
      "Load decoder.1.upsampler.1.weight in to ResNet UNet Diffusion\n",
      "Load decoder.1.upsampler.1.bias in to ResNet UNet Diffusion\n",
      "Load decoder.1.upsampler.1.running_mean in to ResNet UNet Diffusion\n",
      "Load decoder.1.upsampler.1.running_var in to ResNet UNet Diffusion\n",
      "Load decoder.1.upsampler.1.num_batches_tracked in to ResNet UNet Diffusion\n",
      "Load decoder.1.upsampler.3.weight in to ResNet UNet Diffusion\n",
      "Load decoder.1.upsampler.3.bias in to ResNet UNet Diffusion\n",
      "Load decoder.1.upsampler.4.weight in to ResNet UNet Diffusion\n",
      "Load decoder.1.upsampler.4.bias in to ResNet UNet Diffusion\n",
      "Load decoder.1.upsampler.4.running_mean in to ResNet UNet Diffusion\n",
      "Load decoder.1.upsampler.4.running_var in to ResNet UNet Diffusion\n",
      "Load decoder.1.upsampler.4.num_batches_tracked in to ResNet UNet Diffusion\n",
      "Load decoder.1.upsampler.6.weight in to ResNet UNet Diffusion\n",
      "Load decoder.1.upsampler.6.bias in to ResNet UNet Diffusion\n",
      "Load decoder.2.upsampler.0.weight in to ResNet UNet Diffusion\n",
      "Load decoder.2.upsampler.0.bias in to ResNet UNet Diffusion\n",
      "Load decoder.2.upsampler.1.weight in to ResNet UNet Diffusion\n",
      "Load decoder.2.upsampler.1.bias in to ResNet UNet Diffusion\n",
      "Load decoder.2.upsampler.1.running_mean in to ResNet UNet Diffusion\n",
      "Load decoder.2.upsampler.1.running_var in to ResNet UNet Diffusion\n",
      "Load decoder.2.upsampler.1.num_batches_tracked in to ResNet UNet Diffusion\n",
      "Load decoder.2.upsampler.3.weight in to ResNet UNet Diffusion\n",
      "Load decoder.2.upsampler.3.bias in to ResNet UNet Diffusion\n",
      "Load decoder.2.upsampler.4.weight in to ResNet UNet Diffusion\n",
      "Load decoder.2.upsampler.4.bias in to ResNet UNet Diffusion\n",
      "Load decoder.2.upsampler.4.running_mean in to ResNet UNet Diffusion\n",
      "Load decoder.2.upsampler.4.running_var in to ResNet UNet Diffusion\n",
      "Load decoder.2.upsampler.4.num_batches_tracked in to ResNet UNet Diffusion\n",
      "Load decoder.2.upsampler.6.weight in to ResNet UNet Diffusion\n",
      "Load decoder.2.upsampler.6.bias in to ResNet UNet Diffusion\n",
      "Load decoder.3.upsampler.0.weight in to ResNet UNet Diffusion\n",
      "Load decoder.3.upsampler.0.bias in to ResNet UNet Diffusion\n",
      "Load decoder.3.upsampler.1.weight in to ResNet UNet Diffusion\n",
      "Load decoder.3.upsampler.1.bias in to ResNet UNet Diffusion\n",
      "Load decoder.3.upsampler.1.running_mean in to ResNet UNet Diffusion\n",
      "Load decoder.3.upsampler.1.running_var in to ResNet UNet Diffusion\n",
      "Load decoder.3.upsampler.1.num_batches_tracked in to ResNet UNet Diffusion\n",
      "Load decoder.3.upsampler.3.weight in to ResNet UNet Diffusion\n",
      "Load decoder.3.upsampler.3.bias in to ResNet UNet Diffusion\n",
      "Load decoder.3.upsampler.4.weight in to ResNet UNet Diffusion\n",
      "Load decoder.3.upsampler.4.bias in to ResNet UNet Diffusion\n",
      "Load decoder.3.upsampler.4.running_mean in to ResNet UNet Diffusion\n",
      "Load decoder.3.upsampler.4.running_var in to ResNet UNet Diffusion\n",
      "Load decoder.3.upsampler.4.num_batches_tracked in to ResNet UNet Diffusion\n",
      "Load decoder.3.upsampler.6.weight in to ResNet UNet Diffusion\n",
      "Load decoder.3.upsampler.6.bias in to ResNet UNet Diffusion\n",
      "Load decoder.5.weight in to ResNet UNet Diffusion\n",
      "Load decoder.5.bias in to ResNet UNet Diffusion\n",
      "-0.7073746919631958 and 0.6512\n",
      "0.015003346838057041 and 0.015\n"
     ]
    }
   ],
   "source": [
    "from utils import JupyterArgParser\n",
    "from pathlib import Path\n",
    "\n",
    "# ========= global settings =========\n",
    "# Taken from i2sb paper with minor changes\n",
    "\n",
    "RESULT_DIR = Path(\"results\")\n",
    "\n",
    "# --------------- basic ---------------\n",
    "parser = JupyterArgParser()\n",
    "parser.add_argument(\"--seed\",           type=int,   default=0)\n",
    "parser.add_argument(\"--name\",           type=str,   default=None,        help=\"experiment ID\")\n",
    "parser.add_argument(\"--ckpt\",           type=str,   default=None,        help=\"resumed checkpoint name\")\n",
    "parser.add_argument(\"--device\",         type=str,   default=DEVICE,      help=\"type of device to use for training\")\n",
    "parser.add_argument(\"--gpu\",            type=int,   default=None,        help=\"set only if you wish to run on a particular GPU\")\n",
    "\n",
    "# --------------- model ---------------\n",
    "parser.add_argument(\"--image-size\",     type=int,   default=224)\n",
    "parser.add_argument(\"--t0\",             type=float, default=1e-4,        help=\"sigma start time in network parametrization\")\n",
    "parser.add_argument(\"--T\",              type=float, default=1.,          help=\"sigma end time in network parametrization\")\n",
    "parser.add_argument(\"--interval\",       type=int,   default=INTERVAL,        help=\"number of interval\")\n",
    "parser.add_argument(\"--beta-max\",       type=float, default=0.3,         help=\"max diffusion for the diffusion model\")\n",
    "parser.add_argument(\"--beta-schedule\",  type=str,   default=\"i2sb\",    help=\"schedule for beta\")\n",
    "parser.add_argument(\"--ot-ode\",         action=\"store_true\",             help=\"use OT-ODE model\")\n",
    "parser.add_argument(\"--clip-denoise\",   action=\"store_true\",             help=\"clamp predicted image to [-1,1] at each\")\n",
    "parser.add_argument(\"--use-fp16\",       action=\"store_true\",             help=\"use fp16 for training\")\n",
    "parser.add_argument(\"diffusion-type\",   type=str,   default=\"schrodinger_bridge\",      help=\"type of diffusion model\")\n",
    "\n",
    "# --------------- optimizer and loss ---------------\n",
    "parser.add_argument(\"--batch-size\",     type=int,   default=256)\n",
    "parser.add_argument(\"--microbatch\",     type=int,   default=4,           help=\"accumulate gradient over microbatch until full batch-size\")\n",
    "parser.add_argument(\"--num-itr\",        type=int,   default=10001,     help=\"training iteration\")\n",
    "parser.add_argument(\"--lr\",             type=float, default=5e-5,        help=\"learning rate\")\n",
    "parser.add_argument(\"--lr-gamma\",       type=float, default=0.99,        help=\"learning rate decay ratio\")\n",
    "parser.add_argument(\"--lr-step\",        type=int,   default=1000,        help=\"learning rate decay step size\")\n",
    "parser.add_argument(\"--l2-norm\",        type=float, default=0.0)\n",
    "parser.add_argument(\"--ema\",            type=float, default=0.99)\n",
    "\n",
    "# --------------- path and logging ---------------\n",
    "parser.add_argument(\"--dataset-dir\",    type=Path,  default=\"/dataset\",  help=\"path to LMDB dataset\")\n",
    "parser.add_argument(\"--log-dir\",        type=Path,  default=\".log\",      help=\"path to log std outputs and writer data\")\n",
    "parser.add_argument(\"--log-writer\",     type=str,   default=None,        help=\"log writer: can be tensorbard, wandb, or None\")\n",
    "parser.add_argument(\"--wandb-api-key\",  type=str,   default=None,        help=\"unique API key of your W&B account; see https://wandb.ai/authorize\")\n",
    "parser.add_argument(\"--wandb-user\",     type=str,   default=None,        help=\"user name of your W&B account\")\n",
    "parser.add_argument(\"--ckpt-path\",      type=Path,  default=None,        help=\"path to save checkpoints\")\n",
    "parser.add_argument(\"--load\",           type=Path,  default=RUNNER_CKPT_PATH,        help=\"path to load checkpoints\")\n",
    "parser.add_argument(\"--unet_path\",      type=str,   default=None,        help=\"path of UNet model to load for training\")\n",
    "\n",
    "# --------------- distributed ---------------\n",
    "parser.add_argument(\"--local-rank\",     type=int,   default=0)\n",
    "parser.add_argument(\"--global-rank\",    type=int,   default=0)\n",
    "parser.add_argument(\"--global-size\",    type=int,   default=1)\n",
    "\n",
    "opt = parser.get_options()\n",
    "# ========= path handle =========\n",
    "opt.name = \"test\"\n",
    "os.makedirs(opt.log_dir, exist_ok=True)\n",
    "opt.ckpt_path = RESULT_DIR / opt.name if opt.name else RESULT_DIR / \"temp\"\n",
    "os.makedirs(opt.ckpt_path, exist_ok=True)\n",
    "\n",
    "# ========= auto assert =========\n",
    "assert opt.batch_size % opt.microbatch == 0, f\"{opt.batch_size=} is not dividable by {opt.microbatch}!\"\n",
    "\n",
    "\n",
    "run = Runner(opt)\n",
    "# run automatically has ResNet UNet Diffusion weights loaded from runner_ckpt_path\n",
    "# we want to override run.net's center and decoder layers\n",
    "base_dict = run.net.state_dict()\n",
    "for k, v in base_dict.items():\n",
    "    print(k)\n",
    "print(\"_________\")\n",
    "# print(run.net)\n",
    "\n",
    "# load in center weights from pretrained\n",
    "# for layer in CENTER_LAYERS:\n",
    "#     base_dict[layer] = center[layer]\n",
    "#     print(f\"Load {layer} in to ResNet UNet Diffusion\")\n",
    "\n",
    "# load in decoder weights. load in center weights from decoder tuned\n",
    "for k, v in TUNED_DECODER_CKPT.items():\n",
    "    if k.startswith('classification_head'):\n",
    "      suffix = k.split('classification_head.', 1)[1]\n",
    "      new_key = 'decoder.' + suffix # RN_NOSKIP calls it classification_head.<> but RNUNDiff calls it decoder.<>\n",
    "      base_dict[new_key] = v\n",
    "      print(f\"Load {new_key} in to ResNet UNet Diffusion\")\n",
    "    if k.startswith('center'):\n",
    "      base_dict[k] = v\n",
    "      print(f\"Load {k} in to ResNet UNet Diffusion\")\n",
    "\n",
    "run.net.load_state_dict(base_dict) # load in updated dict to model\n",
    "\n",
    "# sanity check\n",
    "loaded_dict = run.net.state_dict()\n",
    "\n",
    "# sanity check that the first weight value matches for CENTER\n",
    "# assert math.isclose(loaded_dict['center.decoder.0.weight'].flatten()[0].item(), PRETRAINED_CENTER_DECODER_0_WEIGHT_FIRST_VALUE, abs_tol=1e-4), f\"first weight from center.decoder.0.weight={loaded_dict['center.decoder.0.weight'].flatten()[0].item()} does not equal PRETRAINED_CENTER_DECODER_0_WEIGHT_FIRST_VALUE={PRETRAINED_CENTER_DECODER_0_WEIGHT_FIRST_VALUE}\"\n",
    "print(loaded_dict['center.decoder.0.weight'].flatten()[0].item() , 'and', PRETRAINED_CENTER_DECODER_0_WEIGHT_FIRST_VALUE)\n",
    "\n",
    "# sanity check that the first weight value matches for ALL_MOCO\n",
    "assert math.isclose(loaded_dict['encoder.0.weight'].flatten()[0].item(), PRETRAINED_ALL_MOCO_WEIGHT_FIRST_VALUE, abs_tol=1e-4), f\"first weight from center.decoder.0.weight={loaded_dict['encoder.0.weight'].flatten()[0].item()} does not equal PRETRAINED_ALL_MOCO_WEIGHT_FIRST_VALUE={PRETRAINED_ALL_MOCO_WEIGHT_FIRST_VALUE}\"\n",
    "print(loaded_dict['encoder.0.weight'].flatten()[0].item() , 'and', PRETRAINED_ALL_MOCO_WEIGHT_FIRST_VALUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d95c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jamaica_satellite shape: (584, 13, 224, 224) | jamaica_label shape: (584, 1, 224, 224)\n",
      "jamaica_loader of length 18\n"
     ]
    }
   ],
   "source": [
    "# load satellite images:labels\n",
    "jamaica_satellite = np.load(OM_SATELLITE_PATH, 'r')\n",
    "zero_channel = np.zeros((jamaica_satellite.shape[0], 1, jamaica_satellite.shape[2], jamaica_satellite.shape[3]))\n",
    "jamaica_satellite = np.concatenate((jamaica_satellite[:,:10], zero_channel, jamaica_satellite[:,10:]), axis=1)\n",
    "jamaica_label = np.load(OM_LABEL_PATH, 'r')\n",
    "assert len(jamaica_satellite) == len(jamaica_label), f\"jamaica_satellite b={jamaica_satellite.shape[0]} and jamaica_label b={jamaica_label.shape[0]} don't have the same B\"\n",
    "print(f\"jamaica_satellite shape: {jamaica_satellite.shape} | jamaica_label shape: {jamaica_label.shape}\")\n",
    "\n",
    "jamaica_dataset = MemmapDataset(images=jamaica_satellite, labels=jamaica_label, rgb_means=RGB_MEANS, rgb_sds=RGB_SDS, satellite_means=SATELLITE_MEANS, satellite_sds=SATELLITE_SDS)\n",
    "jamaica_loader = DataLoader(jamaica_dataset, batch_size=32, shuffle=False, drop_last=True)\n",
    "print(f\"jamaica_loader of length {len(jamaica_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a59cd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 18/18\r"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "total_TP = 0\n",
    "total_FP = 0\n",
    "total_FN = 0\n",
    "total_TN = 0\n",
    "\n",
    "run.net.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (sat, label) in enumerate(jamaica_loader):\n",
    "        print(f\"Batch {batch_idx + 1}/{len(jamaica_loader)}\", end=\"\\r\")\n",
    "        sat = sat.to(DEVICE)\n",
    "        label = label.float().to(DEVICE)\n",
    "\n",
    "        # encode\n",
    "        if len(sat.shape) == 3:\n",
    "            sat = sat.unsqueeze(0)\n",
    "        sat_x1 = run.net.encoder(sat)\n",
    "        \n",
    "        # diffuse\n",
    "        sat_xs, sat_pred_x0s = run.ddpm_sampling(opt, sat_x1, clip_denoise=opt.clip_denoise, verbose=False)\n",
    "        sat_x0_hat = sat_pred_x0s[:, -1].to(DEVICE)\n",
    "\n",
    "        # decode\n",
    "        label_hat = run.net.decoder(run.net.center(sat_x0_hat))\n",
    "\n",
    "        # plot sat_img, gt label, label_hat\n",
    "        # if batch_idx == 0:\n",
    "        #     idx = 30\n",
    "        #     plot_img_gt_pred(sat[idx], label[idx], label_hat[idx], rgb_mean_tensor=RGB_MEANS_TENSOR, rgb_std_tensor=RGB_SDS_TENSOR, satellite_mean_tensor=SATELLITE_MEANS_TENSOR, satellite_std_tensor=SATELLITE_SDS_TENSOR)\n",
    "        #     break\n",
    "        \n",
    "        loss = LOSS(label_hat, label)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        label_hat = torch.sigmoid(label_hat).view(-1)\n",
    "        label = label.view(-1)\n",
    "\n",
    "        TP = (label_hat * label).sum().item()\n",
    "        FP = ((1 - label) * label_hat).sum().item()\n",
    "        FN = (label * (1 - label_hat)).sum().item()\n",
    "        TN = ((1 - label) * (1 - label_hat)).sum().item()\n",
    "\n",
    "        total_TP += TP\n",
    "        total_FP += FP\n",
    "        total_FN += FN\n",
    "        total_TN += TN\n",
    "\n",
    "avg_loss = total_loss / len(jamaica_loader)\n",
    "precision = total_TP / (total_TP + total_FP) if (total_TP + total_FP) > 0 else 0\n",
    "recall = total_TP / (total_TP + total_FN) if (total_TP + total_FN) > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "iou = total_TP / (total_TP + total_FP + total_FN) if (total_TP + total_FP + total_FN) > 0 else 0\n",
    "accuracy = (total_TP + total_TN) / (total_TP + total_FP + total_FN + total_TN) if (total_TP + total_FP + total_FN + total_TN) > 0 else 0\n",
    "specificity = total_TN / (total_TN + total_FP) if (total_TN + total_FP) > 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aa0cf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Loss': 0.9443539016776614, 'Precision': 0.16594861151401172, 'Recall': 0.07228525565055922, 'f1_score': 0.10070472305987575, 'IOU': 0.05302215210165581, 'Accuracy': 0.4707415907716637, 'Specificity': 0.7475828107968292}\n"
     ]
    }
   ],
   "source": [
    "metrics = {\n",
    "        'Loss': avg_loss,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'f1_score': f1_score,\n",
    "        'IOU': iou,\n",
    "        'Accuracy': accuracy,\n",
    "        'Specificity': specificity\n",
    "    }\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0e825fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856453.5634765625\n",
      "4304502.92578125\n",
      "10991793.4375\n",
      "12748626.21875\n"
     ]
    }
   ],
   "source": [
    "print(total_TP)\n",
    "print(total_FP)\n",
    "print(total_FN)\n",
    "print(total_TN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
