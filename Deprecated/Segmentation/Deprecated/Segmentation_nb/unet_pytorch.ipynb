{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load \"../ml-mangrove/Segmentation/unet.py\"\n",
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "#import keras\n",
    "#import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import rasterio\n",
    "import subprocess\n",
    "#import tensorflow_datasets as tfds\n",
    "from PIL import Image\n",
    "#from segmentation_models.utils import set_trainable\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing notebooks\n",
    "#from ipynb.fs.full.<notebook_name> import *\n",
    "from ipynb.fs.full.create_seg_dataset import create_seg_dataset\n",
    "from ipynb.fs.full.gen_seg_labels import gen_seg_labels, tif_to_jpg, tile_raster\n",
    "from ipynb.fs.full.raster_mask import raster_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resources: https://yann-leguilly.gitlab.io/post/2019-12-14-tensorflow-tfdata-segmentation/\n",
    "\n",
    "'''\n",
    "Documentation/Usage: This script is meant to be called with command line arguments.\n",
    "--width (required): tile width\n",
    "--input_rasters (required): space separated list of rasters (orthomosaics)\n",
    "--input_vectors (required for training): space separated list of shapefiles (ordering should correspond with rasters)\n",
    "--train: Flag. Add if training.\n",
    "--test: Flag. Add if testing.\n",
    "--weights (required): path to weights file, either to write to for training, or to use for testing (.h5)\n",
    "--backbone (required): name of backbone to use, ex: resnet34, vgg16\n",
    "\n",
    "For training it should be sufficient to just call the script using the list of rasters and vectors (and other required arguments), \n",
    "and currently you have to manually set the hyperparams in the code, but this should eventually be offloaded to a settings file or \n",
    "command line arguments. This will result in the training weights being saved in the specified .h5 file.\n",
    "\n",
    "For testing you just need to call the script on the list of rasters and it will produce a mask of the entire\n",
    "orthomosaic.\n",
    "'''\n",
    "#keras.backend.set_image_data_format('channels_first')\n",
    "sm.set_framework('tf.keras')    # need this otherwise currently a bug in model.fit when used with tf.Datasets\n",
    "\n",
    "# Globals\n",
    "N_CHANNELS = 3\n",
    "WIDTH = 256\n",
    "HEIGHT = 256\n",
    "LOSS_FUNC = sm.losses.DiceLoss()\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_image(img_path: str) -> dict:\n",
    "    \"\"\"Load an image and its annotation (mask) and returning\n",
    "    a dictionary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_path : str\n",
    "        Image (not the mask) location.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary mapping an image and its annotation.\n",
    "    \"\"\"\n",
    "    image = tf.io.read_file(img_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.convert_image_dtype(image, tf.uint8)\n",
    "\n",
    "    # Creating mask path from image path\n",
    "    mask_path = tf.strings.regex_replace(img_path, \"images\", \"annotations\")\n",
    "    mask_path = tf.strings.regex_replace(mask_path, \"image\", \"annotation\")\n",
    "    mask = tf.io.read_file(mask_path)\n",
    "\n",
    "    # The masks contain a class index for each pixels\n",
    "    mask = tf.image.decode_jpeg(mask, channels=1)\n",
    "    mask = tf.image.convert_image_dtype(mask, tf.uint8)\n",
    "    \n",
    "    #mask = tf.where(mask == 255, np.dtype('uint8').type(0), mask)\n",
    "    # Note that we have to convert the new value (0)\n",
    "    # With the same dtype than the tensor itself\n",
    "    \n",
    "\n",
    "    return {'image': image, 'segmentation_mask': mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def normalize(input_image: tf.Tensor, input_mask: tf.Tensor) -> tuple:\n",
    "    \"\"\"Rescale the pixel values of the images/masks between 0.0 and 1.0\n",
    "    compared to [0,255] originally.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_image : tf.Tensor\n",
    "        Tensorflow tensor containing an image of size [SIZE,SIZE,3].\n",
    "    input_mask : tf.Tensor\n",
    "        Tensorflow tensor containing an annotation of size [SIZE,SIZE,1].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Normalized image and its annotation.\n",
    "    \"\"\"\n",
    "    input_mask = tf.cast(input_mask, tf.float32) / 255.0 # attempting to fix metrics\n",
    "    #input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "    return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def load_image_train(datapoint: dict) -> tuple:\n",
    "    \"\"\"Apply some transformations to an input dictionary\n",
    "    containing a train image and its annotation.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    An annotation is a regular  channel image.\n",
    "    If a transformation such as rotation is applied to the image,\n",
    "    the same transformation has to be applied on the annotation also.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datapoint : dict\n",
    "        A dict containing an image and its annotation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A modified image and its annotation.\n",
    "    \"\"\"\n",
    "   \n",
    "    input_image = tf.image.resize(datapoint['image'], (HEIGHT, WIDTH))\n",
    "    input_mask = tf.image.resize(datapoint['segmentation_mask'], (HEIGHT, WIDTH))\n",
    "    #input_mask = tf.image.rgb_to_grayscale(datapoint['segmentation_mask'])\n",
    "    \n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        input_mask = tf.image.flip_left_right(input_mask)\n",
    "    \n",
    "    #input_mask = tf.reshape(input_mask, (HEIGHT, WIDTH))  # removing single channel\n",
    "\n",
    "    input_image, input_mask = normalize(input_image, input_mask)\n",
    "    \n",
    "    return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def load_image_val(datapoint: dict) -> tuple:\n",
    "    \"\"\"Normalize and resize a test image and its annotation.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Since this is for the val set, we don't need to apply\n",
    "    any data augmentation technique.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datapoint : dict\n",
    "        A dict containing an image and its annotation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A modified image and its annotation.\n",
    "    \"\"\"\n",
    "    input_image = tf.image.resize(datapoint['image'], (HEIGHT, WIDTH))\n",
    "    input_mask = tf.image.resize(datapoint['segmentation_mask'], (HEIGHT, WIDTH))\n",
    "    \n",
    "    input_image, input_mask = normalize(input_image, input_mask)\n",
    "    #input_mask = tf.reshape(input_mask, (HEIGHT, WIDTH)) # removing single channel\n",
    "\n",
    "    \n",
    "    return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def load_image(datapoint: dict) -> tuple:\n",
    "    \"\"\"Loads and image and resizes it\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datapoint : dict\n",
    "        A dict containing an image and its annotation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A image and its annotation.\n",
    "    \"\"\"\n",
    "    input_image = tf.image.resize(datapoint['image'], (HEIGHT, WIDTH))\n",
    "    input_mask = tf.image.resize(datapoint['segmentation_mask'], (HEIGHT, WIDTH))\n",
    "    #input_mask = tf.image.resize(datapoint['label'], (HEIGHT, WIDTH))\n",
    "\n",
    "    return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\n",
    "    pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\n",
    "    #return pred_mask[0]\n",
    "    return pred_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(model=None, dataset=None, num=1):\n",
    "    if dataset:\n",
    "        for image, mask in dataset.take(num):\n",
    "            pred_mask = model.predict(image)\n",
    "            display([image[0], mask[0], create_mask(pred_mask[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(backbone, weight_file):\n",
    "    # For tensorboard\n",
    "    #logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    #tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir, update_freq='epoch')\n",
    "\n",
    "\n",
    "    # For more information about autotune:\n",
    "    # https://www.tensorflow.org/guide/data_performance#prefetching\n",
    "    #AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    #print(f\"Tensorflow ver. {tf.__version__}\")\n",
    "\n",
    "    # For reproducibility\n",
    "    SEED = 42\n",
    "\n",
    "    # Data\n",
    "    training_data = \"../dataset/training/\"\n",
    "    #val_data = \"../dataset/validation/\"\n",
    "\n",
    "    # Listing GPU info\n",
    "    #gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    #if gpus:\n",
    "    #    try:\n",
    "     #       for gpu in gpus:\n",
    "      #          tf.config.experimental.set_memory_growth(gpu, True)\n",
    "       #     logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        #print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        #except RuntimeError as e:\n",
    "         #   print(e)\n",
    "\n",
    "    # Hyperparams\n",
    "    BATCH_SIZE = 16\n",
    "    BUFFER_SIZE = 1000 # See https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle\n",
    "\n",
    "    # Creating and splitting dataset\n",
    "    DATASET_SIZE = len(glob(training_data + \"images/*.jpg\"))\n",
    "    print(f\"The Training Dataset contains {DATASET_SIZE} images.\")\n",
    "\n",
    "    TRAIN_SIZE = int(0.8 * DATASET_SIZE)\n",
    "    VAL_SIZE = int(0.2 * DATASET_SIZE)\n",
    "\n",
    "    \n",
    "    \n",
    "    full_dataset = tf.data.Dataset.list_files(training_data + \"images/*.jpg\", seed=SEED)\n",
    "    full_dataset = full_dataset.shuffle(buffer_size=BUFFER_SIZE, seed=SEED)\n",
    "    train_dataset = full_dataset.take(TRAIN_SIZE)\n",
    "    val_dataset = full_dataset.skip(TRAIN_SIZE)\n",
    "    \n",
    "    # Creating dict pairs linking images and annotations\n",
    "    train_dataset = train_dataset.map(parse_image)\n",
    "    val_dataset = val_dataset.map(parse_image)\n",
    "\n",
    "    # -- Train Dataset --# - https://stackoverflow.com/questions/49915925/output-differences-when-changing-order-of-batch-shuffle-and-repeat\n",
    "    train_dataset = train_dataset.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=BUFFER_SIZE, seed=SEED)\n",
    "    train_dataset = train_dataset.repeat()\n",
    "    train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "    train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    #-- Validation Dataset --#\n",
    "    #for image, label in tfds.as_numpy(val_dataset):\n",
    "     # print(type(image), type(label), label)\n",
    "    \n",
    "    val_dataset = val_dataset.map(load_image_val, num_parallel_calls=AUTOTUNE)\n",
    "    val_dataset = val_dataset.repeat()\n",
    "    val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "    val_dataset = val_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    \n",
    "    # define model\n",
    "    model = sm.Unet(\n",
    "        #'resnet34',\n",
    "        #'vgg16', \n",
    "        backbone,\n",
    "        input_shape=(HEIGHT, WIDTH, N_CHANNELS), \n",
    "        encoder_weights='imagenet', \n",
    "        encoder_freeze=True,    # only training decoder network\n",
    "        classes=NUM_CLASSES,\n",
    "        activation='sigmoid'\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        'Adam', \n",
    "        loss=LOSS_FUNC, \n",
    "        metrics=[sm.metrics.iou_score] #was giving score over 100 in later epochs before normalizing masks\n",
    "        #[tf.keras.metrics.MeanIoU(num_classes=2)]]\n",
    "    )\n",
    "    # TODO research step sizes\n",
    "    history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=50,\n",
    "    steps_per_epoch=TRAIN_SIZE / BATCH_SIZE,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps= 0.2 * (VAL_SIZE / BATCH_SIZE),\n",
    "    callbacks=[tensorboard_callback]\n",
    "    )\n",
    "\n",
    "    # Saving model\n",
    "    #model.save_weights(\"unet_500_weights_vgg16.h5\")\n",
    "    model.save_weights(weight_file)\n",
    "\n",
    "    # For reinstantiation\n",
    "    #model = keras.models.load_model(your_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to display slice of batch in datasets, as images\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def showDataImgs(train_dataset, val_dataset, n):\n",
    "    train_dataset_np = tfds.as_numpy(train_dataset)\n",
    "    val_dataset_np = tfds.as_numpy(val_dataset)\n",
    "    \n",
    "\n",
    "    example = next(val_dataset_np)\n",
    "    image = example[0]\n",
    "    label = example[1]\n",
    "    \n",
    "    print(\"val_label: \", label.shape)#label[11])\n",
    "    print(\"val_image: \", image.shape)\n",
    "    \n",
    "    label_slice = label[n].reshape(HEIGHT,WIDTH)\n",
    "    plt.imshow(label_slice, cmap='gray')\n",
    "    plt.show()\n",
    "    plt.imshow(image[n])\n",
    "    plt.show()\n",
    "\n",
    "    print(label[n])\n",
    "\n",
    "    \n",
    "    example = next(train_dataset_np)\n",
    "    image = example[0]\n",
    "    label = example[1]\n",
    "    \n",
    "    print(\"train_label: \",label.shape)#label[15])\n",
    "    print(\"train_image: \",image.shape)\n",
    "    \n",
    "    label_slice = label[n].reshape(HEIGHT,WIDTH)\n",
    "    plt.imshow(label_slice, cmap='gray')\n",
    "    plt.show()\n",
    "    plt.imshow(image[n])\n",
    "    plt.show()\n",
    "    \n",
    "    print(label[n])\n",
    "\n",
    "    #print(image[0])\n",
    "    # label_grey = np.mean(label[n], -1)\n",
    "    # label_grey.reshape((256,256, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Training Dataset contains 2985 images.\n",
      "val_label:  (16, 256, 256, 1)\n",
      "val_image:  (16, 256, 256, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAbeElEQVR4nO3dT2wk53nn8e9TVV39h83hv+HM6J/XdqwA61wUaeA14EWQRbCJrYucQxb2IRYWBiYHGUiA7EFJDutLgN3FJgEMZA0osBF5kbXXQGJYh+xuvEIAIwc7HhuORrLW8ThRpMmMRM7wb7ObXd3Vzx66q6Y5zZmhRDarOPx9AILNYpN8pkX+VFXv+z6vuTsiIuOCogsQkfJRMIjIBAWDiExQMIjIBAWDiExQMIjIhKkFg5l93Mx+bGZXzez5af0cETl6No15DGYWAn8P/FvgGvA94NPu/qMj/2EicuSmdcbwEeCqu/+DuyfA14BnpvSzROSIRVP6vo8Ab419fA34V3d7splp+qXI9N109+WDPHFawWD7HNvzx29ml4BLU/r5IjLpnw76xGkFwzXgsbGPHwWujz/B3V8AXgCdMYiUzbTuMXwPeNzMPmBmMfAp4KUp/SwROWJTOWNw976ZfQ74P0AIfNndX5vGzxKRozeV4cp3XYQuJUSOw/fd/eJBnqiZjyIyQcEgIhMUDCIyQcEgIhMUDCIyQcEgIhMUDCIyQcEgIhMUDCIyQcEgIhMUDCIyQcEgIhMUDCIyQcEgIhMUDCIyQcEgIhMUDCIyQcEgIhMUDCIyQcEgIhMUDCIyQcEgIhMUDCIyQcEgIhMUDCIyQcEgIhMUDCIyQcEgIhMUDCIyQcEgIhMUDCIyQcEgIhMUDCIyQcEgIhOiw3yxmb0BbAMp0Hf3i2a2CPxP4P3AG8C/c/f1w5UpIsfpKM4Y/o27P+HuF0cfPw+87O6PAy+PPhaRE2QalxLPAC+OHr8IfHIKP0NEpuiwweDAX5nZ983s0ujYeXe/ATB6f26/LzSzS2Z22cwuH7IGETlih7rHAHzM3a+b2TngW2b2/w76he7+AvACgJn5IesQkSN0qDMGd78+er8CfAP4CPCOmT0EMHq/ctgiReR4vedgMLMZM5vNHgO/DLwKvAQ8O3ras8A3D1ukiByvw1xKnAe+YWbZ9/kf7v6/zex7wNfN7LPAm8CvHb5METlO5l785b3uMYgci++PTSu4J818FJEJCgYRmaBgEJEJCgYRmaBgEJEJCgYRmaBgEJEJCgYRmaBgEJEJCgYRmaBgEJEJCgYRmaBgEJEJCgYRmaBgEJEJCgYRmaBgEJEJCgYRmaBgEJEJpQ+GINhbYq1W2/c4QLVaPZaaRB50pQ+GwWBAGIb5x0mSADA3N0elUmFmZoZKpYKZ0e12uXDhQlGlijwwSh8MAFE07HI/Pz/PqF096+vruDudToder8fc3BxxHPP2228XWarIA+FEBcPGxgZpmnLmzBlmZmbo9/uEYcjCwgIbGxskScLy8nLB1YqcfKUPhkqlws7OTn7/oFarsbW1RbPZxN3Z3NxkfX2dubk5ALa2toosV+SBcNhNbacuu0zodDoA7O7ucu7cOa5cuQIMg8DdMTOiKKLb7RZZrsgDofRnDDC8AZkkCfV6nTAMWVlZYXl5mU6nw/nz52m323Q6HdI0LbpUkQfCiQiGJEkwMzqdDu6Ou9Nut+n3+wCkaUqtViMIAoIgIAzDieHMMAzzoU4RubfSX0pkoxDuTr1e5/r16wA0Gg0Aut0us7OzeVBkzweI45g4jmm1WpgZu7u7x/8PEDmBSn/GEARBft9gZ2eHKIpI05RsM97s8iG7OfnWW28Bw0Dp9/u0Wi2A/OxCRO6v9MGQpinnzp3D3RkMBszMzBCGYT6hKTtz2NzcBODRRx8lSRLcnWq1SqPRYH5+vsh/gsiJU/pgCIKAlZUV0jQlDEN2d3dJkoTV1VWiKMoDYXFxMR++rFQq+eSndrvNxsYGMzMzBf9LRE6O0gfDYDAAbs9PqNfrVCoVlpeXCcOQZrNJq9Wi1+thZszNzdHr9eh0OvT7/XwkY2dnhzNnzhT5TxE5MUofDNVqla2tLRYWFmi328AwLLrdLqurq5gZzWaT3d3d/D5CpVIhDEPCMNwzjKnJTyIHc99gMLMvm9mKmb06dmzRzL5lZj8ZvV8YHTcz+4KZXTWzV8zsyYMWkg0lZtOfz549C5BfFvR6PRqNBpubm4RhSJqmLC8v58OSs7Oz+deO32jMzjjiON5zxpCdbYz/LBEZOsgZw58CH7/j2PPAy+7+OPDy6GOATwCPj94uAV88SBFPPfUUrVaLKIro9/vEcczGxgbuzvb2NkEQUKlUgOGqSnffd9l1Jooi4jjOhy/dnZs3b+ZnDGfOnCFN03zEYm1t7SBlipwa9w0Gd/82cOdfzjPAi6PHLwKfHDv+FR/6DjBvZg8dpBAzY2dnBxhOaMqGH5MkIQgC+v0+q6ur+XPHl2LfKU1T0jQliiK2trbo9XrUarX8zCALiIWFBeD2WYqIDL3Xewzn3f0GwOj9udHxR4C3xp53bXTsnrIzgH6/n182rKys0Ol0iOOYRqOBmbG8vEy/3ydN0/wMYj/Z98lUKhUqlQpXr14ljmMefvhh4PYQZ5Ik9wwakdPmqG8+2j7HfJ9jmNklM7tsZpdv3rzJYDDI5yREUUQURdTrdTY2NgDyP9zBYJDfN7ibarWa/9GPXzbMzc3x9ttv57Mnx+msQeS29xoM72SXCKP3K6Pj14DHxp73KDD5Vwi4+wvuftHdLy4tLREEAdvb23S7XTY3NwmCgCRJ8huGvV4vH5W419kCDM8AlpaWaLVaJEnCYDCg2Wyyvr7OwsJCfjYx3gpOqzJFbnuvwfAS8Ozo8bPAN8eOf2Y0OvFRYDO75LhnEUHAYDDI1zY0Gg2iKMrvL2xvb+eXA3EcA+Q3DveT3cTM5jhUKhW63S4LCwsMBgPSNM3nOjSbzXx0QkRGstWKd3sDvgrcAHoMzwg+CywxHI34yej94ui5Bvwx8FPgCnDxft/f3Xnqqaf8uPR6PXd3v3XrlidJ4kmSeBRF3mg0fH5+3oMgcMArlYo3m838McNLIr3p7SS/XT7I36O73391pbt/+i6f+qV9nuvAc/f7nkXKplHPzc0RhiGDwYB+v0+/388vVWZmZtjZ2aHX67G4uKjhTDl1Sj/z8ailaUq1Ws1vZq6uruYpeebMGRqNBjs7O8zOzgLDPpPZ5YvIaXHqgiFr2JKNdpw7dy7v03D16lU6nc7ECIWWbMtpc+qCIbtcmJ+fp91uY2bUarV8ivXa2hq1Wo3t7e38UuN+w6MiD5pTFwyNRoP19fV87UWapgwGg7zz08zMDG+++SZwe2KUyGlz6oIBhlOhsz/4rD9ktvZid3c3n+vQ6/VI0zTvQJ3J+ksCedt6kQeJpvvdYXZ2lm63S5qme3pI9vt9KpUKURTR6XSYn58nTdN8hqXIg+RUnjHcTXYTMo7jfFaku+et4QaDAZ1Oh4ceeoiNjQ22t7c1YiEPJAXDmOxyIdvcBuDatWv5fphZD8kbN26oh4M80BQMY7KdrLKGMDBsLlutVnF31tfXabfbRFHEzZs3qVar9Hq9gqsWOXoKhjGbm5tUq1XSNM0nOMHtdRlpmtLpdPKek91ud8/ybpEHhYJhzNzcXN5HcryHZLPZZGtrK28s2263WV1dJY5j9XGQB5KC4Q5ZH8nxHpJwe+erbLMbdydJkj37ZQZBkC8TH98RS+SkUTAcULYDVta0dn19Pb+MyDpVj0+U0iWGnGQKhgPKNrxZW1vLd8EC2N7eplar5f0qNzc3ieNYu1/JiaZgOKDBYEC73WZxcZFer8eFCxdwd5rNZt4qbmZmJu88lS3SEjmJFAwHlM16zBZhdTqdfELU/Pw83W6XnZ2dvNW97jHISaZgOKBWq7WnY3W9Xqder+ft6bONcXZ3d/N5DyInlYLhgLL+kWma5rtpd7tdzpw5ky/I6vf7BEGQN7MVOan02/suNJtNwjDMzxrGu0zDcOTCzAiCIB/GHN8cZ2lpKT8mUmYKhilydyqVSj6T8tatW/fdRUukDBQMU9Tv97lx4wa1Wi1vhZ+1thcpMwXDFO3u7rK4uLinZ0N2o1KkzBQMU9RsNvN9NLMNb8Iw1IpMKT0FwxR1u112d3eJ45gLFy5w69atPWsrRMpKwTBlzWaTXq/H9evXCYKAarWqUQkpPfV8nKIsAIIgwMwYDAbaPFdOBJ0xTFEcx/R6vXzX7fHjImWmYJiyNE1J03RPGIyHhEgZ6VJiipIkoVar5TtbuXu+R4XmMkiZ6YxhiuI4JkkSVldX9/SQ1MiElJ2CYYqypi3Ly8t7jmvlpZSdgmGK5ubm6Pf7DAYDtre3ASa2uxMpIwXDlEVRRBAEeYMX7Z4tJ4GCYcrW1tbo9/ucP3+eW7du0e12FQxSevcNBjP7spmtmNmrY8c+b2b/bGY/HL09Pfa53zGzq2b2YzP7lWkVfhIMBgPm5ubynauWlpaoVCrMzMwUXZrIPR3kjOFPgY/vc/yP3P2J0dtfApjZh4FPAT83+pr/ZmantvlA1lm63+/ne10OBoO8o7RIWd03GNz928DaAb/fM8DX3L3r7v8IXAU+coj6TrQwDGm1WkRRtOfm49zcXMGVidzbYe4xfM7MXhldaiyMjj0CvDX2nGujYxPM7JKZXTazy6urq4coo7yCIKDZbOLuzM7O4u6EYbinP4NIGb3XYPgi8DPAE8AN4A9Gx/dbNrjvoL27v+DuF9394p3j/A+SJEnyS4dsT0yRsntPweDu77h76u4D4E+4fblwDXhs7KmPAtcPV+LJFscxzWaTdrud71o1PgtSpIzeUzCY2UNjH/4qkI1YvAR8ysyqZvYB4HHgbw9X4sm1u7ubT38eD4TsfoNIWd13Cp6ZfRX4ReCsmV0D/iPwi2b2BMPLhDeA3wBw99fM7OvAj4A+8Jy7n9qFAbVaLV8slbV3q1arJEmipddSavcNBnf/9D6Hv3SP5/8+8PuHKepBkrV2i6KIarVKv99Xz0cpPU3an6IkSWg0GvmuVN1ul16vR7PZLLgykXtTMEyRu+/Zqi5bNyFSdvotnaI7t7ALwzDfUyJben3mzJn88/V6/fiKE7kHBUNB1tfXed/73sfW1hYwDIVOpzMRJiJFUDAUZGFhgTfffDNfUNXpdAB0qSGloN/CgmWXFAsLw1nlai8vZaBgKEh2CZF1jF5fX9dlhJSGRiUKEkVR3jU64+7qBymloDOGgjQaDYC8/2O9XidJEsLw1LavkBJRMBSo2+3S7/eJoohOp5Pvji1SNAVDgarVKjMzM3kYtFot3WeQUtA9hoK0Wi36/T6tVos4jvMGsRqVkDLQGUNBdnZ2mJ+fx93p9/tUKhXSNN0zE1KkKDpjKMj58+dJkgR3p9FosLOzQxAE+TCmSJF0xlCQbrdLHMcEQcD29jZBEOTLs0WKpmAoSNY5ulKpYGYMBgN2d3c1KiGloGAoSBiG+eSmbPs6GHZ9EimagqEgm5ubecOWRqORD1Oqi7SUgYKhIFmTWIBer5efNWgeg5SBgqEgi4uLdDodbt26lTdvgduLqkSKpGAoSJqmVCoVlpaWaLVaBEGAmWkRlZSCgqEgYRjmlxPNZpOrV6/qMkJKQ8FQkGyPCRiOSnzoQx9id3dXoxJSCgqGgmRnC+12m2q1mq+VUGs3KQP9Fhak0WiwtrZGo9Gg3W7ngTA+WiFSFAVDwZIk2bPJ7XhHJ5GiKBgKtLi4SBzHXL9+PZ/HoAlOUgYKhoJkO163Wi0efvhhGo0G7s7c3FzBlYkoGAqTXT40m03a7Xa+eGpzc7PIskQABUOhst4LjUZjYsbjfhvfZg1kRaZNwVCgbKl1t9ul0WjwzjvvcPbsWcyMVqsFDJdnZ1Omx1dhikyTgqEgvV6P+fl5arVavp/E+fPnuXLlSt7ebXZ2Nr/EUAMXOU76bSvI+B96FEWYGWmacuHChfw+Qza/YTAYaA2FHKv7njGY2WNm9tdm9rqZvWZmvzk6vmhm3zKzn4zeL4yOm5l9wcyumtkrZvbktP8RJ1F2udDtdvOQaLVatFotKpUK9XqdNE3zwBgMBjprkGNzkEuJPvDb7v4vgY8Cz5nZh4HngZfd/XHg5dHHAJ8AHh+9XQK+eORVPyCiKMrXS6yvrzM3N0ez2aTX6+07E1K7VMlxuW8wuPsNd//B6PE28DrwCPAM8OLoaS8Cnxw9fgb4ig99B5g3s4eOvPITLk3TfMGUu+e7XadpytbWVh4MURTl9xy0jkKOy7v6TTOz9wM/D3wXOO/uN2AYHsC50dMeAd4a+7Jro2MyZvz//uPToMMwZHZ2Nm8UW6/X89mQmhUpx+XAwWBmTeDPgd9y93ttfrDfZP+JO2dmdsnMLpvZ5dXV1YOWcSr0+32CIKDX67G9vU2SJNTrdd2AlGNzoGAwswrDUPgzd/+L0eF3skuE0fuV0fFrwGNjX/4ocP3O7+nuL7j7RXe/uLy8/F7rfyBFUbRnF2yATqdTZElyyhxkVMKALwGvu/sfjn3qJeDZ0eNngW+OHf/MaHTio8BmdskhB5MkyZ6ZkNk9BnV4kuNykPGvjwG/Dlwxsx+Ojv0u8J+Ar5vZZ4E3gV8bfe4vgaeBq0Ab+PdHWvEpEMcx7o6Z0el06HQ6BEGgDW/l2Nw3GNz9b9j/vgHAL+3zfAeeO2Rdp16SJARBQK1Wo9vtkqbpnj6RItOkGTMllKYpcRwzGAz2rI9QKMhx0cB4Cd3ZxUnzGOS46TethIIgoNVq4e4sLi6ytbWVb3wrchwUDCXUarVoNpvs7u6ytrbGwsJCHhIix0HBUEJZk5Zsn4n19XXOnj3L2tpawZXJaaFgKKGsH2S9Xs/vK9y5x6XINCkYSijrB1mtVllfXweGbd16vV6RZckpouHKEhoMBiRJgrvnU6J3dnYKrkpOE50xlFAQBPT7fer1+p5dqrSvpRwXBUMJbW5u5jcgG41GvkRby67luCgYSmh8hmOv18tDQouo5LgoGEpocXGRTqeTj0RkoxR37j0hMi0KhhJK05RKpcLS0hKtVot+v4+ZqVGLHBsFQwmNr6JsNptUKhVdRsixUjCUULfbzYOg3W5jZvksSJHjoGAoULYNXZqmtNvtvBHLnW3ikyTRBCc5VgqGAsVxDJBvOpOtnoyiKL+vMD8/T71ezzegETkOCoaCbG9vE8cx29vbtNtt4jimXq/nU6CzdRFhGOYjFLrPIMdFwVCQbFZjvV7n7Nmz9Hq9PRvPhGFIEATs7u7mx7INbkWmTcFQEDOj0WgQRRFra2tUKpV8P8tsv8q5uTkANjY2AG1RJ8dHwVCQbEgyTVMWFxfzOQqzs7OYGefOncsvK9ydmZkZTXCSY6NgKEi/3ycMQ8IwzNdAZL0e3Z2VlZU9m95qdaUcJwXDMUiShHa7nQ83pmlKFEVsbW3R6/Wo1Wo0Gg1g7wrKfr+vvSSkEOrHMGVZGGR/+FtbWwwGAxqNBtVqlUqlQpqm9Ho9Go3GnnbxIkXRGcOUBUGQ91PIAmF+fp44jvObilEU5ZOclpaWiixXBFAwTFWv1yMMQ6IoYnV1lcFgQBRFeXem8+fPY2ZEUZRPdrp161bBVYsoGKYqTdN8tGF5eTnfwTqO43xmIww3lMm2pFPDVykDBcMU1Wo1zCy/ZIDhDcWtrS3iOCaKIhqNBmtra/mWdJrdKGWgYJii7Ebi7OxsPgchiiLOnj0LDEMie072+WxhlUiRFAxT1Gg02NzcJAzD/PJhfX1dqySl9BQMU1apVPI1DlEUsbi4mN9rECkrBcOUZV2ekyTJbzZqMZSUnf7XNUXZxjHVapVGo8GZM2dwd9xd9xKk1HTGMEVBELCzs5NPctra2sLdtT+ElN59g8HMHjOzvzaz183sNTP7zdHxz5vZP5vZD0dvT499ze+Y2VUz+7GZ/co0/wFltrm5yQc/+EHCMKTX61Gv12m1WvlMSJGyOsilRB/4bXf/gZnNAt83s2+NPvdH7v5fx59sZh8GPgX8HPAw8H/N7Gfd/YHuS7a9vU2tVqNSqZAkCXEcMz8/v+c5nU4H0P4QUn73/V+Xu99w9x+MHm8DrwOP3ONLngG+5u5dd/9H4CrwkaMotqxWVlaYnZ2lUqnQ6XTyoUmRk+pdndOa2fuBnwe+Ozr0OTN7xcy+bGYLo2OPAG+Nfdk19gkSM7tkZpfN7PLq6uq7LrxMzp07B8Da2lq+O7WmNstJduBgMLMm8OfAb7n7FvBF4GeAJ4AbwB9kT93nyye2UHL3F9z9ortfXF5efteFl0m32yVN07w3o5nl+02KnEQHCgYzqzAMhT9z978AcPd33D119wHwJ9y+XLgGPDb25Y8C14+u5PKJoijvx5jNVdBwpJxkBxmVMOBLwOvu/odjxx8ae9qvAq+OHr8EfMrMqmb2AeBx4G+PruTyGQwGpGlKEARq2CoPhIOMSnwM+HXgipn9cHTsd4FPm9kTDC8T3gB+A8DdXzOzrwM/Yjii8dyDPiJRqVTyEYhsSFKb0MpJZmX45TWzVWAHuFl0LQdwlpNRJ5ycWlXn0duv1n/h7ge6oVeKYAAws8vufrHoOu7npNQJJ6dW1Xn0DlurpuCJyAQFg4hMKFMwvFB0AQd0UuqEk1Or6jx6h6q1NPcYRKQ8ynTGICIlUXgwmNnHR8uzr5rZ80XXcycze8PMroyWll8eHVs0s2+Z2U9G7xfu932mUNeXzWzFzF4dO7ZvXTb0hdFr/IqZPVmCWku3bP8eLQZK9boeSyuErKNQEW9ACPwU+CAQA38HfLjImvap8Q3g7B3H/gvw/Ojx88B/LqCuXwCeBF69X13A08D/YriO5aPAd0tQ6+eB/7DPcz88+j2oAh8Y/X6Ex1TnQ8CTo8ezwN+P6inV63qPOo/sNS36jOEjwFV3/wd3T4CvMVy2XXbPAC+OHr8IfPK4C3D3bwNrdxy+W13PAF/xoe8A83dMaZ+qu9R6N4Ut2/e7txgo1et6jzrv5l2/pkUHw4GWaBfMgb8ys++b2aXRsfPufgOG/5GAc4VVt9fd6irr6/yel+1P2x0tBkr7uh5lK4RxRQfDgZZoF+xj7v4k8AngOTP7haILeg/K+Dofatn+NO3TYuCuT93n2LHVetStEMYVHQylX6Lt7tdH71eAbzA8BXsnO2UcvV8prsI97lZX6V5nL+my/f1aDFDC13XarRCKDobvAY+b2QfMLGbYK/KlgmvKmdmMDftcYmYzwC8zXF7+EvDs6GnPAt8spsIJd6vrJeAzo7voHwU2s1PjopRx2f7dWgxQstf1bnUe6Wt6HHdR73OH9WmGd1V/Cvxe0fXcUdsHGd7N/Tvgtaw+YAl4GfjJ6P1iAbV9leHpYo/h/xE+e7e6GJ5K/vHoNb4CXCxBrf99VMsro1/ch8ae/3ujWn8MfOIY6/zXDE+xXwF+OHp7umyv6z3qPLLXVDMfRWRC0ZcSIlJCCgYRmaBgEJEJCgYRmaBgEJEJCgYRmaBgEJEJCgYRmfD/ATu/Vul9vjX8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]]\n",
      "train_label:  (16, 256, 256, 1)\n",
      "train_image:  (16, 256, 256, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  ...\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]]\n"
     ]
    }
   ],
   "source": [
    "# use to view images/masks in dataset\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "SEED = 42\n",
    "training_data = \"../dataset/training/\"\n",
    "BATCH_SIZE = 16\n",
    "BUFFER_SIZE = 1000 # See https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle\n",
    "\n",
    "# Creating and splitting dataset\n",
    "DATASET_SIZE = len(glob(training_data + \"images/*.jpg\"))\n",
    "print(f\"The Training Dataset contains {DATASET_SIZE} images.\")\n",
    "\n",
    "TRAIN_SIZE = int(0.8 * DATASET_SIZE)\n",
    "VAL_SIZE = int(0.2 * DATASET_SIZE)\n",
    "\n",
    "full_dataset = tf.data.Dataset.list_files(training_data + \"images/*.jpg\", seed=SEED)\n",
    "full_dataset = full_dataset.shuffle(buffer_size=BUFFER_SIZE, seed=SEED)\n",
    "train_dataset = full_dataset.take(TRAIN_SIZE)\n",
    "val_dataset = full_dataset.skip(TRAIN_SIZE)\n",
    "    \n",
    "# Creating d1ict pairs linking images and annotations\n",
    "train_dataset = train_dataset.map(parse_image)\n",
    "val_dataset = val_dataset.map(parse_image)\n",
    "\n",
    "# -- Train Dataset --# - https://stackoverflow.com/questions/49915925/output-differences-when-changing-order-of-batch-shuffle-and-repeat\n",
    "train_dataset = train_dataset.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=BUFFER_SIZE, seed=SEED)\n",
    "train_dataset = train_dataset.repeat()\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "#-- Validation Dataset --#\n",
    "val_dataset = val_dataset.map(load_image_val, num_parallel_calls=AUTOTUNE)\n",
    "val_dataset = val_dataset.repeat()\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "val_dataset = val_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "\n",
    "showDataImgs(train_dataset, val_dataset, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for example in train_dataset.take(1):  # example is `{'image': tf.Tensor, 'label': tf.Tensor}`\n",
    "#for example in next(val_dataset_np):\n",
    "  #print(example)\n",
    "  #image = example[0]\n",
    "  #label = example[1]\n",
    "  #label = tf.cast(label, tf.float32) / 255.0 #normalizing label  \n",
    "  #print((image==label).all())\n",
    "  #print(image.size)\n",
    "#  image_img = Image.fromarray(image, 'RGB')\n",
    " #\n",
    "  #plt.imshow(image)\n",
    "  #plt.show()\n",
    "#   image_img.save('image_test.png')\n",
    "#   image_img.show()\n",
    "\n",
    "#  label_img = Image.fromarray(label, 'RGB')\n",
    "  #label= np.reshape(label, (256,256))\n",
    "  #plt.imshow(label)\n",
    "  #plt.show()\n",
    "#   label_img.save('label_test.png')\n",
    "#   label_img.show()\n",
    "\n",
    "# for example in val_dataset.take(2):  # example is `{'image': tf.Tensor, 'label': tf.Tensor}`\n",
    "#   #print(example)\n",
    "#   #image = example[0]\n",
    "#   #label = example[1]\n",
    "#   print(image.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(backbone, weight_file):\n",
    "\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    print(f\"Tensorflow ver. {tf.__version__}\")\n",
    "\n",
    "    # For reproducibility\n",
    "    SEED = 42\n",
    "\n",
    "    # Relevant directories/files\n",
    "    image_dir = \"../dataset/testing/images\"\n",
    "    annotation_dir = \"../dataset/testing/annotations\"\n",
    "    out_dir = \"../dataset/testing/output\"\n",
    "    testing_data = \"../dataset/testing/\"\n",
    "    model_weights = weight_file\n",
    "\n",
    "    #Listing GPU info\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "       try:\n",
    "           for gpu in gpus:\n",
    "               tf.config.experimental.set_memory_growth(gpu, True)\n",
    "           logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "           print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "       except RuntimeError as e:\n",
    "           print(e)\n",
    "\n",
    "    # Hyperparams\n",
    "    BATCH_SIZE = 16\n",
    "    BUFFER_SIZE = 1000 # See https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle\n",
    "\n",
    "    model = sm.Unet(\n",
    "        #'vgg16', \n",
    "        backbone,\n",
    "        input_shape=(HEIGHT, WIDTH, N_CHANNELS), \n",
    "        encoder_weights='imagenet', \n",
    "        weights=model_weights,\n",
    "        encoder_freeze=True,    # only training decoder network\n",
    "        classes=NUM_CLASSES, \n",
    "        activation='sigmoid'\n",
    "    )\n",
    "\n",
    "    # Might be unnecessary\n",
    "    model.compile(\n",
    "        'Adam', \n",
    "        #loss=sm.losses.bce_jaccard_loss, \n",
    "        loss=LOSS_FUNC,\n",
    "        metrics=[sm.metrics.iou_score]\n",
    "    )\n",
    "\n",
    "    test_dataset = glob(os.path.join(image_dir, \"*.jpg\"))\n",
    "    \n",
    "    # Loop for inference\n",
    "    print(\"\\nStarting inference... \\n\")\n",
    "    for img_file in tqdm(test_dataset):\n",
    "        tif_file = img_file.replace(\"jpg\", \"tif\")\n",
    "\n",
    "        img = np.asarray(Image.open(img_file)) #/ 255.0 # normalization not needed as we dont normalize the img for training\n",
    "        img = img[np.newaxis, ...] # needs (batch_size, height, width, channels)\n",
    "        pred_mask = model.predict(img)[0]\n",
    "        pred_mask = create_mask(pred_mask)\n",
    "        pred_mask = np.array(pred_mask).astype('uint8') * 255\n",
    "\n",
    "        # Reading metadata from .tif\n",
    "        with rasterio.open(tif_file) as src:\n",
    "            tif_meta = src.meta\n",
    "            tif_meta['count'] = 1\n",
    "\n",
    "        # Writing prediction mask as a .tif using extracted metadata\n",
    "        mask_file = tif_file.replace(\"images\", \"output\")\n",
    "        with rasterio.open(mask_file, \"w\", **tif_meta) as dest:\n",
    "            # Rasterio needs [bands, width, height]\n",
    "            pred_mask = np.rollaxis(pred_mask, axis=2)\n",
    "            dest.write(pred_mask)\n",
    "\t#printing out metrics\n",
    "\t#results = model.evaluate(img, pred_mask, batch_size=128)\n",
    "\t#print(\"IOU: \", results) \n",
    "    print(\"Merging tiles (to create mask ortho)...\")\n",
    "    call = \"gdal_merge.py -o \" + testing_data + \"ortho_mask.tif \" + \" \" + out_dir + \"/*\"\n",
    "    print(call)\n",
    "    subprocess.call(call, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_optimized(backbone, weight_file):\n",
    "    '''\n",
    "    Note: This version of test does not work yet. It is optimized to be very efficient and works well for inference on .jpg files.\n",
    "    It lacks the capabilities to link the output predictions to the input .jpgs since the filenames are lost when in the tf.dataset\n",
    "    we map the parse image function. As a result, we need to somehow modify this dataset to retain filename information so we can use it\n",
    "    to link the output prediction to the input image and its corresponding .tif file, which will be used to write the geospatial info to\n",
    "    the prediction.\n",
    "\n",
    "    Initial ideas would be to modify the parse image function and related functions to save filename info, and use this to link the images\n",
    "    in the prediction stage by replacing .jpg with .tif in the filename.\n",
    "    '''\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    print(f\"Tensorflow ver. {tf.__version__}\")\n",
    "\n",
    "    # For reproducibility\n",
    "    SEED = 42\n",
    "\n",
    "    # Relevant directories/files\n",
    "    images = \"../dataset/testing/images\"\n",
    "    annotations = \"../dataset/testing/annotations\"\n",
    "    testing_data = \"../dataset/testing/\"\n",
    "    #model_weights = \"unet_500_weights_vgg16.h5\"\n",
    "    model_weights = weight_file\n",
    "\n",
    "    # Listing GPU info\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "\n",
    "    # Hyperparams\n",
    "    BATCH_SIZE = 16\n",
    "    BUFFER_SIZE = 1000 # See https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle\n",
    "\n",
    "    model = sm.Unet(\n",
    "        'resnet34', \n",
    "        input_shape=(HEIGHT, WIDTH, N_CHANNELS), \n",
    "        encoder_weights='imagenet', \n",
    "        weights=model_weights,\n",
    "        encoder_freeze=True,    # only training decoder network\n",
    "        classes=2, \n",
    "        activation='softmax'\n",
    "    )\n",
    "\n",
    "    # Might be unnecessary\n",
    "    model.compile(\n",
    "        'Adam', \n",
    "        loss=LOSS_FUNC, \n",
    "        metrics=[sm.metrics.iou_score]\n",
    "    )\n",
    "\n",
    "    test_dataset = tf.data.Dataset.list_files(testing_data + \"images/*.jpg\", seed=SEED)\n",
    "    test_dataset = test_dataset.map(parse_image)\n",
    "    test_dataset = test_dataset.map(load_image_val, num_parallel_calls=AUTOTUNE)\n",
    "    test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "    test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    image_data = []\n",
    "    annotation_data = []\n",
    "    \n",
    "    '''\n",
    "    for img_file in tqdm(os.listdir(images)): \n",
    "        annotation_file = \"annotation_\" + img_file.split('_')[1]\n",
    "        img_file = os.path.join(images, img_file)\n",
    "        ann_file = os.path.join(annotations, annotation_file)\n",
    "        image = np.array(Image.open(img_file))\n",
    "        annotation = np.array(Image.open(ann_file))\n",
    "        image_data.append(image)\n",
    "        annotation_data.append(annotation)\n",
    "    '''\n",
    "\n",
    "\n",
    "    #prediction = model.predict(test_dataset, steps=1)\n",
    "    #print(type(prediction))\n",
    "\n",
    "\n",
    "    #display([first_image[0], first_mask[0], create_mask(first_pred_mask)])\n",
    "\n",
    "    #pred_mask = model.predict(test_dataset)\n",
    "    #display([image[0], mask[0], create_mask(pred_mask)])\n",
    "\n",
    "    show_predictions(model=model, dataset=test_dataset, num=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_setup(raster_files, vector_files, out_width):\n",
    "    # Uses raster and vector file to create dataset for training\n",
    "    data_files = zip(raster_files, vector_files)\n",
    "    map_files = [] \n",
    "\n",
    "    for raster_file, vector_file in data_files:\n",
    "        # Generates raster masks\n",
    "        print(\"Creating raster_masks...\")\n",
    "        raster_mask(raster_file, vector_file)\n",
    "        temp_dir = os.path.dirname(vector_file)\n",
    "        mask_file = os.path.join(temp_dir, \"masks\", \"mask_binary.tif\")\n",
    "\n",
    "        # Generates segmentation labels\n",
    "        out_dir = os.path.dirname(raster_file)\n",
    "        gen_seg_labels(out_width, raster_file, vector_file, mask_file, out_dir, True, True)\n",
    "        map_file = os.path.join(out_dir, \"map.txt\")\n",
    "        map_files.append(map_file)\n",
    "\n",
    "    # Creating dataset to train UNet\n",
    "    create_seg_dataset(map_files, \"training\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_setup(raster_files, out_width):\n",
    "    out_dir = \"../dataset/testing/output\"\n",
    "    test_dir = \"../dataset/testing\"\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "\n",
    "    print(\"\\nTiling rasters...\")\n",
    "    for raster_file in raster_files:\n",
    "        tile_raster(out_width, raster_file, test_dir, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initalize arguments\n",
    "out_width = \"256\"\n",
    "raster_files = [\"../dataset/training/images/lap_2019-07_site03_120m_RGB_quick.tif\"]#, \"../dataset/training/images/lap_2019-07_site06_120m_RGB_quick.tif\"]\n",
    "vector_files = [\"../dataset/training/vectors/lap_2019-07_site03_labels_m-nm.shp\"]#, \"../dataset/training/vectors/lap_2019-07_site06_labels_m-nm.shp\"]\n",
    "weight_file = \"../dataset/training/weights/07_16_vgg16_50_03_weight.h5\"\n",
    "backbone = \"vgg16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Testing\n",
    "test_setup(raster_files, out_width)\n",
    "test(backbone, weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_setup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c424a217fea2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Run Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraster_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_setup' is not defined"
     ]
    }
   ],
   "source": [
    "#Run Training\n",
    "train_setup(raster_files, vector_files, out_width)\n",
    "train(backbone, weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ddc828fad694>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../dataset/testing/ortho_mask.tif\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;31m#show(resampled_10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/rasterio/plot.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(source, with_bounds, contour, contour_label_kws, ax, title, transform, adjust, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mrasterio/_io.pyx\u001b[0m in \u001b[0;36mrasterio._io.DatasetReaderBase.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py37_tensorflow/lib/python3.7/site-packages/numpy/ma/core.py\u001b[0m in \u001b[0;36marray\u001b[0;34m(data, dtype, copy, order, mask, fill_value, keep_mask, hard_mask, shrink, subok, ndmin)\u001b[0m\n\u001b[1;32m   6455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6457\u001b[0;31m def array(data, dtype=None, copy=False, order=None,\n\u001b[0m\u001b[1;32m   6458\u001b[0m           \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnomask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6459\u001b[0m           hard_mask=False, shrink=True, subok=True, ndmin=0):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#view downsmapled testing output ortho_mask.tif\n",
    "# !sudo apt-get update\n",
    "# !sudo apt-get install libgdal-dev -y\n",
    "# !sudo apt-get install python-gdal -y\n",
    "# !sudo apt-get install python-numpy python-scipy -y\n",
    "# !pip install rasterio\n",
    "# !pip install fiona\n",
    "# !pip install geopandas\n",
    "# !pip install -i https://test.pypi.org/simple/ gis-utils-pkg-dillhicks==0.0.1\n",
    "from gis_utils import raster\n",
    "from rasterio.plot import show\n",
    "\n",
    "#img_10, meta1 = raster.load_image(\"../dataset/testing/ortho_mask_10_2.tif\")\n",
    "#img_1, meta10 = raster.load_image(\"../dataset/testing/ortho_mask_1_1.tif\")\n",
    "\n",
    "#downsampling images \n",
    "ds_factor = 1\n",
    "#resampled_1, transform = raster.downsample_raster(img_1, ds_factor)\n",
    "#resampled_10, transform = raster.downsample_raster(img_10, ds_factor)\n",
    "\n",
    "img, meta = raster.load_image(\"../dataset/testing/ortho_mask.tif\")\n",
    "show(img)\n",
    "#show(resampled_10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     TRAIN = False\n",
    "#     TEST = False\n",
    "\n",
    "#     parser = argparse.ArgumentParser(description=\"UNet Training and Inference Script (Note: order of rasters and vectors must correspond to one another)\")\n",
    "#     parser.add_argument(\"--width\",help = \"Width of output tiles\")\n",
    "#     parser.add_argument(\"--input_rasters\", nargs='*', help = \"space separated input orthomosaic (.tif)\")\n",
    "#     parser.add_argument(\"--input_vectors\", nargs='*', help = \"space separated input labels (.shp)\")\n",
    "#     parser.add_argument(\"--train\", action='store_true', help = \"training UNet\")\n",
    "#     parser.add_argument(\"--test\", action='store_true', help = \"testing UNet\")\n",
    "#     parser.add_argument(\"--weights\", help = \"path to weight file, either to save or use (.h5)\")\n",
    "#     parser.add_argument(\"--backbone\", help = \"segmentation model backbone, ex: resnet34, vgg16, etc.\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     # Parsing arguments\n",
    "#     if args.width:\n",
    "#         out_width = args.width\n",
    "#     else:\n",
    "#         print(\"Need to specify width, exiting.\")\n",
    "#         exit()\n",
    "#     if args.input_rasters:\n",
    "#         raster_files = args.input_rasters\n",
    "#     else:\n",
    "#         # Always needs a raster\n",
    "#         print(\"Need to specify raster file, exiting.\")\n",
    "#         exit()\n",
    "#     if args.input_vectors:\n",
    "#         vector_files = args.input_vectors\n",
    "#     else:\n",
    "#         # Requires vector labes for training, not inference\n",
    "#         if args.train:\n",
    "#             print(\"Need to specify input vector, exiting.\")\n",
    "#             exit()\n",
    "#     if args.train and args.test:\n",
    "#         print(\"Can't train and test at the same time... exiting.\")\n",
    "#         exit()\n",
    "#     elif args.train:\n",
    "#         TRAIN = True\n",
    "#     elif args.test:\n",
    "#         TEST = True\n",
    "#     if args.weights:\n",
    "#         weight_file = args.weights\n",
    "#     else:\n",
    "#         print(\"Need weight file, exiting.\")\n",
    "#         exit()\n",
    "#     if args.backbone:\n",
    "#         backbone = args.backbone\n",
    "#     else:\n",
    "#         print(\"Need to specify backbone, exiting.\")\n",
    "#         exit()\n",
    "\n",
    "#     # Selecting mode\n",
    "#     if TRAIN: \n",
    "#         train_setup(raster_files, vector_files, out_width)\n",
    "#         train(backbone, weight_file)\n",
    "#     if TEST:\n",
    "#         test_setup(raster_files, out_width)\n",
    "#         test(backbone, weight_file)\n",
    "#         #test_optimized(backbone, weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
