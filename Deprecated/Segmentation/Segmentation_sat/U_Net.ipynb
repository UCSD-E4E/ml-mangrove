{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XPs293SKmUlW",
    "outputId": "ddf6d732-2fba-437e-c277-fd4e72071cfb"
   },
   "outputs": [],
   "source": [
    "# #please uncomment the following if needed\n",
    "# ! pip install torch torchvision\n",
    "# ! pip install matplotlib\n",
    "# ! pip install opencv-contrib-python\n",
    "# ! pip install imutils\n",
    "# ! pip install scikit-learn\n",
    "# ! pip install tqdm\n",
    "# ! pip install rasterio\n",
    "# ! pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "v8QDUl5wrbU7"
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from rasterio.plot import reshape_as_raster, reshape_as_image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from rasterio.plot import show\n",
    "import cv2\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import ConvTranspose2d\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import Module\n",
    "from torch.nn import ModuleList\n",
    "from torch.nn import ReLU\n",
    "from torchvision.transforms import CenterCrop\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "from imutils import paths\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from SegDataset import SegmentationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LfYIG5Uil91U"
   },
   "outputs": [],
   "source": [
    "#read data from google drive by first upload the dataset to My Drive,\n",
    "#can also find it in https://www.kaggle.com/datasets/stanleydil/e4e-jamaica-segmentation-dataset by read from kaggle\n",
    "#For jupyter notebook\n",
    "with open('128px_images.pkl', 'rb') as handle:\n",
    "    images = pickle.load(handle)\n",
    "\n",
    "with open('128px_labels.pkl', 'rb') as handle:\n",
    "    labels = pickle.load(handle)\n",
    "\n",
    "with open('128_mlabels.pkl', 'rb') as handle:\n",
    "    m_tiles = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For google colab\n",
    "# with open('/content/drive/My Drive/128px_images.pkl', 'rb') as handle:\n",
    "#     images = pickle.load(handle)\n",
    "\n",
    "# with open('/content/drive/My Drive/128px_labels.pkl', 'rb') as handle:\n",
    "#     labels = pickle.load(handle)\n",
    "\n",
    "# with open('/content/drive/My Drive/128_mlabels.pkl', 'rb') as handle:\n",
    "#     m_tiles = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9FAKvdzvWgW"
   },
   "source": [
    "Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AG4-7_T3uWXH"
   },
   "outputs": [],
   "source": [
    "TEST_SPLIT = 0.15\n",
    "# determine the device to be used for training and evaluation\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# determine if we will be pinning memory during data loading\n",
    "PIN_MEMORY = True if DEVICE == \"cuda\" else False\n",
    "# define the number of channels in the input, number of classes,\n",
    "# and number of levels in the U-Net model\n",
    "NUM_CHANNELS = 9\n",
    "NUM_CLASSES = 1\n",
    "NUM_LEVELS = 3\n",
    "# initialize learning rate, number of epochs to train for, and the\n",
    "# batch size\n",
    "INIT_LR = 0.001\n",
    "NUM_EPOCHS = 40\n",
    "BATCH_SIZE = 64\n",
    "# define the input image dimensions\n",
    "INPUT_IMAGE_WIDTH = 128 #or 256\n",
    "INPUT_IMAGE_HEIGHT = 128\n",
    "# define threshold to filter weak predictions\n",
    "THRESHOLD = 0.5\n",
    "# define the path to the base output directory\n",
    "BASE_OUTPUT = \"output\"\n",
    "# define the path to the output serialized model, model training\n",
    "# plot, and testing image paths\n",
    "# MODEL_PATH = os.path.join(BASE_OUTPUT, \"unet_mangrove.pth\")\n",
    "# PLOT_PATH = os.path.sep.join([BASE_OUTPUT, \"plot.png\"])\n",
    "# TEST_PATHS = os.path.sep.join([BASE_OUTPUT, \"test_paths.txt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0HBrFN4wCWY"
   },
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bbdblfQlwJiB"
   },
   "outputs": [],
   "source": [
    "# Moved to SegDataset.py\n",
    "# class SegmentationDataset(Dataset):\n",
    "#   def __init__(self, images, labels, transforms):\n",
    "#     # store the image and mask filepaths, and augmentation\n",
    "# \t\t# transforms\n",
    "#     self.transforms = transforms\n",
    "#     self.images = images\n",
    "#     self.labels = labels\n",
    "#   def __len__(self):\n",
    "#     # return the number of total samples contained in the dataset\n",
    "#     return np.array(self.images).shape[0]\n",
    "#   def __getitem__(self, idx):\n",
    "#     try:\n",
    "#       image = self.images[idx]\n",
    "#       mask = self.labels[idx]\n",
    "#     except:\n",
    "#       print (\"no index at\", idx)\n",
    "#     #image = self.images[idx]\n",
    "# \t\t# load the image from disk, swap its channels from BGR to RGB,\n",
    "# \t\t# and read the associated mask from disk in grayscale mode\n",
    "# \t\t# check to see if we are applying any transformations\n",
    "#       if self.transforms is not None:\n",
    "# \t\t\t# apply the transformations to both image and its mask\n",
    "#         image = self.transforms(image)\n",
    "#         mask = self.transforms(mask)\n",
    "# \t\t# return a tuple of the image and its mask\n",
    "#     return (image, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIOEsbVSdSfz"
   },
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tecqW-CQdcSB"
   },
   "outputs": [],
   "source": [
    "class Block(Module):\n",
    "  def __init__(self, inChannels, outChannels):\n",
    "    super().__init__()\n",
    "\t\t# store the convolution and RELU layers\n",
    "    self.conv1 = Conv2d(inChannels, outChannels, 3)\n",
    "    self.relu = ReLU()\n",
    "    self.conv2 = Conv2d(outChannels, outChannels, 3)\n",
    "  def forward(self, x):\n",
    "\t\t# apply CONV => RELU => CONV block to the inputs and return it\n",
    "    return self.conv2(self.relu(self.conv1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7NCw5u9pdsIo"
   },
   "outputs": [],
   "source": [
    "class Encoder(Module):\n",
    "  def __init__(self, channels=(9, 16, 32, 64)):\n",
    "    super().__init__()\n",
    "    # store the encoder blocks and maxpooling layer\n",
    "    self.encBlocks = ModuleList(\n",
    "        [Block(channels[i], channels[i + 1])\n",
    "        for i in range(len(channels) - 1)])\n",
    "    self.pool = MaxPool2d(2)\n",
    "  def forward(self, x):\n",
    "\t\t# initialize an empty list to store the intermediate outputs\n",
    "    blockOutputs = []\n",
    "\t\t# loop through the encoder blocks\n",
    "    for block in self.encBlocks:\n",
    "\t\t\t# pass the inputs through the current encoder block, store\n",
    "\t\t\t# the outputs, and then apply maxpooling on the output\n",
    "      x = block(x)\n",
    "      blockOutputs.append(x)\n",
    "      x = self.pool(x)\n",
    "\t\t# return the list containing the intermediate outputs\n",
    "    return blockOutputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dBQmY_c7duHt"
   },
   "outputs": [],
   "source": [
    "class Decoder(Module):\n",
    "  def __init__(self, channels=(64, 32, 16)):\n",
    "    super().__init__()\n",
    "\t\t# initialize the number of channels, upsampler blocks, and\n",
    "\t\t# decoder blocks\n",
    "    self.channels = channels\n",
    "    self.upconvs = ModuleList(\n",
    "        [ConvTranspose2d(channels[i], channels[i + 1], 2, 2)\n",
    "        for i in range(len(channels) - 1)])\n",
    "    self.dec_blocks = ModuleList(\n",
    "        [Block(channels[i], channels[i + 1])\n",
    "        for i in range(len(channels) - 1)])\n",
    "  def forward(self, x, encFeatures):\n",
    "\t\t# loop through the number of channels\n",
    "    for i in range(len(self.channels) - 1):\n",
    "\t\t\t# pass the inputs through the upsampler blocks\n",
    "      x = self.upconvs[i](x)\n",
    "\t\t\t# crop the current features from the encoder blocks,\n",
    "\t\t\t# concatenate them with the current upsampled features,\n",
    "\t\t\t# and pass the concatenated output through the current\n",
    "\t\t\t# decoder block\n",
    "      encFeat = self.crop(encFeatures[i], x)\n",
    "      x = torch.cat([x, encFeat], dim=1)\n",
    "      x = self.dec_blocks[i](x)\n",
    "\t\t# return the final decoder output\n",
    "    return x\n",
    "  def crop(self, encFeatures, x):\n",
    "\t\t# grab the dimensions of the inputs, and crop the encoder\n",
    "\t\t# features to match the dimensions\n",
    "    (_, _, H, W) = x.shape\n",
    "    encFeatures = CenterCrop([H, W])(encFeatures)\n",
    "\t\t# return the cropped features\n",
    "    return encFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "EQ_u-EsieFew"
   },
   "outputs": [],
   "source": [
    "class UNet(Module):\n",
    "  def __init__(self, encChannels=(9, 16, 32, 64), #9 channels since 9 features\n",
    "               decChannels=(64, 32, 16),\n",
    "               nbClasses=1, retainDim=True, #one channel for binary be thresholding\n",
    "               outSize=(INPUT_IMAGE_HEIGHT,  INPUT_IMAGE_WIDTH)):\n",
    "    super().__init__()\n",
    "\t\t# initialize the encoder and decoder\n",
    "    self.encoder = Encoder(encChannels)\n",
    "    self.decoder = Decoder(decChannels)\n",
    "\t\t# initialize the regression head and store the class variables\n",
    "    self.head = Conv2d(decChannels[-1], nbClasses, 1)\n",
    "    self.retainDim = retainDim\n",
    "    self.outSize = outSize\n",
    "  def forward(self, x):\n",
    "\t\t# grab the features from the encoder\n",
    "    encFeatures = self.encoder(x)\n",
    "\t\t# pass the encoder features through decoder making sure that\n",
    "\t\t# their dimensions are suited for concatenation\n",
    "    decFeatures = self.decoder(encFeatures[::-1][0],\n",
    "                               encFeatures[::-1][1:])\n",
    "\t\t# pass the decoder features through the regression head to\n",
    "\t\t# obtain the segmentation mask\n",
    "    map = self.head(decFeatures)\n",
    "\t\t# check to see if we are retaining the original output\n",
    "\t\t# dimensions and if so, then resize the output to match them\n",
    "    if self.retainDim:\n",
    "      map = F.interpolate(map, self.outSize)\n",
    "\t\t# return the segmentation map\n",
    "    return map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9r0eXcipgDnY"
   },
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nRlQggGCgh4T",
    "outputId": "5ecc546b-7cec-4a70-a18b-921221a52d26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] found 3478 examples in the training set...\n",
      "[INFO] found 614 examples in the test set...\n"
     ]
    }
   ],
   "source": [
    "# partition the data into training and testing splits using 85% of\n",
    "# the data for training and the remaining 15% for testing\n",
    "split = train_test_split(images, labels, #images, labels,\n",
    "                         test_size=TEST_SPLIT, random_state=42)\n",
    "# unpack the data split\n",
    "(trainImages, testImages) = split[:2]\n",
    "(trainMasks, testMasks) = split[2:]\n",
    "# define transformations\n",
    "transforms = transforms.Compose([transforms.ToPILImage(),\n",
    "                                 transforms.Resize((INPUT_IMAGE_HEIGHT,\n",
    "                                                    INPUT_IMAGE_WIDTH)),\n",
    "                                 transforms.ToTensor()])\n",
    "# create the train and test datasets\n",
    "trainDS = SegmentationDataset(images=trainImages, labels=trainMasks, transforms = None)\n",
    "                              #transforms=transforms)\n",
    "testDS = SegmentationDataset(images=testImages, labels=testMasks, transforms = None)\n",
    "                             #transforms=transforms)\n",
    "print(f\"[INFO] found {len(trainDS)} examples in the training set...\")\n",
    "print(f\"[INFO] found {len(testDS)} examples in the test set...\")\n",
    "# create the training and test data loaders #config\n",
    "trainLoader = DataLoader(trainDS, shuffle=True,\n",
    "                         batch_size=BATCH_SIZE, #pin_memory=PIN_MEMORY,\n",
    "                         num_workers=0)#os.cpu_count())\n",
    "testLoader = DataLoader(testDS, shuffle=False,\n",
    "                        batch_size=BATCH_SIZE, #pin_memory=PIN_MEMORY,\n",
    "                        num_workers=0)\n",
    "# initialize our UNet model #config\n",
    "unet = UNet().to(DEVICE)\n",
    "# initialize loss function and optimizer\n",
    "lossFunc = BCEWithLogitsLoss()\n",
    "opt = Adam(unet.parameters(), lr=INIT_LR)\n",
    "# calculate steps per epoch for training and test set #config\n",
    "trainSteps = len(trainDS) // BATCH_SIZE\n",
    "testSteps = len(testDS) // BATCH_SIZE\n",
    "# initialize a dictionary to store training history\n",
    "H = {\"train_loss\": [], \"test_loss\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T1Sd5NeJlUa8",
    "outputId": "23bce897-016f-4673-d72a-12e44bf043f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training the network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|█                                        | 1/40 [02:48<1:49:17, 168.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 1/40\n",
      "Train loss: 0.213196, Test loss: 0.2004\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|██                                       | 2/40 [05:48<1:51:11, 175.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 2/40\n",
      "Train loss: 0.199471, Test loss: 0.1974\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|███                                      | 3/40 [09:25<1:59:41, 194.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 3/40\n",
      "Train loss: 0.187341, Test loss: 0.1914\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████                                     | 4/40 [12:34<1:55:19, 192.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 4/40\n",
      "Train loss: 0.175862, Test loss: 0.1667\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████▏                                   | 5/40 [15:27<1:48:11, 185.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 5/40\n",
      "Train loss: 0.158456, Test loss: 0.1692\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|██████▏                                  | 6/40 [18:15<1:41:45, 179.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 6/40\n",
      "Train loss: 0.150576, Test loss: 0.1513\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|███████▏                                 | 7/40 [21:16<1:38:51, 179.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 7/40\n",
      "Train loss: 0.143138, Test loss: 0.1387\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████▏                                | 8/40 [24:08<1:34:33, 177.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 8/40\n",
      "Train loss: 0.148717, Test loss: 0.1451\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|█████████▏                               | 9/40 [27:32<1:35:59, 185.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 9/40\n",
      "Train loss: 0.140662, Test loss: 0.1452\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██████████                              | 10/40 [30:15<1:29:20, 178.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 10/40\n",
      "Train loss: 0.140718, Test loss: 0.1357\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|███████████                             | 11/40 [33:18<1:26:58, 179.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 11/40\n",
      "Train loss: 0.277177, Test loss: 0.7450\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████                            | 12/40 [36:09<1:22:41, 177.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 12/40\n",
      "Train loss: 0.442092, Test loss: 0.1710\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|█████████████                           | 13/40 [39:13<1:20:40, 179.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 13/40\n",
      "Train loss: 0.159486, Test loss: 0.1506\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|██████████████                          | 14/40 [42:11<1:17:31, 178.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 14/40\n",
      "Train loss: 0.148129, Test loss: 0.1412\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|███████████████                         | 15/40 [45:00<1:13:20, 176.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 15/40\n",
      "Train loss: 0.144600, Test loss: 0.1475\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████████████████                        | 16/40 [47:43<1:08:53, 172.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 16/40\n",
      "Train loss: 0.146955, Test loss: 0.1471\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|█████████████████                       | 17/40 [50:45<1:07:07, 175.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 17/40\n",
      "Train loss: 0.146933, Test loss: 0.1415\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|██████████████████                      | 18/40 [53:36<1:03:43, 173.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 18/40\n",
      "Train loss: 0.143202, Test loss: 0.1406\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|███████████████████▉                      | 19/40 [56:10<58:46, 167.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 19/40\n",
      "Train loss: 0.139161, Test loss: 0.1378\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n",
      "checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|███████████████████                     | 19/40 [58:13<1:04:20, 183.85s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m \t\t\u001b[38;5;66;03m# first, zero out any previously accumulated gradients, then\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \t\t\u001b[38;5;66;03m# perform backpropagation, and then update model parameters\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 28\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MangroveML/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/MangroveML/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loop over epochs #config\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()\n",
    "for e in tqdm(range(NUM_EPOCHS)):\n",
    "\t# set the model in training mode\n",
    "  unet.train()\n",
    "\n",
    "\t# initialize the total training and validation loss\n",
    "  totalTrainLoss = 0\n",
    "  totalTestLoss = 0\n",
    "\n",
    "\t# loop over the training set\n",
    "  for (i, (x, y)) in enumerate(trainLoader):\n",
    "\t\t# send the input to the device\n",
    "    x = np.transpose(x, (0, 3, 1, 2))\n",
    "    y = np.transpose(y, (0, 3, 1, 2))\n",
    "    (x, y) = (x.to(DEVICE), y.to(DEVICE)) #config\n",
    "\n",
    "\t\t# perform a forward pass and calculate the training loss\n",
    "    #print(x.shape)\n",
    "    pred = unet(x)\n",
    "    loss = lossFunc(pred, y)\n",
    "\n",
    "\t\t# first, zero out any previously accumulated gradients, then\n",
    "\t\t# perform backpropagation, and then update model parameters\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(\"checkpoint\")\n",
    "\n",
    "\t\t# add the loss to the total training loss so far\n",
    "    totalTrainLoss += loss\n",
    "\n",
    "\t# switch off autograd\n",
    "  with torch.no_grad():\n",
    "\t\t# set the model in evaluation mode\n",
    "    unet.eval()\n",
    "\n",
    "\t\t# loop over the validation set\n",
    "    for (x, y) in testLoader:\n",
    "\t\t\t# send the input to the device\n",
    "      x = np.transpose(x, (0, 3, 1, 2))\n",
    "      y = np.transpose(y, (0, 3, 1, 2))\n",
    "      (x, y) = (x.to(DEVICE), y.to(DEVICE)) #config\n",
    "\n",
    "\t\t\t# make the predictions and calculate the validation loss\n",
    "\n",
    "      pred = unet(x)\n",
    "      totalTestLoss += lossFunc(pred, y)\n",
    "\n",
    "\t# calculate the average training and validation loss\n",
    "  avgTrainLoss = totalTrainLoss / trainSteps\n",
    "  avgTestLoss = totalTestLoss / testSteps\n",
    "\n",
    "\t# update our training history\n",
    "  H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "  H[\"test_loss\"].append(avgTestLoss.cpu().detach().numpy())\n",
    "\n",
    "\t# print the model training and validation information\n",
    "  print(\"[INFO] EPOCH: {}/{}\".format(e + 1, NUM_EPOCHS)) #config\n",
    "  print(\"Train loss: {:.6f}, Test loss: {:.4f}\".format(\n",
    "      avgTrainLoss, avgTestLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "srY1tqjKl8rO",
    "outputId": "574c6f87-da18-47b0-df8d-16641aee81d9"
   },
   "outputs": [],
   "source": [
    "# display the total time needed to perform the training\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
    "    endTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dir(__main__)#you are using every core\n",
    "#I don't think this is a mutliprocessing issue\n",
    "#with 0 works, i'm still seeing this\n",
    "#something to try is creating a py file and importing the class, like put the dataset class in a py file\n",
    "#there is some kind of werid like scope issue i think"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 494
    },
    "id": "aI9hPDeemFsl",
    "outputId": "0bbc51d2-7919-4b38-e6db-3f12ade95168"
   },
   "outputs": [],
   "source": [
    "# plot the training loss\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(H[\"test_loss\"], label=\"test_loss\")\n",
    "plt.title(\"Training Loss on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "#plt.savefig(PLOT_PATH) #config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-ulLe1WmnxA"
   },
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfH_eHYBnEyl"
   },
   "outputs": [],
   "source": [
    "def prepare_plot(origImage, origMask, predMask):\n",
    "\t# initialize our figure\n",
    "  figure, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 10))\n",
    "\t# plot the original image, its mask, and the predicted mask\n",
    "  feature_num = 0 #can modify this to see different feature\n",
    "  #ax[0].imshow(origImage[:,:,feature_num])\n",
    "  ax[0].imshow(origMask)\n",
    "  ax[1].imshow(predMask)\n",
    "\n",
    "\t# set the titles of the subplots\n",
    "  ax[0].set_title(\"Original Mask\")\n",
    "  ax[1].set_title(\"Predicted Mask\")\n",
    "\n",
    "\t# set the layout of the figure and display it\n",
    "  figure.tight_layout()\n",
    "  figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eidFu_-k6iee"
   },
   "outputs": [],
   "source": [
    "def prepare_plot_with_one_feature(origImage, origMask, predMask, feature_num = 0): #can modify feature_num to see different feature\n",
    "\t# initialize our figure\n",
    "  figure, ax = plt.subplots(nrows=1, ncols=3, figsize=(10, 10))\n",
    "\t# plot the original image, its mask, and the predicted mask\n",
    "  ax[0].imshow(origImage[:,:,feature_num])\n",
    "  ax[1].imshow(origMask)\n",
    "  ax[2].imshow(predMask)\n",
    "\n",
    "\t# set the titles of the subplots\n",
    "  ax[0].set_title(\"Image of feature\"+str(feature_num))\n",
    "  ax[1].set_title(\"Original Mask\")\n",
    "  ax[2].set_title(\"Predicted Mask\")\n",
    "\n",
    "\t# set the layout of the figure and display it\n",
    "  figure.tight_layout()\n",
    "  figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LK_Ikxo45s3T"
   },
   "outputs": [],
   "source": [
    "def prepare_plot_with_all_features(origImage, origMask, predMask):\n",
    "\t# initialize our figure\n",
    "  figure, ax = plt.subplots(nrows=3, ncols=4, figsize=(10, 10))\n",
    "\t# plot the original image, its mask, and the predicted mask\n",
    "  feature_num = 0 #can modify this to see different feature\n",
    "  #ax[0].imshow(origImage[:,:,feature_num])\n",
    "  ax[0, 0].imshow(origMask)\n",
    "  ax[0, 1].imshow(predMask)\n",
    "  for feature_num in range(2, 11):\n",
    "    ax[int(feature_num//4), int(feature_num%4)].imshow(origImage[:,:,feature_num-2])\n",
    "\n",
    "\t# set the titles of the subplots\n",
    "  ax[0, 0].set_title(\"Original Mask\")\n",
    "  ax[0, 1].set_title(\"Predicted Mask\")\n",
    "  for feature_num in range(2, 11):\n",
    "    ax[int(feature_num//4), int(feature_num%4)].set_title(\"Image of feature\"+str(feature_num-2))\n",
    "\n",
    "\t# set the layout of the figure and display it\n",
    "  figure.tight_layout()\n",
    "  figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZezAd6MInVdf"
   },
   "outputs": [],
   "source": [
    "#def make_predictions(model, imagePath):\n",
    "def make_predictions(model, image_idx):\n",
    "#def make_predictions(model, image_1, mask_1):\n",
    "\t# set model to evaluation mode\n",
    "  model.eval()\n",
    "\n",
    "\t# turn off gradient tracking\n",
    "  with torch.no_grad():\n",
    "\t\t# load the image from disk, swap its color channels, cast it\n",
    "\t\t# to float data type, and scale its pixel values\n",
    "    image = images[image_idx]\n",
    "\n",
    "\t\t# resize the image and make a copy of it for visualization\n",
    "    #image = cv2.resize(image, (128, 128))\n",
    "    orig = image.copy()\n",
    "\n",
    "\t\t# find the filename and generate the path to ground truth\n",
    "\t\t# mask\n",
    "\n",
    "\t\t# load the ground-truth segmentation mask in grayscale mode\n",
    "\t\t# and resize it\n",
    "    gtMask = labels[image_idx] #in grayscale?\n",
    "    gtMask = cv2.resize(gtMask, (INPUT_IMAGE_HEIGHT, #config\n",
    "                                 INPUT_IMAGE_HEIGHT)) #config\n",
    "    #print(np.array(gtMask.flatten()).sum())\n",
    "\n",
    "\t\t# make the channel axis to be the leading one, add a batch\n",
    "\t\t# dimension, create a PyTorch tensor, and flash it to the\n",
    "\t\t# current device\n",
    "    image = np.transpose(image, (2, 0, 1))\n",
    "    #print(image.shape)\n",
    "    #image = np.expand_dims(image, 0)\n",
    "    image = np.array([image])\n",
    "    image = torch.from_numpy(image).to(DEVICE) #config\n",
    "    #print(image.shape)\n",
    "\t\t# make the prediction, pass the results through the sigmoid\n",
    "\t\t# function, and convert the result to a NumPy array\n",
    "\n",
    "\t\t# filter out the weak predictions and convert them to integers\n",
    "    #predMask = (predMask > thres)* 255 #config\n",
    "    predMask = model(image).detach().cpu().numpy() > 0\n",
    "    predMask = predMask[0][0]\n",
    "    #predMask = predMask.astype(np.uint8)\n",
    "\t\t# prepare a plot for visualization\n",
    "    prepare_plot_with_one_feature(orig, gtMask, predMask)\n",
    "    return predMask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3Dty5NeO18_"
   },
   "source": [
    "Draft for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N97eCORC5gVK"
   },
   "outputs": [],
   "source": [
    "exp = images[m_tiles[50]]\n",
    "exp = np.transpose(exp, (2, 0, 1))\n",
    "exp = np.array([exp])\n",
    "exp = torch.from_numpy(exp).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XyPPi9Ng5tUb",
    "outputId": "1853d114-8a78-4238-d4a5-98667b240f82"
   },
   "outputs": [],
   "source": [
    "exp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Qmlfm944mqF"
   },
   "outputs": [],
   "source": [
    "pred_exp = unet(exp).detach().cpu().numpy() > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "siCga97v6e40",
    "outputId": "91c32c67-daab-4b21-c880-3754149e46a0"
   },
   "outputs": [],
   "source": [
    "show(pred_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 361
    },
    "id": "2GPSY3lX7ZBZ",
    "outputId": "5c96d413-658e-4c2b-a38c-64eaf1c1aed0"
   },
   "outputs": [],
   "source": [
    "b_new= make_predictions(unet, m_tiles[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "XoI7bZAT9f9b",
    "outputId": "5f03333b-3a4f-499c-e772-7f312b785de5"
   },
   "outputs": [],
   "source": [
    "make_predictions(unet, m_tiles[62])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "wc2vjvc5968y",
    "outputId": "0106a5ad-2cdc-4e8a-c370-03918bf1aa57"
   },
   "outputs": [],
   "source": [
    "make_predictions(unet, m_tiles[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "bhMtYwJR-HAY",
    "outputId": "2f601ee3-9e05-4d23-fcb9-4203810e79bf"
   },
   "outputs": [],
   "source": [
    "make_predictions(unet, m_tiles[68])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "okDPk4ad9LBs",
    "outputId": "4627d339-5ff5-4632-8f4d-8272dab54761"
   },
   "outputs": [],
   "source": [
    "make_predictions(unet, 3984)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZoNwzZ22PDA"
   },
   "outputs": [],
   "source": [
    "# load the image paths in our testing file and randomly select 10\n",
    "# image paths\n",
    "# print(\"[INFO] loading up test image paths...\")\n",
    "# imagePaths = open(TEST_PATHS).read().strip().split(\"\\n\") #config\n",
    "# imagePaths = np.random.choice(imagePaths, size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FRipN8ah2Se7"
   },
   "outputs": [],
   "source": [
    "# # load our model from disk and flash it to the current device\n",
    "# print(\"[INFO] load up model...\")\n",
    "# unet = torch.load(config.MODEL_PATH).to(config.DEVICE) #config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OvqMzxby2UXV"
   },
   "outputs": [],
   "source": [
    "# # iterate over the randomly selected test image paths\n",
    "# for path in imagePaths:\n",
    "#   # make predictions and visualize the results\n",
    "#   make_predictions(unet, path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
